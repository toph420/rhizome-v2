# Storage-First Portability System - Complete Guide

**Last Updated**: 2025-10-13
**Status**: Production Ready âœ…
**Version**: 1.0

---

## Table of Contents

1. [Overview](#overview)
2. [Core Philosophy](#core-philosophy)
3. [Getting Started](#getting-started)
4. [Admin Panel Guide](#admin-panel-guide)
5. [Common Workflows](#common-workflows)
6. [Conflict Resolution](#conflict-resolution)
7. [Connection Reprocessing](#connection-reprocessing)
8. [Export & Portability](#export--portability)
9. [Troubleshooting](#troubleshooting)
10. [Developer Reference](#developer-reference)

---

## Overview

The **Storage-First Portability System** is a comprehensive data management solution for Rhizome V2 that treats **Supabase Storage as the source of truth** for expensive AI-enriched data, while treating the **PostgreSQL database as a queryable cache**.

### Key Concepts

**Problem Solved**:
- Reprocessing documents costs $0.20-0.60 per document and takes 5-25 minutes
- Database resets during development lose all processed data
- No easy way to backup or migrate processed documents
- Difficult to recover from database corruption

**Solution**:
- Automatic export of all processed data to Supabase Storage
- Import from Storage with intelligent conflict resolution
- ZIP bundle generation for complete portability
- Smart Mode connection reprocessing preserves user work

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Document Processing                      â”‚
â”‚  (PDF, EPUB, YouTube, Web, Markdown, Text, Paste)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Gemini / Ollama Processing  â”‚
    â”‚   (Extraction + Metadata)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Supabase Storage   â”‚  â”‚  PostgreSQL DB   â”‚
    â”‚  (Source of Truth)  â”‚  â”‚  (Query Cache)   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ chunks.json       â”‚  â”‚ â€¢ chunks table   â”‚
    â”‚ â€¢ metadata.json     â”‚  â”‚ â€¢ embeddings     â”‚
    â”‚ â€¢ manifest.json     â”‚  â”‚ â€¢ connections    â”‚
    â”‚ â€¢ cached_chunks     â”‚  â”‚ â€¢ entities/comp  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                      â”‚
                â”‚                      â”‚
                â–¼                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Admin Panel (Cmd+Shift+A)      â”‚
    â”‚  Scanner | Import | Export | Connectionsâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits

1. **Cost Savings**: $0.20-0.60 saved per document by importing instead of reprocessing
2. **Time Savings**: DB reset + restore in 6 minutes vs 25 minutes reprocessing
3. **Data Safety**: Zero data loss, automatic backups to Storage
4. **Portability**: Complete ZIP bundles for backup or migration
5. **Development Velocity**: Quick database resets without losing work

---

## Core Philosophy

### Storage is the Source of Truth

**Traditional Approach** (Wrong):
```
Database stores everything â†’ Processing results only in DB â†’ Storage is secondary
```
âŒ Problem: Database resets lose all processed data, reprocessing is expensive

**Storage-First Approach** (Right):
```
Processing â†’ Save to Storage (source of truth) â†’ Sync to DB (cache for queries)
```
âœ… Benefit: Can always restore from Storage, database is disposable cache

### Automatic vs Manual Operations

**Automatic** (No user action required):
- Every document processing saves to Storage
- chunks.json, metadata.json, manifest.json created automatically
- cached_chunks.json saved in LOCAL mode

**Manual** (User-initiated via Admin Panel):
- Scanning Storage to find sync issues
- Importing from Storage when DB is out of sync
- Exporting to ZIP bundles
- Reprocessing connections

---

## Getting Started

### Prerequisites

- Rhizome V2 app running (`npm run dev`)
- Supabase local instance running (`npx supabase start`)
- At least one document processed

### Accessing the Admin Panel

**Three Ways to Open**:

1. **Mouse**: Click the **Database icon** in the TopNav header
2. **Keyboard**: Press `Cmd+Shift+A` (Mac) or `Ctrl+Shift+A` (Windows)
3. **Already Open**: Press the shortcut again to close

**Panel Features**:
- Slides down from top (85vh height)
- Backdrop dims the page
- Press `Esc` to close

### First-Time Setup

No setup required! The system works automatically once you process a document.

**Verify It's Working**:
1. Process any document (PDF, EPUB, etc.)
2. Open Supabase Storage UI: `http://localhost:54323` â†’ Storage tab
3. Navigate to: `documents/{your_user_id}/{document_id}/`
4. You should see: `chunks.json`, `metadata.json`, `manifest.json`

âœ… If these files exist, the system is working correctly!

---

## Admin Panel Guide

### Tab 1: Scanner

**Purpose**: Compare Storage vs Database state and identify sync issues

**What It Does**:
- Lists all documents in Storage
- Checks if they exist in Database
- Shows sync state for each document

**Sync States**:
- ğŸŸ¢ **Healthy**: Storage and Database match
- ğŸŸ¡ **Out of Sync**: Different chunk counts
- ğŸ”´ **Missing from DB**: In Storage but not in Database
- âšª **Missing from Storage**: In Database but not in Storage (unusual)

**How to Use**:
1. Open Admin Panel â†’ Scanner tab
2. Scanner automatically scans on load
3. Use filters to view specific states:
   - **All**: Show everything
   - **Missing from DB**: Show documents that need import
   - **Out of Sync**: Show discrepancies
   - **Healthy**: Show documents in sync
4. Click a row to expand and see file details
5. Use action buttons: Import, Sync, Export, Details

**Summary Statistics**:
- Total documents in Storage
- Total documents in Database
- Missing from Database count
- Out of Sync count
- Healthy count

---

### Tab 2: Import

**Purpose**: Restore chunks from Storage to Database

**When to Use**:
- After database reset (development)
- Database corruption recovery
- Moving chunks from Storage to a new database
- Syncing documents after manual Storage changes

**How to Use**:

**Basic Import (No Conflict)**:
1. Open Admin Panel â†’ Import tab
2. Select documents to import (checkboxes)
3. Choose import options:
   - â˜‘ **Regenerate Embeddings**: Create new 768d vectors (slower)
   - â˜ **Reprocess Connections**: Run 3-engine connection detection (optional)
4. Click "Import Selected"
5. Monitor progress in the tab
6. Wait for completion

**Import with Conflict** (see [Conflict Resolution](#conflict-resolution) below):
1. Same steps as above
2. If chunks already exist, ConflictResolutionDialog opens
3. Choose strategy: Skip, Replace, or Merge Smart
4. Apply resolution
5. Import proceeds with chosen strategy

**Import Options Explained**:

**Regenerate Embeddings**:
- âœ… Use when: Switching embedding models, embedding corruption
- âŒ Skip when: Embeddings are fine (faster import)
- Time: Adds ~2-5 minutes for 300 chunks

**Reprocess Connections**:
- âœ… Use when: Want fresh connection detection
- âŒ Skip when: Connections are fine (faster import)
- Time: Adds ~10-15 minutes for 300 chunks

**Progress Tracking**:
- Import job shows stages: Reading â†’ Processing â†’ Inserting â†’ Complete
- Percentage updates in real-time
- Details show current operation (e.g., "Inserting chunk 150/300")

---

### Tab 3: Export

**Purpose**: Generate ZIP bundles for complete document portability

**When to Use**:
- Creating backups for critical documents
- Migrating documents to another system
- Sharing complete document bundles
- Archiving finished projects

**How to Use**:

**Export Single Document**:
1. Open Admin Panel â†’ Export tab
2. Check one document
3. Choose export options:
   - â˜‘ **Include Connections**: Add 3-engine collision detection results
   - â˜‘ **Include Annotations**: Add user highlights and notes
4. Click "Export Selected (1)"
5. Wait for ZIP generation (< 2 minutes)
6. "Download ZIP" button appears
7. Click to download

**Batch Export (Multiple Documents)**:
1. Select 5-10 documents with checkboxes
2. Choose export options
3. Click "Export Selected (5)"
4. Progress shows: "Processing document 1 of 5", "Processing document 2 of 5", etc.
5. Download button appears when complete

**ZIP Bundle Structure**:
```
export-2025-10-13.zip
â”œâ”€â”€ manifest.json                       # Top-level: describes all documents
â”œâ”€â”€ doc-id-1/
â”‚   â”œâ”€â”€ source.pdf                      # Original file
â”‚   â”œâ”€â”€ content.md                      # Cleaned markdown
â”‚   â”œâ”€â”€ chunks.json                     # Enriched chunks
â”‚   â”œâ”€â”€ metadata.json                   # Document metadata
â”‚   â”œâ”€â”€ manifest.json                   # File inventory
â”‚   â”œâ”€â”€ cached_chunks.json              # (if LOCAL mode)
â”‚   â”œâ”€â”€ connections.json                # (if Include Connections)
â”‚   â””â”€â”€ annotations.json                # (if Include Annotations)
â”œâ”€â”€ doc-id-2/
â”‚   â””â”€â”€ (same structure)
â””â”€â”€ doc-id-3/
    â””â”€â”€ (same structure)
```

**Download URL**:
- Expires after 24 hours
- Download immediately or save the URL
- Generate new export after expiry

---

### Tab 4: Connections

**Purpose**: Reprocess chunk connections with 3 engines

**When to Use**:
- New documents added (Add New mode)
- Engine improvements released (Reprocess All)
- User validated connections (Smart Mode to preserve)
- Connection quality issues

**Reprocessing Modes**:

**1. Reprocess All** (Fresh Start):
- **What**: Deletes ALL connections, regenerates from scratch
- **When**: Major engine updates, starting fresh
- **Warning**: âš ï¸ Loses user-validated connections
- **Time**: ~15 minutes for 300 chunks

**2. Add New** (Incremental):
- **What**: Keeps existing, adds connections to newer documents
- **When**: New documents processed after this one
- **Benefit**: Preserves existing work
- **Time**: Depends on new document count

**3. Smart Mode** (Recommended):
- **What**: Preserves user-validated, updates the rest
- **When**: Most common use case
- **Options**:
  - â˜‘ **Preserve user-validated connections**: Keep manually reviewed connections
  - â˜‘ **Save backup before reprocessing**: Storage backup for safety
- **Time**: ~15 minutes for 300 chunks

**Engine Selection**:

Choose which engines to run (all recommended):

â˜‘ **Semantic Similarity** (~200ms per chunk, free):
- Embedding-based, finds "these say the same thing"
- Fast, no AI calls

â˜‘ **Contradiction Detection** (~50ms per chunk, free):
- Metadata-based, finds conceptual tensions
- "Same concept, opposite emotional polarity"

â˜‘ **Thematic Bridge** (~500ms per chunk, $0.20):
- AI-powered cross-domain matching
- "Paranoia in Gravity's Rainbow â†” surveillance capitalism"
- Aggressive filtering keeps cost down

**Estimate Display**:
- Shows estimated time: "~8 minutes"
- Shows estimated cost: "$0.20"
- Updates based on selections

**How to Use**:
1. Open Admin Panel â†’ Connections tab
2. Select a document
3. View current stats: "85 connections (12 user-validated)"
4. Choose mode: **Smart Mode** (recommended)
5. Check Smart Mode options (both recommended)
6. Select engines (all 3 recommended)
7. Review estimate
8. Click "Start Reprocessing"
9. Monitor progress
10. Review results when complete

---

### Tab 5: Integrations

**Purpose**: Centralize Obsidian and Readwise operations

**Obsidian Section**:
- **Export to Obsidian**: Save markdown to your vault
- **Sync from Obsidian**: Import edited markdown with fuzzy annotation recovery
- **Vault Path**: Configure Obsidian vault location
- **Last Sync**: Timestamp of most recent operation

**Readwise Section**:
- **Import Highlights**: Import from Readwise export JSON
- **Last Import**: Timestamp and highlight count
- **API Key**: Configure Readwise API credentials (optional)

**Operation History**:
- Table shows recent integration operations
- Columns: Operation Type, Status, Timestamp
- Shows last 10 operations
- Click to see details

---

### Tab 6: Jobs

**Purpose**: Background job management

**Quick Actions**:
- **Clear Completed Jobs**: Remove successful jobs from queue
- **Clear Failed Jobs**: Remove failed jobs for cleanup

**Emergency Controls**:
- **Stop All Processing**: Force-cancel all running jobs (for stuck jobs)
- **Clear All Jobs**: Delete ALL jobs (completed, failed, pending)
- **Nuclear Reset**: âš ï¸ DANGER - Deletes all jobs AND documents with status 'processing'

**Job List**:
- Shows all background jobs (import, export, reprocess, etc.)
- Columns: Job Type, Status, Progress, Created At, Actions
- Real-time updates

---

## Common Workflows

### Workflow 1: Database Reset During Development

**Scenario**: You're developing a feature and need to reset the database to test migrations.

**Problem**: Resetting wipes all processed documents (expensive to reprocess).

**Solution**: Import from Storage after reset.

**Steps**:
1. Before reset, verify Storage has your documents:
   ```bash
   # Check Supabase Storage UI
   http://localhost:54323 â†’ Storage â†’ documents
   ```
2. Reset database:
   ```bash
   npx supabase db reset
   ```
3. Open Admin Panel (`Cmd+Shift+A`) â†’ Scanner tab
4. All documents show "Missing from DB"
5. Switch to Import tab
6. Click "Select All"
7. Leave "Regenerate Embeddings" unchecked (faster)
8. Click "Import Selected"
9. Wait ~3-6 minutes for import
10. âœ… All documents restored to database!

**Time Comparison**:
- Without Storage-First: 25+ minutes to reprocess all documents
- With Storage-First: 3-6 minutes to import from Storage
- **Time Saved**: ~20 minutes per reset

---

### Workflow 2: Conflict Resolution (Import)

**Scenario**: You're importing a document that already has chunks in the database, but the versions differ.

**Problem**: Import could overwrite, skip, or merge data. Need to choose wisely.

**Solution**: ConflictResolutionDialog provides 3 strategies.

**Steps**:
1. Select document in Import tab
2. Click "Import"
3. ConflictResolutionDialog opens (conflict detected)
4. Review side-by-side comparison:
   - **Existing (Database)**: 382 chunks, processed 2025-10-10
   - **Import (Storage)**: 382 chunks, processed 2025-10-12
5. Review sample chunks (first 3 shown with differences highlighted)
6. Choose strategy (see [Conflict Resolution](#conflict-resolution) below)
7. Click "Apply Resolution"
8. Import proceeds with chosen strategy

**Strategy Decision Tree**:
```
Has annotations? â”€â”€Yesâ”€â”€> Use Merge Smart (preserves annotations)
     â”‚
     No
     â”‚
     â–¼
Database version is correct? â”€â”€Yesâ”€â”€> Use Skip (keep database)
     â”‚
     No
     â”‚
     â–¼
Storage version is correct? â”€â”€Yesâ”€â”€> Use Replace (use storage)
```

---

### Workflow 3: Connection Reprocessing with Smart Mode

**Scenario**: You've manually validated 10 connections as important. Now you want to regenerate the rest to get fresh connections from improved engines.

**Problem**: Reprocess All would delete your validated connections.

**Solution**: Smart Mode preserves validated connections.

**Steps**:
1. Mark connections as validated (if not already):
   ```sql
   UPDATE chunk_connections
   SET user_validated = true
   WHERE id IN (SELECT id FROM chunk_connections WHERE /* your criteria */ LIMIT 10);
   ```
2. Open Admin Panel â†’ Connections tab
3. Select the document
4. Verify current stats: "85 connections (10 user-validated)"
5. Select mode: **Smart Mode**
6. Check options:
   - â˜‘ Preserve user-validated connections
   - â˜‘ Save backup before reprocessing
7. Select all 3 engines
8. Review estimate: "~12 minutes, $0.20"
9. Click "Start Reprocessing"
10. Monitor progress: "Backing up validated connections..." â†’ "Deleting non-validated..." â†’ "Running engines..."
11. On completion, verify:
    - 10 validated connections still exist
    - New connections generated for the rest
    - Backup file in Storage: `validated-connections-{timestamp}.json`

**Verification**:
```sql
SELECT COUNT(*) FROM chunk_connections
WHERE user_validated = true
AND source_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>');
-- Should show 10
```

---

### Workflow 4: Export to ZIP for Backup

**Scenario**: You've processed 50 important books and want to create a complete backup.

**Problem**: Database backup is not portable, doesn't include Storage files.

**Solution**: Export to ZIP bundles.

**Steps**:
1. Open Admin Panel â†’ Export tab
2. Click "Select All" (or select specific documents)
3. Check both export options:
   - â˜‘ Include Connections
   - â˜‘ Include Annotations
4. Verify estimated size (e.g., "~2.5GB estimated")
5. Click "Export Selected (50)"
6. Wait for ZIP generation (~10-20 minutes for 50 documents)
7. "Download ZIP" button appears
8. Click download
9. Save to safe location (external drive, cloud storage)

**ZIP Contents**:
- All 50 documents with complete file sets
- Top-level manifest.json listing all documents
- connections.json for each document (3-engine results)
- annotations.json for each document (your highlights and notes)

**Use Cases for ZIP**:
- âœ… Backup before major refactoring
- âœ… Migrate to another instance
- âœ… Share complete document bundles
- âœ… Archive completed projects

---

## Conflict Resolution

### Understanding Conflicts

**When Conflicts Occur**:
- Importing a document that already has chunks in database
- Chunk counts differ (Storage: 382, Database: 150)
- Metadata differences (processed at different times)

**Why Conflicts Matter**:
- Wrong choice can lose user work (annotations)
- Wrong choice can lose data integrity
- Understanding strategies is critical

---

### Strategy 1: Skip Import

**What It Does**: Keeps existing database data, ignores import data from Storage

**When to Use**:
- Database version is correct and up-to-date
- You made manual edits to database you want to keep
- Import data is outdated or wrong

**Impact**:
- âœ… No changes to database
- âœ… Annotations preserved (no change)
- âŒ Storage data ignored (not used)

**Warning**: None (safest option, no changes)

**Example Dialog**:
```
Resolution: Skip Import

âš ï¸ Warning: Import data will be ignored.
The database will remain unchanged.

[Cancel] [Apply Resolution]
```

---

### Strategy 2: Replace All

**What It Does**: Deletes ALL existing chunks, inserts chunks from Storage

**When to Use**:
- Storage version is definitely correct
- Database is corrupted or wrong
- Starting fresh after major changes

**Impact**:
- âœ… Database matches Storage exactly
- âŒ **All existing chunks deleted** (including IDs)
- âŒ **Annotations will break** (chunk IDs changed)
- âŒ **Connections reset** (references to old chunk IDs)

**Warning**: ğŸš¨ **DESTRUCTIVE** - Will reset all annotation positions!

**Example Dialog**:
```
Resolution: Replace All

ğŸš¨ DANGER: This will delete all existing chunks and replace them with import data.

âš ï¸ Warnings:
- Will reset all annotation positions
- Connections will reference wrong chunks
- User work may be lost

Only use if you're certain the import data is correct.

[Cancel] [Apply Resolution]
```

---

### Strategy 3: Merge Smart (Recommended)

**What It Does**: Updates metadata fields while preserving chunk IDs and content

**When to Use**:
- **Most common use case** (default recommendation)
- You have annotations you want to keep
- Database and Storage both have valid data
- Want to update metadata without breaking annotations

**How It Works**:
1. Matches chunks by `chunk_index` (position in document)
2. Updates metadata fields only:
   - themes
   - importance_score
   - summary
   - emotional_metadata
   - conceptual_metadata
   - domain_metadata
   - metadata_extracted_at
3. Preserves:
   - chunk.id (UUID) - Annotations reference this!
   - chunk.content - Actual text unchanged
   - chunk.embedding - Vectors unchanged
   - chunk.document_id - References intact

**Impact**:
- âœ… Metadata updated to match Storage
- âœ… Annotations preserved (chunk IDs unchanged)
- âœ… Connections preserved (chunk references intact)
- âœ… Content and embeddings unchanged
- âœ… Data integrity maintained

**Warning**: Info message explaining preservation

**Example Dialog**:
```
Resolution: Merge Smart (Recommended)

â„¹ï¸ This strategy preserves annotations by keeping chunk IDs.

What happens:
- Metadata updated from import data
- Chunk IDs stay the same
- Annotations continue to work
- Connections remain valid

This is the safest option when you have annotations.

[Cancel] [Apply Resolution]
```

---

### Conflict Resolution Decision Matrix

| Scenario | Has Annotations? | Database Correct? | Storage Correct? | **Strategy** |
|----------|------------------|-------------------|------------------|--------------|
| DB reset recovery | No | No | âœ… Yes | **Replace** |
| DB reset recovery | âœ… Yes | No | âœ… Yes | **Merge Smart** |
| Manual DB edits | N/A | âœ… Yes | No | **Skip** |
| Metadata refresh | âœ… Yes | Partial | âœ… Yes | **Merge Smart** |
| Corruption | âœ… Yes | No | âœ… Yes | **Merge Smart** |
| Fresh start | No | No | âœ… Yes | **Replace** |
| Testing | No | Either | Either | **Replace** or **Skip** |

**Rule of Thumb**: **If you have annotations, always use Merge Smart.**

---

## Connection Reprocessing

### Why Reprocess Connections?

**Reasons**:
1. **New Documents**: Added documents after this one (Add New mode)
2. **Engine Improvements**: Engines updated with better algorithms (Reprocess All)
3. **Quality Issues**: Connections seem wrong or incomplete (Reprocess All)
4. **Preservation**: Want fresh connections but keep validated (Smart Mode)

---

### Mode 1: Reprocess All

**What It Does**:
1. Deletes ALL connections for the document
2. Runs selected engines from scratch
3. Generates fresh connections

**SQL Equivalent**:
```sql
-- Delete all connections
DELETE FROM chunk_connections
WHERE source_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>')
OR target_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>');

-- Run orchestrator (3 engines)
-- Generates new connections
```

**Use Cases**:
- âœ… Major engine update released
- âœ… Starting completely fresh
- âœ… Previous connections are wrong
- âœ… Testing new engine parameters

**Warnings**:
- âŒ Loses ALL user-validated connections
- âŒ Cannot undo (unless backup exists)

**Time**: ~15 minutes for 300 chunks, all 3 engines

---

### Mode 2: Add New

**What It Does**:
1. Keeps ALL existing connections
2. Identifies documents processed after this one
3. Runs engines only for connections to those newer documents

**Logic**:
```sql
-- Find newer documents
SELECT id FROM documents
WHERE created_at > (
  SELECT created_at FROM documents WHERE id = '<current_doc_id>'
);

-- Only process connections between current doc and newer docs
-- Existing connections preserved
```

**Use Cases**:
- âœ… New documents added regularly
- âœ… Want incremental updates
- âœ… Preserve existing work

**Benefits**:
- âœ… Fast (only processes new connections)
- âœ… Preserves all existing connections
- âœ… Incremental approach

**Time**: Depends on number of newer documents

---

### Mode 3: Smart Mode (Recommended)

**What It Does**:
1. **Backup Phase**: Saves user-validated connections to Storage
2. **Delete Phase**: Deletes only non-validated connections
3. **Process Phase**: Runs selected engines
4. **Preserve Phase**: User-validated connections remain untouched

**SQL Equivalent**:
```sql
-- 1. Query validated connections
SELECT * FROM chunk_connections
WHERE user_validated = true
AND (source_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>')
     OR target_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>'));

-- 2. Save to Storage: validated-connections-{timestamp}.json

-- 3. Delete non-validated only
DELETE FROM chunk_connections
WHERE user_validated IS NULL
AND (source_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>')
     OR target_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>'));

-- 4. Run orchestrator (3 engines)
-- Generates new connections

-- 5. Validated connections remain (not deleted in step 3)
```

**Options**:

**Preserve user-validated connections**:
- âœ… Checked (default, recommended): Keeps validated connections
- âŒ Unchecked: Deletes all connections (acts like Reprocess All)

**Save backup before reprocessing**:
- âœ… Checked (default, recommended): Saves `validated-connections-{timestamp}.json` to Storage
- âŒ Unchecked: No backup (faster, but risky)

**Use Cases**:
- âœ… **Most common use case**
- âœ… You have manually validated connections
- âœ… Want fresh connections for the rest
- âœ… Balance between fresh data and preserving work

**Benefits**:
- âœ… Preserves user work (validated connections)
- âœ… Updates non-validated connections
- âœ… Storage backup for safety
- âœ… Best of both worlds

**Time**: ~15 minutes for 300 chunks, all 3 engines

---

### Engine Selection Guide

**All 3 Engines (Recommended)**:
- Best quality connections
- Cost: ~$0.20 per document
- Time: ~15 minutes for 300 chunks

**Semantic Similarity Only (Fast & Free)**:
- Finds "these say the same thing"
- Cost: $0 (no AI calls)
- Time: ~2 minutes for 300 chunks

**Semantic + Contradiction (Good Balance)**:
- Finds similarities and tensions
- Cost: $0 (no AI calls)
- Time: ~3 minutes for 300 chunks

**All Except Thematic Bridge (Free)**:
- Skips expensive AI-powered cross-domain matching
- Cost: $0
- Time: ~3 minutes for 300 chunks

**Thematic Bridge Only (Specific Use Case)**:
- Only cross-domain concept matching
- Cost: ~$0.20
- Time: ~12 minutes for 300 chunks

---

## Export & Portability

### ZIP Bundle Generation

**What's in a ZIP**:
```
export-2025-10-13.zip (2.5GB)
â”‚
â”œâ”€â”€ manifest.json                           # Top-level manifest
â”‚   {
â”‚     "version": "1.0",
â”‚     "export_date": "2025-10-13T12:00:00Z",
â”‚     "document_count": 3,
â”‚     "total_size_bytes": 2684354560,
â”‚     "documents": [
â”‚       {
â”‚         "document_id": "doc-abc",
â”‚         "title": "Gravity's Rainbow",
â”‚         "author": "Thomas Pynchon",
â”‚         "chunk_count": 382,
â”‚         "...": "..."
â”‚       },
â”‚       "..."
â”‚     ]
â”‚   }
â”‚
â”œâ”€â”€ doc-abc/
â”‚   â”œâ”€â”€ source.pdf                          # Original PDF file
â”‚   â”œâ”€â”€ content.md                          # Cleaned markdown
â”‚   â”œâ”€â”€ chunks.json                         # Enriched chunks (final)
â”‚   â”‚   {
â”‚   â”‚     "version": "1.0",
â”‚   â”‚     "document_id": "doc-abc",
â”‚   â”‚     "processing_mode": "cloud",
â”‚   â”‚     "created_at": "2025-10-12T10:00:00Z",
â”‚   â”‚     "chunks": [
â”‚   â”‚       {
â”‚   â”‚         "chunk_index": 0,
â”‚   â”‚         "content": "Chapter 1. It begins...",
â”‚   â”‚         "summary": "Introduction to...",
â”‚   â”‚         "themes": ["paranoia", "war", "..."],
â”‚   â”‚         "importance_score": 0.85,
â”‚   â”‚         "emotional_metadata": { "polarity": 0.2, "..." },
â”‚   â”‚         "conceptual_metadata": { "concepts": ["..."], "..." },
â”‚   â”‚         "domain_metadata": { "primary_domain": "literary", "..." }
â”‚   â”‚       },
â”‚   â”‚       "..."
â”‚   â”‚     ]
â”‚   â”‚   }
â”‚   â”œâ”€â”€ metadata.json                       # Document metadata
â”‚   â”‚   {
â”‚   â”‚     "document_id": "doc-abc",
â”‚   â”‚     "title": "Gravity's Rainbow",
â”‚   â”‚     "author": "Thomas Pynchon",
â”‚   â”‚     "word_count": 150000,
â”‚   â”‚     "page_count": 500,
â”‚   â”‚     "language": "en",
â”‚   â”‚     "genre": "literary fiction",
â”‚   â”‚     "publication_year": 1973,
â”‚   â”‚     "isbn": "...",
â”‚   â”‚     "..."
â”‚   â”‚   }
â”‚   â”œâ”€â”€ manifest.json                       # File inventory
â”‚   â”‚   {
â”‚   â”‚     "version": "1.0",
â”‚   â”‚     "document_id": "doc-abc",
â”‚   â”‚     "files": {
â”‚   â”‚       "source": { "path": "source.pdf", "size": 8421600, "hash": "..." },
â”‚   â”‚       "content": { "path": "content.md", "size": 524288, "hash": "..." },
â”‚   â”‚       "chunks": { "path": "chunks.json", "size": 2097152, "count": 382, "hash": "..." },
â”‚   â”‚       "metadata": { "path": "metadata.json", "size": 4096, "hash": "..." }
â”‚   â”‚     },
â”‚   â”‚     "processing_cost": {
â”‚   â”‚       "extraction": 0.12,
â”‚   â”‚       "metadata": 0.20,
â”‚   â”‚       "embeddings": 0.02,
â”‚   â”‚       "connections": 0.20,
â”‚   â”‚       "total": 0.54
â”‚   â”‚     },
â”‚   â”‚     "processing_time": {
â”‚   â”‚       "extraction": 180,
â”‚   â”‚       "cleanup": 30,
â”‚   â”‚       "chunking": 60,
â”‚   â”‚       "metadata": 240,
â”‚   â”‚       "embeddings": 120,
â”‚   â”‚       "total": 630
â”‚   â”‚     }
â”‚   â”‚   }
â”‚   â”œâ”€â”€ cached_chunks.json                  # (if LOCAL mode)
â”‚   â”œâ”€â”€ connections.json                    # (if Include Connections)
â”‚   â”‚   {
â”‚   â”‚     "version": "1.0",
â”‚   â”‚     "document_id": "doc-abc",
â”‚   â”‚     "created_at": "2025-10-12T11:00:00Z",
â”‚   â”‚     "connections": [
â”‚   â”‚       {
â”‚   â”‚         "connection_type": "semantic_similarity",
â”‚   â”‚         "source_chunk_index": 5,
â”‚   â”‚         "target_chunk_index": 42,
â”‚   â”‚         "target_document_id": "doc-abc",
â”‚   â”‚         "strength": 0.92,
â”‚   â”‚         "reasoning": "Both discuss paranoia themes",
â”‚   â”‚         "user_validated": false
â”‚   â”‚       },
â”‚   â”‚       "..."
â”‚   â”‚     ]
â”‚   â”‚   }
â”‚   â””â”€â”€ annotations.json                    # (if Include Annotations)
â”‚       {
â”‚         "version": "1.0",
â”‚         "document_id": "doc-abc",
â”‚         "annotations": [
â”‚           {
â”‚             "chunk_index": 10,
â”‚             "text": "Important passage about...",
â”‚             "selection_range": { "start": 0, "end": 50 },
â”‚             "note": "This connects to...",
â”‚             "created_at": "2025-10-13T09:00:00Z",
â”‚             "tags": ["key-concept", "thesis"]
â”‚           },
â”‚           "..."
â”‚         ]
â”‚       }
â”‚
â”œâ”€â”€ doc-def/
â”‚   â””â”€â”€ (same structure)
â”‚
â””â”€â”€ doc-ghi/
    â””â”€â”€ (same structure)
```

---

### Import from ZIP (Future Feature)

**Planned Workflow**:
1. Extract ZIP bundle
2. Admin Panel â†’ Import tab
3. "Import from ZIP" button
4. Select extracted folder
5. System reads all JSON files
6. Restores to database with conflict resolution
7. Rebuilds embeddings if needed

**Status**: Not yet implemented (future enhancement)

**Workaround**: Manually upload JSON files to Storage, then use Import tab

---

## Troubleshooting

### Problem: Scanner Shows "Missing from DB"

**Symptoms**:
- Document exists in Storage
- Not showing in Database
- Scanner shows red "Missing from DB" badge

**Diagnosis**:
```sql
-- Check if document exists
SELECT id, title, status FROM documents WHERE id = '<doc_id>';

-- Check if chunks exist
SELECT COUNT(*) FROM chunks WHERE document_id = '<doc_id>';
```

**Solutions**:
1. **Import from Storage**: Admin Panel â†’ Import tab â†’ Select document â†’ Import
2. **If import fails**: Check Supabase Storage to verify files exist
3. **If files missing**: Reprocess the document

---

### Problem: Conflict Resolution Dialog Won't Close

**Symptoms**:
- Dialog stuck open
- Cannot click "Apply Resolution"
- Button disabled

**Diagnosis**:
- Check browser console for errors
- Verify strategy selected (radio button)

**Solutions**:
1. Select a strategy (radio button)
2. Refresh page if button still disabled
3. Check browser console for JavaScript errors

---

### Problem: Import Job Stuck at "Processing"

**Symptoms**:
- Import job shows "Processing" for >10 minutes
- Progress not updating
- No errors visible

**Diagnosis**:
```bash
# Check worker logs
cd worker && npm run dev
# Look for errors in output

# Check database
SELECT * FROM background_jobs WHERE job_type = 'import_document' ORDER BY created_at DESC LIMIT 5;
```

**Solutions**:
1. **Check Worker**: Ensure worker is running (`cd worker && npm run dev`)
2. **Check Job Status**: Admin Panel â†’ Jobs tab â†’ Look for errors
3. **Cancel Job**: Admin Panel â†’ Jobs tab â†’ Cancel stuck job
4. **Retry**: Try import again

---

### Problem: Export ZIP Download Fails

**Symptoms**:
- Download button appears
- Click doesn't download
- Browser shows error

**Diagnosis**:
- Check signed URL expiry (24 hours)
- Check browser network tab for 403 error
- Check Supabase Storage for ZIP file

**Solutions**:
1. **Expired URL**: Generate new export (ZIP creation is fast, <2 min)
2. **Storage Issue**: Check Supabase Storage UI to verify ZIP exists
3. **Browser Issue**: Try different browser or incognito mode

---

### Problem: Smart Mode Doesn't Preserve Validated Connections

**Symptoms**:
- User-validated connections deleted
- After reprocess, validated connections missing

**Diagnosis**:
```sql
-- Check validated connections before reprocess
SELECT COUNT(*) FROM chunk_connections
WHERE user_validated = true
AND source_chunk_id IN (SELECT id FROM chunks WHERE document_id = '<doc_id>');

-- Check after reprocess
-- Should be same count
```

**Solutions**:
1. **Verify Option Checked**: Connections tab â†’ "Preserve user-validated connections" must be checked
2. **Check Backup**: Storage â†’ `validated-connections-*.json` should exist
3. **Restore from Backup**: Manually restore from JSON file if available

---

### Problem: Annotations Break After Import

**Symptoms**:
- Annotations don't show up
- Annotations point to wrong text
- Annotation positions wrong

**Diagnosis**:
- Check which import strategy was used
- Check chunk IDs in database

**Root Cause**: Used "Replace" strategy instead of "Merge Smart"

**Solutions**:
1. **Prevention**: Always use "Merge Smart" when annotations exist
2. **Recovery**: If backup exists, import again with Merge Smart
3. **No Recovery**: Annotations cannot be recovered if chunk IDs changed and no backup

---

## Developer Reference

### File Locations

**Backend**:
```
worker/
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ storage-helpers.ts              # Storage operations (save, read, hash, list)
â”œâ”€â”€ types/
â”‚   â””â”€â”€ storage.ts                      # Export schemas (TypeScript interfaces)
â”œâ”€â”€ handlers/
â”‚   â”œâ”€â”€ import-document.ts              # Import workflow handler
â”‚   â”œâ”€â”€ export-document.ts              # Export workflow handler (ZIP generation)
â”‚   â””â”€â”€ reprocess-connections.ts        # Connection reprocessing handler
â””â”€â”€ processors/
    â”œâ”€â”€ base.ts                         # BaseProcessor with saveStageResult()
    â”œâ”€â”€ pdf-processor.ts                # PDF processing with Storage saves
    â””â”€â”€ epub-processor.ts               # EPUB processing with Storage saves
```

**Frontend**:
```
src/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ actions/
â”‚       â””â”€â”€ documents.ts                # Server Actions (scanStorage, importFromStorage, exportDocuments, reprocessConnections)
â””â”€â”€ components/
    â””â”€â”€ admin/
        â”œâ”€â”€ AdminPanel.tsx              # Sheet-based panel with tabs
        â”œâ”€â”€ ConflictResolutionDialog.tsx # Conflict resolution UI
        â””â”€â”€ tabs/
            â”œâ”€â”€ ScannerTab.tsx          # Storage scanner UI
            â”œâ”€â”€ ImportTab.tsx           # Import workflow UI
            â”œâ”€â”€ ExportTab.tsx           # Export workflow UI
            â”œâ”€â”€ ConnectionsTab.tsx      # Connection reprocessing UI
            â”œâ”€â”€ IntegrationsTab.tsx     # Obsidian/Readwise UI
            â””â”€â”€ JobsTab.tsx             # Background job management UI
```

**Scripts**:
```
scripts/
â””â”€â”€ validate-complete-system.ts         # Automated validation (23 tests)
```

**Documentation**:
```
docs/
â”œâ”€â”€ STORAGE_FIRST_PORTABILITY_GUIDE.md  # This file
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ storage-first-portability.md    # Task breakdown (T-001 to T-024)
â”‚   â”œâ”€â”€ MANUAL_TESTING_CHECKLIST_T024.md # Manual testing guide (35+ scenarios)
â”‚   â””â”€â”€ T024_COMPLETION_SUMMARY.md      # Implementation summary
â””â”€â”€ prps/
    â””â”€â”€ storage-first-portability.md    # Original PRP (Product Requirements & Plans)
```

---

### Storage Paths

**Document Files**:
```
Storage: documents/{userId}/{documentId}/
â”œâ”€â”€ source.pdf                          # Original uploaded file
â”œâ”€â”€ content.md                          # Cleaned markdown
â”œâ”€â”€ chunks.json                         # Enriched chunks (FINAL)
â”œâ”€â”€ metadata.json                       # Document metadata (FINAL)
â”œâ”€â”€ manifest.json                       # File inventory + costs (FINAL)
â”œâ”€â”€ cached_chunks.json                  # Docling chunks (LOCAL mode FINAL)
â””â”€â”€ stage-*.json                        # Intermediate stages (optional)
```

**Connection Backups**:
```
Storage: documents/{userId}/{documentId}/
â””â”€â”€ validated-connections-{timestamp}.json  # Smart Mode backups
```

**Export Bundles**:
```
Storage: documents/{userId}/exports/
â””â”€â”€ export-{timestamp}.zip                  # ZIP bundles (24-hour signed URL)
```

---

### Database Tables

**Documents**:
```sql
documents
â”œâ”€â”€ id (UUID, primary key)
â”œâ”€â”€ user_id (UUID, foreign key)
â”œâ”€â”€ title (TEXT)
â”œâ”€â”€ status (TEXT: processing, completed, failed)
â”œâ”€â”€ source_type (TEXT: pdf, epub, youtube, web, etc.)
â”œâ”€â”€ created_at (TIMESTAMP)
â””â”€â”€ ...
```

**Chunks**:
```sql
chunks
â”œâ”€â”€ id (UUID, primary key)
â”œâ”€â”€ document_id (UUID, foreign key)
â”œâ”€â”€ chunk_index (INTEGER)
â”œâ”€â”€ content (TEXT)
â”œâ”€â”€ summary (TEXT)
â”œâ”€â”€ themes (TEXT[])
â”œâ”€â”€ importance_score (REAL)
â”œâ”€â”€ embedding (VECTOR(768))
â”œâ”€â”€ emotional_metadata (JSONB)
â”œâ”€â”€ conceptual_metadata (JSONB)
â”œâ”€â”€ domain_metadata (JSONB)
â”œâ”€â”€ heading_path (TEXT[])
â”œâ”€â”€ heading_level (INTEGER)
â”œâ”€â”€ section_marker (TEXT)
â””â”€â”€ ...
```

**Cached Chunks** (LOCAL mode):
```sql
cached_chunks
â”œâ”€â”€ id (UUID, primary key)
â”œâ”€â”€ document_id (UUID, foreign key)
â”œâ”€â”€ extraction_mode (TEXT: pdf, epub)
â”œâ”€â”€ markdown_hash (TEXT)
â”œâ”€â”€ chunk_data (JSONB)
â”œâ”€â”€ structure_data (JSONB)
â””â”€â”€ created_at (TIMESTAMP)
```

**Chunk Connections**:
```sql
chunk_connections
â”œâ”€â”€ id (UUID, primary key)
â”œâ”€â”€ source_chunk_id (UUID, foreign key)
â”œâ”€â”€ target_chunk_id (UUID, foreign key)
â”œâ”€â”€ connection_type (TEXT: semantic_similarity, contradiction_detection, thematic_bridge)
â”œâ”€â”€ strength (REAL)
â”œâ”€â”€ reasoning (TEXT)
â”œâ”€â”€ user_validated (BOOLEAN)
â””â”€â”€ created_at (TIMESTAMP)
```

**Background Jobs**:
```sql
background_jobs
â”œâ”€â”€ id (UUID, primary key)
â”œâ”€â”€ user_id (UUID, foreign key)
â”œâ”€â”€ job_type (TEXT: import_document, export_document, reprocess_connections, etc.)
â”œâ”€â”€ status (TEXT: pending, processing, completed, failed)
â”œâ”€â”€ progress (REAL: 0.0 to 1.0)
â”œâ”€â”€ input_data (JSONB)
â”œâ”€â”€ output_data (JSONB)
â”œâ”€â”€ error_message (TEXT)
â”œâ”€â”€ created_at (TIMESTAMP)
â””â”€â”€ updated_at (TIMESTAMP)
```

---

### JSON Schemas

**ChunksExport** (`chunks.json`):
```typescript
interface ChunksExport {
  version: string                        // "1.0"
  document_id: string
  processing_mode: 'local' | 'cloud'
  created_at: string                     // ISO 8601
  chunks: Array<{
    chunk_index: number
    content: string
    summary: string | null
    themes: string[]
    importance_score: number
    emotional_metadata: EmotionalMetadata | null
    conceptual_metadata: ConceptualMetadata | null
    domain_metadata: DomainMetadata | null
    metadata_extracted_at: string | null
    heading_path: string[] | null
    heading_level: number | null
    section_marker: string | null
    // NOTE: Excludes id, embedding, document_id (not portable)
  }>
}
```

**MetadataExport** (`metadata.json`):
```typescript
interface MetadataExport {
  version: string                        // "1.0"
  document_id: string
  title: string
  author: string | null
  word_count: number
  page_count: number
  language: string
  genre: string | null
  publication_year: number | null
  isbn: string | null
  source_type: 'pdf' | 'epub' | 'youtube' | 'web' | 'markdown' | 'text' | 'paste'
  original_filename: string | null
  created_at: string                     // ISO 8601
}
```

**ManifestExport** (`manifest.json`):
```typescript
interface ManifestExport {
  version: string                        // "1.0"
  document_id: string
  created_at: string                     // ISO 8601
  files: {
    source: FileInfo
    content: FileInfo
    chunks: FileInfo & { count: number }
    metadata: FileInfo
    cached_chunks?: FileInfo             // LOCAL mode only
  }
  processing_cost: {
    extraction: number
    metadata: number
    embeddings: number
    connections: number
    total: number
  }
  processing_time: {
    extraction: number                   // seconds
    cleanup: number
    chunking: number
    metadata: number
    embeddings: number
    total: number
  }
}

interface FileInfo {
  path: string
  size: number                           // bytes
  hash: string                           // SHA256
  created_at: string                     // ISO 8601
}
```

**CachedChunksExport** (`cached_chunks.json`, LOCAL mode only):
```typescript
interface CachedChunksExport {
  version: string                        // "1.0"
  document_id: string
  extraction_mode: 'pdf' | 'epub'
  markdown_hash: string                  // SHA256 of cleaned markdown
  created_at: string                     // ISO 8601
  chunks: Array<{
    chunk_index: number
    text: string
    provenance: {
      page_numbers: number[]
      bbox: BBox | null
    }
  }>
  structure: {
    headings: Array<{
      level: number
      text: string
      page_number: number | null
    }>
    total_pages: number
    has_toc: boolean
  }
}

interface BBox {
  l: number
  t: number
  r: number
  b: number
  page_number: number
}
```

---

### API Reference

**Server Actions** (in `src/app/actions/documents.ts`):

```typescript
// Scan Storage and compare to Database
export async function scanStorage(): Promise<{
  success: boolean
  documents: Array<{
    documentId: string
    title: string
    storageFiles: string[]
    inDatabase: boolean
    chunkCount: number | null
    syncState: 'healthy' | 'missing_from_db' | 'missing_from_storage' | 'out_of_sync'
    createdAt: string | null
  }>
  error?: string
}>

// Import from Storage with conflict resolution
export async function importFromStorage(
  documentId: string,
  options: {
    strategy?: 'skip' | 'replace' | 'merge_smart'
    regenerateEmbeddings?: boolean
    reprocessConnections?: boolean
  } = {}
): Promise<{
  success: boolean
  jobId?: string
  needsResolution?: boolean
  conflict?: ImportConflict
  error?: string
}>

// Export documents to ZIP bundle
export async function exportDocuments(
  documentIds: string[],
  options: {
    includeConnections?: boolean
    includeAnnotations?: boolean
    format?: 'storage' | 'zip'
  } = {}
): Promise<{
  success: boolean
  jobId?: string
  error?: string
}>

// Reprocess connections with mode selection
export async function reprocessConnections(
  documentId: string,
  options: {
    mode: 'all' | 'add_new' | 'smart'
    engines: Array<'semantic_similarity' | 'contradiction_detection' | 'thematic_bridge'>
    preserveValidated?: boolean
    backupFirst?: boolean
  }
): Promise<{
  success: boolean
  jobId?: string
  error?: string
}>
```

---

### Validation Commands

**Automated Validation**:
```bash
# Quick smoke tests (< 1 second)
npx tsx scripts/validate-complete-system.ts --quick

# Full validation with database checks
npx tsx scripts/validate-complete-system.ts --full

# Expected output:
# âœ… ALL VALIDATIONS PASSED
# Total Tests:  23
# âœ“ Passed:     23
# âœ— Failed:     0
# Duration:     0.00s
```

**Manual Testing**:
```bash
# Follow comprehensive manual testing guide
open docs/tasks/MANUAL_TESTING_CHECKLIST_T024.md

# Estimated time: 3-4 hours for complete validation
```

**Storage Validation** (for specific document):
```bash
cd worker
npx tsx scripts/validate-storage-export.ts <document_id> <user_id> cloud

# For LOCAL mode:
npx tsx scripts/validate-storage-export.ts <document_id> <user_id> local
```

---

## Conclusion

The Storage-First Portability System provides a robust, production-ready solution for managing AI-enriched document data in Rhizome V2.

**Key Takeaways**:
1. **Storage is the source of truth** - Database is a queryable cache
2. **Automatic backups** - Every processing saves to Storage
3. **Intelligent import** - 3 conflict resolution strategies protect user work
4. **Smart Mode** - Preserves validated connections during reprocessing
5. **Complete portability** - ZIP bundles for backup and migration

**Next Steps**:
1. Process a document and verify Storage files exist
2. Open Admin Panel (`Cmd+Shift+A`) and explore tabs
3. Try a database reset + import workflow
4. Export a document to ZIP for backup
5. Read task breakdown for implementation details: `docs/tasks/storage-first-portability.md`

**Support & Documentation**:
- Task Breakdown: `docs/tasks/storage-first-portability.md`
- Manual Testing: `docs/tasks/MANUAL_TESTING_CHECKLIST_T024.md`
- Implementation Summary: `docs/tasks/T024_COMPLETION_SUMMARY.md`
- Validation Script: `scripts/validate-complete-system.ts`

---

**Last Updated**: 2025-10-13
**Version**: 1.0
**Status**: Production Ready âœ…

ğŸ‰ **The Storage-First Portability System is complete and ready to use!**
