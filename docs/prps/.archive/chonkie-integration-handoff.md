# Chonkie Pipeline Integration - Session Handoff Summary

**Date**: 2025-01-13
**Session Duration**: ~3 hours
**Status**: Research Complete, PRP Drafted, Ready for Phase 1 Prototype
**Primary Document**: `docs/prps/chonkie-semantic-chunking-integration.md`

---

## Executive Summary

Researched and designed a comprehensive integration of Chonkie's semantic chunking pipeline into Rhizome's document processing system. The integration uses a **composable pipeline architecture** (CHOMP) that combines Docling's structural extraction with Chonkie's semantic intelligence, potentially improving connection detection by **15-40%** depending on configuration.

**Key Insight**: Rhizome's existing recovery systems (`recoverAnnotations` with 4-tier fuzzy matching, `remapConnections` with embedding-based similarity) are **accidentally perfect** for semantic chunking. This makes it a **low-risk, high-reward** enhancement.

---

## What We Discovered

### 1. Chonkie Library Overview

**What is Chonkie?**
- YC-backed text chunking library for RAG applications
- 2000+ GitHub stars, actively maintained
- **4.35x faster** than LangChain for semantic chunking
- 8 different chunkers, we focus on 2:
  - **SemanticChunker**: Embedding-based similarity grouping (fast, proven)
  - **SlumberChunker**: LLM-powered boundary decisions (slower, potentially better)

**Key Features**:
- Model2Vec embeddings (500x faster than transformers)
- CHOMP pipeline architecture (composable processing stages)
- OverlapRefinery (adds context between chunks)
- Percentile-based thresholds (adaptive to document)

### 2. Pipeline Architecture (CHOMP)

**CHOMP = CHOnkie's Multi-step Pipeline**

```
Fetcher â†’ Chef â†’ Chunker â†’ Refinery â†’ Porter
   â†“        â†“       â†“         â†“         â†“
(Extract) (Clean) (Split) (Enhance) (Export)
```

**Rhizome Integration**:
```
Docling     â†’ Cleanup â†’ Chonkie  â†’ Overlap    â†’ Metadata
(Fetcher)     (Chef)    (Chunker)  (Refinery)   (Porter)
```

**Three Variants Planned**:
1. **Variant A** (Phase 1-2): SemanticChunker only â†’ 15-20% improvement
2. **Variant B** (Phase 3): SemanticChunker + OverlapRefinery â†’ 20-30% improvement
3. **Variant C** (Phase 5): SlumberChunker + OverlapRefinery â†’ 25-40% improvement

### 3. Critical Discovery: Rhizome's Recovery Systems Are Already Built for This

**Annotation Recovery** (`worker/handlers/recover-annotations.ts`):
- 4-tier fuzzy matching (exact â†’ context â†’ chunk-bounded â†’ trigram)
- Character-offset based (NOT chunk-ID dependent)
- Confidence thresholds: â‰¥0.85 auto-recover, 0.75-0.85 review, <0.75 lost
- **Works identically with semantic boundaries as with page-break boundaries**
- Expected performance: 90-95% recovery (matches hybrid chunker)

**Connection Remapping** (`worker/handlers/remap-connections.ts`):
- Embedding-based cosine similarity matching
- Thresholds: â‰¥0.95 auto-remap, 0.85-0.95 review, <0.85 lost
- **Benefits from semantic coherence** (complete concepts = stable embeddings)
- Expected performance: â‰¥95% auto-remap (potentially BETTER than hybrid)

**Why this matters**: Most document processors would break with semantic chunking. Rhizome won't. This is a **competitive advantage**.

---

## Key Technical Decisions

### 1. Preserve Docling for Extraction âœ…

**Decision**: Keep Docling for PDF/EPUB extraction (Fetcher/Chef stage)
**Reason**: Docling provides structural metadata (pages, headings, bboxes) that Chonkie doesn't extract
**Strategy**: Docling extracts structure â†’ Chonkie creates semantic boundaries â†’ Transfer metadata via offset mapping

### 2. Pipeline Composition Over Single Chunker âœ…

**Decision**: Don't choose "hybrid OR semantic" - compose a pipeline
**Reason**: Combines strengths (Docling structure + Chonkie semantics + Overlap context)
**Benefit**: Incremental validation (test each stage independently)

### 3. OverlapRefinery for Connection Detection âœ…

**Decision**: Add 50-100 token overlap between chunks (Phase 3+)
**Reason**: Makes thematic bridges EXPLICIT instead of implicit
**Example**:
```
Without overlap:
Chunk 42: "Paranoia emerges as central theme..."
Chunk 43: "Surveillance systems mirror this anxiety..."
â†’ Connection detection must infer relationship

WITH overlap (50 tokens):
Chunk 42: "Paranoia emerges as central theme in postmodern literature..."
Chunk 43: "...in postmodern literature. Surveillance systems mirror this anxiety..."
â†’ Overlapping context makes bridge VISIBLE
```

**Hypothesis**: +5-10% additional connection improvement via overlap

### 4. SlumberChunker as Advanced Option âœ…

**Decision**: Test SlumberChunker (LLM-powered) in Phase 5 if SemanticChunker succeeds
**Reason**: LLM reasoning understands conceptual arcs better than embedding similarity
**Trade-off**: Slower (+15-20 min) but potentially better quality (+10-20% more connections)
**Constraint**: Only in LOCAL mode (Ollama) to avoid cost explosion

### 5. Phased Rollout Strategy âœ…

**Decision**: Incremental validation, not all-at-once implementation
**Phases**:
1. **Phase 1-2** (12-16 hrs): Validate SemanticChunker + recovery systems
2. **Phase 3** (10-12 hrs): A/B test with OverlapRefinery
3. **Phase 4** (8-10 hrs): Evaluate and decide (ship/iterate/abandon)
4. **Phase 5** (optional): Test SlumberChunker if hungry for more quality

---

## What We Created

### 1. Comprehensive PRP Document

**File**: `docs/prps/chonkie-semantic-chunking-integration.md`
**Length**: ~1,200 lines
**Sections**:
- Overview & value proposition
- Problem statement
- Goals & non-goals
- Technical design (CHOMP pipeline architecture)
- Component design (pipeline integration, metadata transfer, DB schema)
- Implementation plan (5 phases, 38-50 hours)
- Testing strategy (unit, integration, recovery validation)
- Risks & mitigations
- Success metrics & decision criteria
- Timeline & dependencies
- 3 appendices (config reference, decision tree, recovery systems deep-dive)

### 2. Research Documentation

**Sources Analyzed**:
- Chonkie documentation (https://docs.chonkie.ai)
- Chonkie GitHub repository
- SemanticChunker implementation details
- SlumberChunker technical overview
- Pipeline (CHOMP) architecture
- OverlapRefinery capabilities
- Benchmarks and performance data

**Key Findings Documented**:
- SemanticChunker: 2.5x faster than LangChain via running mean pooling
- Model2Vec: 500x faster than transformers, 92% quality
- Percentile thresholds: adaptive to document characteristics
- Overlap refinement: explicit chunk relationships

---

## Critical Files & References

### Existing Rhizome Files (Study These)

**Recovery Systems** (your competitive advantage):
- `worker/handlers/recover-annotations.ts` - 4-tier fuzzy matching
- `worker/handlers/remap-connections.ts` - Embedding-based remapping

**Current Processing Pipeline**:
- `docs/PROCESSING_PIPELINE.md` - Dual-mode architecture (LOCAL/CLOUD)
- `worker/processors/pdf-processor.ts` - PDF pipeline orchestration
- `worker/processors/epub-processor.ts` - EPUB pipeline orchestration
- `worker/lib/local/bulletproof-matcher.ts` - 5-layer matching system
- `worker/lib/cached-chunks.ts` - Chunk caching for zero-cost reprocessing

**Database**:
- `supabase/migrations/046_cached_chunks_table.sql` - Cached chunks architecture
- `supabase/migrations/047_chunk_validation_corrections.sql` - Validation/correction tracking

### Files to Create (Phase 1)

**Chonkie Integration**:
- `worker/lib/local/chonkie-pipeline.ts` - TypeScript pipeline handler
- `worker/scripts/chonkie_pipeline.py` - Python subprocess wrapper
- `worker/lib/local/metadata-transfer.ts` - Offset-based metadata mapping

**Testing**:
- `worker/lib/local/__tests__/chonkie-pipeline.test.ts` - Unit tests
- `scripts/validate-recovery-systems.ts` - Recovery validation
- `worker/processors/__tests__/semantic-chunking.integration.test.ts` - Integration tests

**Database**:
- `supabase/migrations/048_add_chunker_type.sql` - Add `chunker_type` column, `document_chunkers` table

---

## Phase 1 Quick-Start Guide

### Setup (15 minutes)

```bash
# Install Chonkie
cd worker
pip install chonkie[semantic]

# Verify installation
python -c "from chonkie import SemanticChunker; print('âœ“ Chonkie installed')"

# Install Ollama (if not already installed)
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen2.5:32b-instruct-q4_K_M

# Verify Ollama
curl -s http://127.0.0.1:11434/api/version
```

### Implementation Tasks (12-16 hours)

**Task 1**: Create `chonkie_pipeline.py` (3 hours)
- Python script that uses Chonkie Pipeline API
- Takes markdown input via stdin
- Outputs JSON chunks to stdout
- Supports SemanticChunker configuration

**Task 2**: Create `chonkie-pipeline.ts` (2 hours)
- TypeScript subprocess handler
- Spawns Python script, handles IPC
- Parses JSON output
- Error handling and timeout

**Task 3**: Create `metadata-transfer.ts` (4 hours)
- Build character offset â†’ metadata map from Docling chunks
- Query map for each Chonkie chunk range
- Transfer pages, headings, bboxes
- Calculate confidence scores (exact/high/medium/synthetic)

**Task 4**: Integrate into `pdf-processor.ts` (2 hours)
- Add semantic chunking branch
- Wire up quality gates (abort if >15% synthetic)
- Integration tests

**Task 5**: Validation on 3 sample documents (3 hours)
- Small (50 pages), medium (200 pages), large (500 pages)
- Measure metadata quality metrics
- **NEW**: Test annotation recovery (simulate edits)
- **NEW**: Test connection remapping
- Compare vs hybrid chunker

**Task 6**: Document findings (1 hour)
- Success criteria met?
- Metadata quality â‰¥85%?
- Annotation recovery â‰¥90%?
- Connection remapping â‰¥95%?
- Decision: proceed to Phase 2 or iterate?

---

## Success Criteria (Phase 1)

### Must Have âœ…
- [ ] Chonkie pipeline produces valid chunks with offsets
- [ ] Metadata quality â‰¥85% (exact + high confidence)
- [ ] Synthetic chunks <15%
- [ ] Processing time <30 minutes for 500 pages
- [ ] **Annotation recovery â‰¥90%** (matches hybrid)
- [ ] **Connection remapping â‰¥95%** (embedding-based)

### Nice to Have ðŸŽ¯
- [ ] Metadata quality â‰¥90% (exceed target)
- [ ] Synthetic chunks <10%
- [ ] Annotation recovery â‰¥95% (exceeds hybrid)
- [ ] Visual chunk boundary analysis (do semantic splits look better?)

### Decision Gate
**Proceed to Phase 2 if**:
- All "Must Have" criteria met
- No major technical blockers discovered
- Qualitative assessment: semantic boundaries feel better than structural

**Iterate on Phase 1 if**:
- Metadata quality 80-85% (tune parameters)
- Annotation recovery 85-90% (investigate edge cases)
- Processing time 30-35 min (optimize)

**Abandon if**:
- Metadata quality <80% (fundamental issue)
- Annotation recovery <85% (breaks user experience)
- Processing time >40 min (too slow)
- No clear quality improvement visible

---

## Key Questions to Answer in Phase 1

### Technical Validation
1. **Does offset-based metadata transfer maintain â‰¥85% quality?**
   - Measure: exact/high/medium/synthetic distribution
   - Target: <15% synthetic chunks

2. **Do recovery systems handle semantic boundaries?**
   - Test: Create annotations â†’ edit markdown â†’ reprocess â†’ measure recovery
   - Target: â‰¥90% success rate (matching hybrid)

3. **Does semantic coherence improve embedding stability?**
   - Test: Create connections â†’ reprocess â†’ measure remapping
   - Hypothesis: â‰¥95% auto-remap (better than hybrid)

### Quality Assessment
4. **Do semantic boundaries look better than page boundaries?**
   - Manual review: Read 20 random chunks from each
   - Question: Do semantic chunks feel more coherent?

5. **Are chunk boundaries at natural semantic breaks?**
   - Manual review: Check 10 chunk boundaries
   - Question: Do splits happen at topic shifts, not mid-thought?

### Performance Validation
6. **Is processing time acceptable?**
   - Measure: End-to-end time for 50, 200, 500 page books
   - Target: <30 minutes for 500 pages

7. **What's the chunk count variance?**
   - Measure: Hybrid vs Semantic chunk counts
   - Target: 0.8-1.2x ratio (Â±20%)

---

## Important Context for Next Session

### Why This Matters
Rhizome is fundamentally about **finding non-obvious connections between ideas**. Semantic chunking that groups related concepts is *exactly* what could unlock cross-domain thematic bridges. Right now, HybridChunker might split a single argument across pages, fragmenting the conceptual unit. SemanticChunker keeps complete thoughts together.

### Your Competitive Advantage
Most document processors can't do semantic chunking because they rely on stable chunk IDs or fixed offsets. Rhizome's recovery systems are **character-offset based with fuzzy matching**, treating semantic boundary shifts as just another text transformation. This wasn't designed for semantic chunking, but it's the **perfect foundation** for it.

### The Big Opportunity
If Phase 1 validates the approach:
- **Variant B** (Overlap): +5-10% additional improvement by making chunk relationships explicit
- **Variant C** (Slumber): +10-20% via LLM reasoning about conceptual arcs

Combined: **25-40% improvement in connection detection** vs current HybridChunker.

### Risk Mitigation
- Incremental phases (can stop anytime)
- A/B testing validates improvement before shipping
- Recovery systems provide safety net
- User-selectable (no forced migration)
- Reversible (can switch back to hybrid)

---

## Related Documentation

**Primary Documents**:
- `docs/prps/chonkie-semantic-chunking-integration.md` - Full PRP (this session's output)
- `docs/PROCESSING_PIPELINE.md` - Current dual-mode pipeline architecture
- `docs/local-pipeline-setup.md` - LOCAL mode setup guide

**Recovery Systems** (critical reading):
- `worker/handlers/recover-annotations.ts` - Annotation recovery implementation
- `worker/handlers/remap-connections.ts` - Connection remapping implementation

**External Resources**:
- Chonkie Docs: https://docs.chonkie.ai/common/welcome
- Chonkie GitHub: https://github.com/chonkie-inc/chonkie
- Chonkie Pipelines: https://docs.chonkie.ai/oss/pipelines
- SemanticChunker: https://docs.chonkie.ai/oss/chunkers/semantic-chunker
- SlumberChunker: https://docs.chonkie.ai/oss/chunkers/slumber-chunker

---

## Next Steps (Immediate Action Items)

1. **Review PRP** (30 min)
   - Read `docs/prps/chonkie-semantic-chunking-integration.md`
   - Approve or request changes

2. **Setup Environment** (15 min)
   - Install Chonkie: `pip install chonkie[semantic]`
   - Verify Ollama running

3. **Begin Phase 1 Prototype** (12-16 hours)
   - Create `chonkie_pipeline.py`
   - Create `chonkie-pipeline.ts`
   - Create `metadata-transfer.ts`
   - Integrate into `pdf-processor.ts`
   - Test on 3 sample documents
   - **Validate annotation recovery** (critical!)
   - **Validate connection remapping** (critical!)

4. **Decision Gate** (1 hour)
   - Review metrics against success criteria
   - Decide: proceed to Phase 2, iterate, or abandon

---

## Open Questions & Future Considerations

### Phase 1 Questions
- What's the optimal overlap size? (50, 75, or 100 tokens?)
- Should overlap be trailing, leading, or bidirectional?
- Does Model2Vec (500x faster) have acceptable quality for embeddings?
- Can we detect which documents benefit most from semantic chunking?

### Phase 3+ Questions (if Phase 1 succeeds)
- Does OverlapRefinery actually improve thematic bridge detection?
- Is SlumberChunker worth the added complexity?
- Should this be default for narrative content?
- Can we auto-detect document type (technical vs narrative) and choose chunker?

### Architecture Questions
- Should chunks store overlap metadata separately or inline?
- How do overlapping chunks affect storage size?
- Does the 3-engine system need tuning for overlapping chunks?
- Can we visualize chunk boundaries in the reader UI for debugging?

---

## Session Artifacts

**Created**:
- `docs/prps/chonkie-semantic-chunking-integration.md` (comprehensive PRP, ~1200 lines)
- `docs/prps/chonkie-integration-handoff.md` (this document)

**Research Conducted**:
- Deep-dive into Chonkie architecture
- SemanticChunker algorithm analysis
- SlumberChunker capabilities assessment
- Pipeline (CHOMP) architecture study
- OverlapRefinery functionality research
- Performance benchmarks analysis

**Key Insights Documented**:
- Rhizome's recovery systems are perfectly suited for semantic chunking
- Pipeline composition > single chunker choice
- Overlap refinement could amplify connection detection
- Incremental validation reduces risk

---

## Contact & Collaboration Notes

**Decision Authority**: @topher
**Timeline**: 4-5 weeks (38-50 hours) for full implementation
**Current Phase**: Phase 0 (Research & Planning) â†’ Ready for Phase 1
**Risk Level**: Low (recovery systems provide safety net)
**Confidence Level**: High (85%) - architecture analysis is solid, empirical validation needed

**If blocked**:
1. Check recovery system implementations (`recover-annotations.ts`, `remap-connections.ts`)
2. Review processing pipeline docs (`docs/PROCESSING_PIPELINE.md`)
3. Reference Chonkie docs (https://docs.chonkie.ai)
4. Validate metadata quality gates (â‰¥85% threshold)

**Success indicators for next session**:
- Chonkie pipeline running end-to-end
- Metadata quality metrics calculated
- Annotation recovery validated
- Decision: proceed/iterate/abandon

---

**End of Handoff Summary**

**Status**: Ready to begin Phase 1 prototype
**Recommendation**: High confidence this will improve connection detection, low risk due to recovery systems
**Next Session Goal**: Validate technical feasibility via 3-document test

Good luck! ðŸš€
