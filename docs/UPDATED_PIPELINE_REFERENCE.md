


# Rhizome Document Processing Pipeline Reference

**Version:** 2.0 (Fully Local, Docling-Powered)  
**Last Updated:** January 2025  
**Status:** Production-Ready

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture Philosophy](#architecture-philosophy)
3. [Technology Stack](#technology-stack)
4. [Processing Pipelines](#processing-pipelines)
   - [PDF Pipeline](#pdf-pipeline)
   - [EPUB Pipeline](#epub-pipeline)
   - [Markdown Pipeline](#markdown-pipeline)
   - [Text Pipeline](#text-pipeline)
5. [Key Concepts](#key-concepts)
6. [Database Schema](#database-schema)
7. [Performance & Costs](#performance--costs)
8. [Troubleshooting](#troubleshooting)

---

## Overview

Rhizome transforms documents into a rich knowledge graph with **100% local processing**. Every documentâ€”whether PDF, EPUB, Markdown, or plain textâ€”goes through a multi-stage pipeline that:

1. **Extracts** content while preserving structure
2. **Cleans** artifacts and formatting issues
3. **Chunks** intelligently at semantic boundaries
4. **Enriches** with AI-generated metadata (themes, concepts, importance)
5. **Embeds** for semantic search
6. **Connects** chunks across your entire library

**Key Guarantees:**
- âœ… 100% chunk recovery (never lose metadata)
- âœ… 100% local processing (zero API costs, complete privacy)
- âœ… Structure preservation (headings, pages, hierarchy)
- âœ… Type-safe outputs (PydanticAI validation)
- âœ… Resumable processing (checkpoint system)

---

## Architecture Philosophy

### The Hybrid Model: Display vs. Connections

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISPLAY LAYER                            â”‚
â”‚  What You Read: Clean, continuous markdown (content.md)     â”‚
â”‚  - Natural reading flow                                      â”‚
â”‚  - No chunk boundaries visible                              â”‚
â”‚  - Portable (standard markdown)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONNECTION LAYER                           â”‚
â”‚  What The System Uses: Semantic chunks (database)           â”‚
â”‚  - Rich metadata (themes, concepts, importance)             â”‚
â”‚  - Precise offsets (for annotations, connections)           â”‚
â”‚  - Structural data (pages, headings, hierarchy)             â”‚
â”‚  - Embeddings (for semantic search)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

The Bridge: Bulletproof chunk matching maps between layers
```

**Why This Works:**
- You read **clean markdown** without interruptions
- System operates on **semantic chunks** with rich metadata
- Chunks are **metadata overlays** on the display text
- **Citations work** (chunks preserve page/section numbers)
- **Connections work** (chunks have precise offsets)
- **Never lose data** (even if chunk position is approximate)

### File-Over-App Principle

```
storage/
â”œâ”€â”€ {userId}/{documentId}/
â”‚   â”œâ”€â”€ content.md              â† Clean markdown (what you read)
â”‚   â”œâ”€â”€ annotations.json        â† Your highlights/notes
â”‚   â””â”€â”€ metadata.json           â† Document properties

database/
â”œâ”€â”€ chunks                      â† Semantic units with metadata
â”œâ”€â”€ connections                 â† Relationships between chunks
â””â”€â”€ documents                   â† Document-level data + structure
```

**Benefits:**
- Your documents are **portable markdown files**
- Can edit in any text editor
- Version control with Git
- Obsidian integration built-in
- Database is **enrichment layer**, not lock-in

---

## Technology Stack

### Core Technologies

| Component | Technology | Purpose | Why This Choice |
|-----------|-----------|---------|-----------------|
| **PDF Extraction** | Docling | Extract PDFs to structured markdown | 100% local, preserves structure, no hallucinations |
| **EPUB Extraction** | JSZip + Docling | Parse EPUB HTML, extract structure | Native HTML understanding, preserves semantics |
| **Chunking** | Docling HybridChunker | Create semantic chunks | Respects document structure, deterministic, free |
| **LLM Processing** | Qwen 32B (Ollama) | Cleanup, metadata extraction, connections | Powerful local model, structured outputs |
| **Structured Outputs** | PydanticAI | Type-safe LLM responses | Automatic validation, retry on errors, no JSON parsing issues |
| **Embeddings** | sentence-transformers | Semantic search | Fast local embeddings, good quality |
| **Vector Search** | pgvector (Supabase) | Similarity search | Mature, PostgreSQL-native, efficient |
| **Validation** | Fuzzy matching + LLM | Chunk position recovery | 5-layer failsafe ensures 100% recovery |

### Model Specifications

```yaml
Qwen 32B (via Ollama):
  Parameters: 32 billion
  Quantization: Q4_K_M (recommended)
  RAM Required: 20-24GB
  Speed: ~30 tokens/sec on M3 Max
  Use Cases: Cleanup, metadata extraction, connection detection

sentence-transformers/all-MiniLM-L6-v2:
  Dimensions: 384
  Speed: ~2000 chunks/sec
  Quality: Good for most use cases
  
sentence-transformers/all-mpnet-base-v2:
  Dimensions: 768
  Speed: ~1000 chunks/sec
  Quality: Better, recommended for production
```

---

## Processing Pipelines

### PDF Pipeline

```
ğŸ“š PDF DOCUMENT PROCESSING (100% LOCAL)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: DOCLING EXTRACTION (15-50%)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  PDF file from Supabase Storage                              â”‚
â”‚ Action: Docling processes PDF with Python                           â”‚
â”‚         â”œâ”€ Extract markdown (structure preserved)                   â”‚
â”‚         â”œâ”€ Document structure (sections, headings, hierarchy)       â”‚
â”‚         â”œâ”€ HybridChunker (512 tokens, semantic boundaries)          â”‚
â”‚         â””â”€ Rich metadata per chunk:                                 â”‚
â”‚             â€¢ page_start, page_end (for citations)                  â”‚
â”‚             â€¢ heading, heading_path (for navigation)                â”‚
â”‚             â€¢ heading_level (hierarchy depth)                       â”‚
â”‚             â€¢ bboxes (PDF coordinates for highlighting)             â”‚
â”‚         â”œâ”€ Optional extraction:                                     â”‚
â”‚         â”‚   â€¢ Tables (structured data)                              â”‚
â”‚         â”‚   â€¢ Figures (captions + positions)                        â”‚
â”‚         â”‚   â€¢ Citations (bibliography)                              â”‚
â”‚         â”‚   â””â”€ Code blocks (syntax preserved)                       â”‚
â”‚ Output: originalMarkdown, doclingStructure, doclingChunks           â”‚
â”‚ Cost:   $0 (100% local)                                             â”‚
â”‚ Time:   ~9 minutes for 500 pages                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: LOCAL REGEX CLEANUP (50-55%)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  originalMarkdown from Docling                               â”‚
â”‚ Action: Deterministic cleanup with regex                            â”‚
â”‚         â”œâ”€ Remove page numbers, headers, footers                    â”‚
â”‚         â”œâ”€ Fix obvious hyphenation (end-of-line breaks)             â”‚
â”‚         â”œâ”€ Normalize whitespace (consistent spacing)                â”‚
â”‚         â””â”€ Skip heading generation (Docling provides structure)     â”‚
â”‚ Output: regexCleanedMarkdown                                         â”‚
â”‚ Cost:   $0 (local regex)                                            â”‚
â”‚ Time:   <1 second                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OPTIONAL CHECKPOINT: Review Docling Extraction                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User Setting: reviewDoclingExtraction = true                         â”‚
â”‚ Action:       Export to Obsidian for manual review                  â”‚
â”‚ User Choice:  â”œâ”€ Continue with LLM cleanup                          â”‚
â”‚               â””â”€ Skip LLM cleanup (use regex-cleaned version)       â”‚
â”‚ Status:       Pipeline pauses until user decision                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: LOCAL LLM CLEANUP (55-70%) [Optional]                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User Setting: cleanMarkdown = true (default)                         â”‚
â”‚ Input:        regexCleanedMarkdown                                   â”‚
â”‚ Action:       Multi-pass cleanup with Qwen 32B (Ollama)             â”‚
â”‚               Pass 1: Remove artifacts (headers, footers, noise)    â”‚
â”‚                       â”œâ”€ Identify and remove running headers        â”‚
â”‚                       â”œâ”€ Remove page number fragments               â”‚
â”‚                       â””â”€ Clean OCR errors                           â”‚
â”‚               Pass 2: Fix formatting issues                          â”‚
â”‚                       â”œâ”€ Merge incorrectly split paragraphs         â”‚
â”‚                       â”œâ”€ Fix hyphenation across lines               â”‚
â”‚                       â””â”€ Normalize inconsistent spacing             â”‚
â”‚               Pass 3: Validate and polish                            â”‚
â”‚                       â”œâ”€ Check markdown structure                   â”‚
â”‚                       â”œâ”€ Verify heading hierarchy                   â”‚
â”‚                       â””â”€ Final quality pass                         â”‚
â”‚         Strategy: Heading-split for large docs (>100K chars)        â”‚
â”‚                  â”œâ”€ Split at ## headings before cleanup             â”‚
â”‚                  â”œâ”€ Clean each section independently               â”‚
â”‚                  â””â”€ Join sections (no stitching complexity)         â”‚
â”‚         Validation: PydanticAI ensures structured cleanup           â”‚
â”‚ Output: cleanedMarkdown (polished, may differ from original)        â”‚
â”‚ Cost:   $0 (local Ollama)                                           â”‚
â”‚ Time:   ~10-30 minutes (depends on size)                            â”‚
â”‚ Quality: Removes 90%+ of artifacts while preserving content         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OPTIONAL CHECKPOINT: Review LLM Cleanup                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User Setting: reviewBeforeChunking = true                            â”‚
â”‚ Action:       Export cleaned markdown to Obsidian                   â”‚
â”‚ Purpose:      Verify quality before expensive enrichment            â”‚
â”‚ Status:       Pipeline pauses until user approves                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: BULLETPROOF CHUNK MATCHING (70-75%)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Problem: doclingChunks have offsets for originalMarkdown            â”‚
â”‚          But we display cleanedMarkdown (different text!)           â”‚
â”‚ Solution: 5-layer failsafe matching system                          â”‚
â”‚                                                                       â”‚
â”‚ Phase 1: Enhanced Fuzzy Matching (85% success)                     â”‚
â”‚   Method: String similarity algorithms                               â”‚
â”‚   â”œâ”€ Exact match: Find identical text                              â”‚
â”‚   â”œâ”€ Normalized match: Ignore whitespace differences               â”‚
â”‚   â”œâ”€ Multi-anchor search: Use start/middle/end phrases             â”‚
â”‚   â””â”€ Sliding window: Find best matching window                     â”‚
â”‚   Cost: $0 (local string operations)                                â”‚
â”‚   Time: ~5 seconds                                                   â”‚
â”‚   Result: 85% of chunks matched with high confidence                â”‚
â”‚                                                                       â”‚
â”‚ Phase 2: Embedding-Based Matching (98% cumulative)                 â”‚
â”‚   Method: Semantic similarity via embeddings                         â”‚
â”‚   â”œâ”€ Embed all chunk contents                                      â”‚
â”‚   â”œâ”€ Create sliding windows of cleaned markdown                    â”‚
â”‚   â”œâ”€ Embed all windows                                             â”‚
â”‚   â”œâ”€ Find best matching window via cosine similarity               â”‚
â”‚   â””â”€ Threshold: 0.85+ for confidence                               â”‚
â”‚   Cost: $0 (local sentence-transformers)                            â”‚
â”‚   Time: ~30 seconds                                                  â”‚
â”‚   Result: +13% chunks matched (98% total)                           â”‚
â”‚                                                                       â”‚
â”‚ Phase 3: LLM-Assisted Matching (99.9% cumulative)                  â”‚
â”‚   Method: Qwen 32B semantic understanding                           â”‚
â”‚   â”œâ”€ For remaining unmatched chunks                                â”‚
â”‚   â”œâ”€ Give LLM the chunk content + search window                    â”‚
â”‚   â”œâ”€ LLM finds semantic match (handles rephrasing)                 â”‚
â”‚   â””â”€ Returns position with confidence level                        â”‚
â”‚   Cost: $0 (local Ollama)                                           â”‚
â”‚   Time: ~15 seconds (~7 chunks)                                     â”‚
â”‚   Result: +1.9% chunks matched (99.9% total)                        â”‚
â”‚                                                                       â”‚
â”‚ Phase 4: Anchor Interpolation (100% guaranteed)                    â”‚
â”‚   Method: Position interpolation between neighbors                  â”‚
â”‚   â”œâ”€ For any remaining unmatched chunks                            â”‚
â”‚   â”œâ”€ Find nearest matched neighbors (before/after)                 â”‚
â”‚   â”œâ”€ Interpolate position based on chunk index                     â”‚
â”‚   â”œâ”€ Mark as "synthetic" with confidence tracking                  â”‚
â”‚   â””â”€ PRESERVE all metadata (pages, themes intact)                  â”‚
â”‚   Cost: $0 (local math)                                             â”‚
â”‚   Time: Instant                                                      â”‚
â”‚   Result: 100% chunk recovery (no data loss ever)                   â”‚
â”‚                                                                       â”‚
â”‚ Output: rematchedChunks with:                                        â”‚
â”‚         â”œâ”€ NEW offsets (in cleanedMarkdown)                         â”‚
â”‚         â”œâ”€ OLD metadata (pages, headings, bboxes)                   â”‚
â”‚         â””â”€ Confidence tracking (exact/high/medium/synthetic)        â”‚
â”‚ Guarantee: 100% chunk recovery, metadata always preserved           â”‚
â”‚ Cost: ~$0.06 total (mostly free, small embedding cost)             â”‚
â”‚ Time: ~50 seconds                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 5: LOCAL LLM METADATA ENRICHMENT (75-90%)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  rematchedChunks (now with correct offsets)                  â”‚
â”‚ Action: Extract semantic metadata using Qwen 32B + PydanticAI       â”‚
â”‚         For each chunk:                                              â”‚
â”‚         â”œâ”€ Themes: 2-3 key topics (e.g., "entropy", "thermodynamics")â”‚
â”‚         â”œâ”€ Concepts: Entities with importance scores                â”‚
â”‚         â”‚   Example: [{"text": "V-2 rocket", "importance": 0.9}]    â”‚
â”‚         â”œâ”€ Importance: 0-1 score (how central to document)          â”‚
â”‚         â”œâ”€ Summary: One-sentence description                        â”‚
â”‚         â”œâ”€ Emotional Tone: {polarity, primaryEmotion, intensity}    â”‚
â”‚         â””â”€ Domain: Primary field (science, history, etc.)           â”‚
â”‚                                                                       â”‚
â”‚ PydanticAI Benefits:                                                 â”‚
â”‚   âœ… Type-safe outputs (no JSON parsing errors)                     â”‚
â”‚   âœ… Automatic retries (LLM fixes validation failures)              â”‚
â”‚   âœ… Clear error messages (knows exactly what's wrong)              â”‚
â”‚   âœ… Schema enforcement (guaranteed structure)                      â”‚
â”‚                                                                       â”‚
â”‚ Processing: Batch 10 chunks at a time for efficiency                â”‚
â”‚                                                                       â”‚
â”‚ Preserved: All Docling structural metadata                          â”‚
â”‚   â”œâ”€ page_start, page_end (unchanged)                              â”‚
â”‚   â”œâ”€ heading, heading_path, heading_level (unchanged)              â”‚
â”‚   â””â”€ bboxes (unchanged)                                            â”‚
â”‚                                                                       â”‚
â”‚ Output: enrichedChunks (structure + semantics combined)             â”‚
â”‚         Each chunk now has:                                          â”‚
â”‚         â”œâ”€ Docling metadata (structure, pages, headings)            â”‚
â”‚         â””â”€ AI metadata (themes, concepts, importance)               â”‚
â”‚ Cost:   $0 (local Ollama + PydanticAI)                             â”‚
â”‚ Time:   ~10-20 minutes (depends on chunk count)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 6: LOCAL EMBEDDING GENERATION (90-95%)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  enrichedChunks (with all metadata)                          â”‚
â”‚ Action: Generate embeddings for semantic search                     â”‚
â”‚         Model: sentence-transformers/all-mpnet-base-v2              â”‚
â”‚                (768 dimensions, better quality)                      â”‚
â”‚         Alternative: all-MiniLM-L6-v2 (384 dims, faster)            â”‚
â”‚         Batch: Process 100 chunks at a time                         â”‚
â”‚         Validation: Verify all vectors are 768-dimensional          â”‚
â”‚ Output: embeddings[] (one per chunk)                                â”‚
â”‚ Cost:   $0 (local sentence-transformers)                            â”‚
â”‚ Time:   ~1-2 minutes for 380 chunks                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 7: SAVE TO DATABASE (95-98%)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Save cleanedMarkdown:                                                â”‚
â”‚   Location: storage/{userId}/{documentId}/content.md                â”‚
â”‚   Purpose:  What users read (display layer)                         â”‚
â”‚                                                                       â”‚
â”‚ Save document metadata:                                              â”‚
â”‚   â”œâ”€ structure: Docling's hierarchical sections                     â”‚
â”‚   â”œâ”€ tables: Extracted tables (if any)                             â”‚
â”‚   â”œâ”€ figures: Extracted figures with captions                       â”‚
â”‚   â”œâ”€ citations: Bibliography entries                                â”‚
â”‚   â”œâ”€ matching_stats: Chunk recovery statistics                     â”‚
â”‚   â””â”€ warnings: Any synthetic chunks flagged                         â”‚
â”‚                                                                       â”‚
â”‚ Insert enrichedChunks to database:                                   â”‚
â”‚   Each chunk contains:                                               â”‚
â”‚   â”œâ”€ content: The actual text                                       â”‚
â”‚   â”œâ”€ start_offset, end_offset: Position in cleanedMarkdown         â”‚
â”‚   â”œâ”€ chunk_index: Sequential order                                  â”‚
â”‚   â”œâ”€ Docling metadata:                                              â”‚
â”‚   â”‚   â”œâ”€ page_start, page_end                                      â”‚
â”‚   â”‚   â”œâ”€ heading, heading_path[], heading_level                    â”‚
â”‚   â”‚   â””â”€ bboxes[] (for PDF highlighting)                           â”‚
â”‚   â”œâ”€ AI metadata:                                                   â”‚
â”‚   â”‚   â”œâ”€ themes[]                                                   â”‚
â”‚   â”‚   â”œâ”€ concepts[] (with importance scores)                       â”‚
â”‚   â”‚   â”œâ”€ importance_score                                           â”‚
â”‚   â”‚   â”œâ”€ summary                                                    â”‚
â”‚   â”‚   â”œâ”€ emotional_metadata                                         â”‚
â”‚   â”‚   â””â”€ domain                                                     â”‚
â”‚   â”œâ”€ embedding: vector(768) for semantic search                    â”‚
â”‚   â””â”€ position_confidence: exact/high/medium/synthetic              â”‚
â”‚                                                                       â”‚
â”‚ Result: Document ready for reading, searching, connecting           â”‚
â”‚ Cost:   $0 (database writes)                                        â”‚
â”‚ Time:   ~30 seconds                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 8: CONNECTION DETECTION (Async, separate background job)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Trigger: Queued as separate job (doesn't block document)            â”‚
â”‚ Purpose: Find relationships between chunks across entire library    â”‚
â”‚                                                                       â”‚
â”‚ 3-Engine Detection System (all local):                              â”‚
â”‚                                                                       â”‚
â”‚ Engine 1: Semantic Similarity                                        â”‚
â”‚   Method: Vector cosine similarity (pgvector)                       â”‚
â”‚   â”œâ”€ Query: For each chunk, find similar chunks via embedding      â”‚
â”‚   â”œâ”€ Threshold: 0.70+ similarity                                   â”‚
â”‚   â”œâ”€ Finds: "These say the same thing"                             â”‚
â”‚   â””â”€ Example: Two chunks both discussing entropy                   â”‚
â”‚   Cost: $0 (database vector search)                                 â”‚
â”‚   Speed: <1 second per chunk                                        â”‚
â”‚                                                                       â”‚
â”‚ Engine 2: Contradiction Detection                                   â”‚
â”‚   Method: Metadata analysis + Qwen 32B                              â”‚
â”‚   â”œâ”€ Filter: Chunks with overlapping concepts                      â”‚
â”‚   â”œâ”€ Check: Opposite emotional polarity                            â”‚
â”‚   â”‚   (polarity > 0.3 vs polarity < -0.3)                          â”‚
â”‚   â”œâ”€ Verify: Ask Qwen 32B if they actually contradict              â”‚
â”‚   â””â”€ Finds: "These disagree about the same topic"                  â”‚
â”‚   Example: Optimistic view vs pessimistic view of AI               â”‚
â”‚   Cost: $0 (local Ollama, filtered candidates)                     â”‚
â”‚   Speed: ~100 comparisons per document                              â”‚
â”‚                                                                       â”‚
â”‚ Engine 3: Thematic Bridge                                           â”‚
â”‚   Method: Cross-domain concept matching via Qwen 32B               â”‚
â”‚   Aggressive Filtering (reduces 160k to ~200 comparisons):         â”‚
â”‚   â”œâ”€ Importance > 0.6 (only important chunks)                      â”‚
â”‚   â”œâ”€ Cross-document only (no self-connections)                     â”‚
â”‚   â”œâ”€ Different domains (cross-pollination)                         â”‚
â”‚   â”œâ”€ Concept overlap 0.2-0.7 (sweet spot)                          â”‚
â”‚   â””â”€ Top 15 candidates per chunk                                   â”‚
â”‚   Action: Qwen 32B analyzes filtered pairs                          â”‚
â”‚   Finds: "These connect different domains via shared concept"       â”‚
â”‚   Example: "paranoia" in literature â†” "surveillance" in tech       â”‚
â”‚   Cost: $0 (local Ollama, ~200 calls)                              â”‚
â”‚   Speed: ~30-60 minutes for full analysis                           â”‚
â”‚                                                                       â”‚
â”‚ Output: connections[] stored in database                             â”‚
â”‚         Each connection has:                                         â”‚
â”‚         â”œâ”€ source_chunk_id, target_chunk_id                         â”‚
â”‚         â”œâ”€ type: semantic/contradiction/thematic_bridge             â”‚
â”‚         â”œâ”€ strength: 0-1 confidence score                           â”‚
â”‚         â”œâ”€ metadata: Why they're connected                          â”‚
â”‚         â””â”€ discovered_at: timestamp                                 â”‚
â”‚                                                                       â”‚
â”‚ Personal Scoring (applied at display time):                         â”‚
â”‚   finalScore = 0.25 Ã— semantic +                                    â”‚
â”‚                0.40 Ã— contradiction +  (highest weight)             â”‚
â”‚                0.35 Ã— thematic_bridge                                â”‚
â”‚                                                                       â”‚
â”‚ Total Cost: $0 (100% local)                                         â”‚
â”‚ Total Time: ~30-60 minutes per document                             â”‚
â”‚ Total Connections: ~50-100 per document (filtered for quality)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š TOTAL PIPELINE COST: $0 (100% local processing)
â±ï¸  TOTAL PIPELINE TIME: ~40-80 minutes for 500-page book
ğŸ’¾ MODEL REQUIREMENTS: ~20GB RAM for Qwen 32B
âœ… SUCCESS RATE: 100% chunk recovery guaranteed
```

---

### EPUB Pipeline

```
ğŸ“• EPUB DOCUMENT PROCESSING (100% LOCAL, UNIFIED WITH PDF)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: EPUB HTML EXTRACTION (15-25%)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  EPUB file from Supabase Storage                             â”‚
â”‚ Action: Parse EPUB as ZIP archive                                   â”‚
â”‚         â”œâ”€ Read META-INF/container.xml (find content.opf)           â”‚
â”‚         â”œâ”€ Parse content.opf (metadata + spine)                     â”‚
â”‚         â”œâ”€ Extract HTML/XHTML files in spine order                  â”‚
â”‚         â””â”€ Concatenate HTML preserving reading order                â”‚
â”‚         Metadata extraction:                                         â”‚
â”‚         â”œâ”€ Title, author, publisher (from OPF)                      â”‚
â”‚         â”œâ”€ Cover image (if present)                                 â”‚
â”‚         â”œâ”€ Table of Contents (if present)                           â”‚
â”‚         â””â”€ Chapter/section markers                                  â”‚
â”‚ Output: unifiedHtml, metadata                                        â”‚
â”‚ Cost:   $0 (local ZIP/XML parsing)                                  â”‚
â”‚ Time:   <30 seconds                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: DOCLING HTML PROCESSING (25-50%)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  unifiedHtml (concatenated from EPUB)                        â”‚
â”‚ Action: Feed HTML directly to Docling (no markdown conversion!)     â”‚
â”‚         Docling understands HTML semantic tags:                      â”‚
â”‚         â”œâ”€ <h1>, <h2>, <h3> â†’ heading hierarchy                    â”‚
â”‚         â”œâ”€ <p> â†’ paragraphs                                         â”‚
â”‚         â”œâ”€ <ul>, <ol>, <li> â†’ lists                                â”‚
â”‚         â”œâ”€ <blockquote> â†’ quotes                                    â”‚
â”‚         â”œâ”€ <table> â†’ structured tables                              â”‚
â”‚         â”œâ”€ <code>, <pre> â†’ code blocks                             â”‚
â”‚         â”œâ”€ <em>, <strong> â†’ emphasis                                â”‚
â”‚         â””â”€ CSS classes (preserved for special handling)            â”‚
â”‚                                                                       â”‚
â”‚         HybridChunker creates structural chunks:                     â”‚
â”‚         â”œâ”€ Tokenizer: sentence-transformers/all-mpnet-base-v2       â”‚
â”‚         â”œâ”€ Max tokens: 512                                           â”‚
â”‚         â”œâ”€ Respects HTML block boundaries                           â”‚
â”‚         â”œâ”€ Never splits mid-paragraph or mid-list                   â”‚
â”‚         â””â”€ Preserves heading hierarchy                              â”‚
â”‚                                                                       â”‚
â”‚         Works WITHOUT chapters:                                      â”‚
â”‚         â”œâ”€ Poetry books â†’ chunks by paragraph/stanza                â”‚
â”‚         â”œâ”€ Chapter books â†’ full hierarchy preserved                 â”‚
â”‚         â”œâ”€ Continuous narratives â†’ sequential markers               â”‚
â”‚         â””â”€ Multi-file EPUBs â†’ unified then chunked                  â”‚
â”‚                                                                       â”‚
â”‚         Benefits over HTMLâ†’MDâ†’Docling:                               â”‚
â”‚         âœ… Better structure (semantic HTML preserved)               â”‚
â”‚         âœ… No conversion artifacts (entities intact)                â”‚
â”‚         âœ… Richer metadata (CSS classes, attributes)                â”‚
â”‚         âœ… Better tables (full structure preserved)                 â”‚
â”‚         âœ… Footnote tracking (ref relationships)                    â”‚
â”‚                                                                       â”‚
â”‚ Output: cleanMarkdown (from Docling's conversion)                   â”‚
â”‚         doclingStructure (sections, hierarchy)                       â”‚
â”‚         doclingChunks with metadata:                                 â”‚
â”‚         â”œâ”€ heading, heading_path[], heading_level                   â”‚
â”‚         â”œâ”€ section_marker (chapter_001, part_003, etc.)             â”‚
â”‚         â”œâ”€ NO page_start/page_end (EPUBs don't have pages)          â”‚
â”‚         â””â”€ tokens (for embedding size validation)                   â”‚
â”‚ Cost:   $0 (local Docling)                                          â”‚
â”‚ Time:   ~2-5 minutes                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: LOCAL LLM CLEANUP (50-65%) [Optional]                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User Setting: cleanMarkdown = true (default)                         â”‚
â”‚ Input:        Docling's markdown output                             â”‚
â”‚ Action:       Multi-pass Qwen 32B cleanup                           â”‚
â”‚               â”œâ”€ Remove HTML artifacts (if any)                     â”‚
â”‚               â”œâ”€ Fix formatting issues                              â”‚
â”‚               â””â”€ Polish structure                                    â”‚
â”‚ Note:         Usually needs LESS cleanup than PDF                    â”‚
â”‚               (HTML is cleaner than OCR)                             â”‚
â”‚ Output: cleanedMarkdown                                              â”‚
â”‚ Cost:   $0 (local Ollama)                                           â”‚
â”‚ Time:   ~3-8 minutes (faster than PDF)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: BULLETPROOF CHUNK MATCHING (65-70%)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Same 5-layer failsafe as PDF pipeline                               â”‚
â”‚ Maps doclingChunks â†’ cleanedMarkdown                                â”‚
â”‚ Preserves metadata:                                                  â”‚
â”‚   â”œâ”€ heading_path (full hierarchy)                                  â”‚
â”‚   â”œâ”€ heading_level                                                   â”‚
â”‚   â”œâ”€ section_marker (for citations)                                 â”‚
â”‚   â””â”€ NO page numbers (use section markers instead)                  â”‚
â”‚ 100% recovery guaranteed                                             â”‚
â”‚ Cost: $0 (all local)                                                â”‚
â”‚ Time: ~50 seconds                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGES 5-8: SAME AS PDF PIPELINE                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Local LLM metadata enrichment (Qwen 32B + PydanticAI)            â”‚
â”‚ â€¢ Local embedding generation (sentence-transformers)                â”‚
â”‚ â€¢ Save to database (chapter metadata instead of pages)             â”‚
â”‚ â€¢ Local connection detection (async, 3 engines)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š TOTAL PIPELINE COST: $0 (100% local)
â±ï¸  TOTAL PIPELINE TIME: ~15-30 minutes (faster than PDF!)
âœ… WORKS WITHOUT CHAPTERS: Yes, any EPUB structure supported
```

**EPUB Citation Format:**
```
PDF:  "Quote text..." (p. 47)
EPUB: "Quote text..." (Chapter 3, Section 2)
      "Quote text..." (Part II)
      "Quote text..." (Section 12)  â† If no chapters
```

---

### Markdown Pipeline

```
ğŸ“ MARKDOWN/TEXT DOCUMENT PROCESSING (100% LOCAL)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: EXTRACTION (15-30%)                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input:  Markdown or text file from storage                          â”‚
â”‚ Action: Download and optional conversion                            â”‚
â”‚         â”œâ”€ Markdown: Use as-is                                      â”‚
â”‚         â””â”€ Plain text: Optional Qwen 32B conversion to markdown     â”‚
â”‚             (adds headings, structure, formatting)                   â”‚
â”‚ Output: markdown content                                             â”‚
â”‚ Cost:   $0 (local)                                                  â”‚
â”‚ Time:   <1 minute                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: OPTIONAL CLEANUP (30-50%)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ markdown_asis:  Skip cleanup (use as-is)                            â”‚
â”‚ markdown_clean: Multi-pass Qwen 32B cleanup                         â”‚
â”‚                 â”œâ”€ Fix formatting issues                            â”‚
â”‚                 â”œâ”€ Improve structure                                â”‚
â”‚                 â””â”€ Polish for readability                           â”‚
â”‚ Cost: $0 (local)                                                    â”‚
â”‚ Time: ~5 minutes                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGES 3-6: SIMPLIFIED PIPELINE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ No bulletproof matching needed (single markdown version)          â”‚
â”‚ â€¢ AI semantic chunking (identifies boundaries)                      â”‚
â”‚ â€¢ Local LLM metadata enrichment                                     â”‚
â”‚ â€¢ Local embedding generation                                        â”‚
â”‚ â€¢ Save to database                                                   â”‚
â”‚ â€¢ Connection detection (async)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š TOTAL COST: $0
â±ï¸  TOTAL TIME: ~10-20 minutes
```

---

## Key Concepts

### 1. Bulletproof Chunk Matching

**The Problem:**
- Docling extracts chunks from **original markdown** (with artifacts)
- We clean the markdown with LLM (changes text)
- Chunk offsets now point to wrong positions
- Must remap chunks to cleaned markdown WITHOUT losing metadata

**The Solution: 5-Layer Failsafe**

```
Layer 1: Enhanced Fuzzy Matching (85% success)
â”œâ”€ Exact match: Find identical text
â”œâ”€ Normalized match: Ignore whitespace
â”œâ”€ Multi-anchor: Use start/middle/end phrases
â””â”€ Sliding window: Find best matching region

Layer 2: Embedding-Based (98% cumulative)
â”œâ”€ Embed chunk contents
â”œâ”€ Embed markdown windows
â””â”€ Cosine similarity matching

Layer 3: LLM-Assisted (99.9% cumulative)
â””â”€ Qwen 32B finds semantic matches

Layer 4: Anchor Interpolation (100%)
â”œâ”€ Use neighboring chunks as anchors
â”œâ”€ Interpolate position
â””â”€ Mark as "synthetic" (user-visible flag)

Result: NEVER lose a chunk, metadata always preserved
```

**What Gets Preserved:**
- âœ… Page numbers (for PDF citations)
- âœ… Section markers (for EPUB citations)
- âœ… Heading hierarchy (for navigation)
- âœ… Themes, concepts (for connections)
- âœ… All AI metadata

**What Changes:**
- start_offset, end_offset (updated for cleaned markdown)

**Confidence Tracking:**
```typescript
type ChunkConfidence = 
  | 'exact'      // Perfect match (85%)
  | 'high'       // Fuzzy/embedding match (13%)
  | 'medium'     // LLM-assisted (1.9%)
  | 'synthetic'  // Interpolated (0.1%)
```

Users can review synthetic chunks if desired.

### 2. PydanticAI Structured Outputs

**The Problem with Raw LLM Output:**
```python
# Without PydanticAI
response = llm.generate("Extract metadata from this chunk")
data = json.loads(response.text)  # Could fail!

# Must manually validate:
if 'themes' not in data:
    raise ValueError("Missing themes")
if not isinstance(data['importance'], float):
    raise ValueError("Importance must be float")
# ... dozens of checks
```

**The Solution: PydanticAI**
```python
# Define structure once
class ChunkMetadata(BaseModel):
    themes: list[str] = Field(min_length=1, max_length=5)
    concepts: list[Concept] = Field(min_length=1)
    importance: float = Field(ge=0.0, le=1.0)
    summary: str = Field(min_length=20, max_length=200)
    emotional: EmotionalTone

# Use with agent
agent = Agent(
    model=QwenModel('qwen2.5:32b'),
    result_type=ChunkMetadata,
    retries=3  # Auto-retry on validation failure
)

result = await agent.run(chunk_content)
metadata = result.data  # Guaranteed to be ChunkMetadata!
```

**Benefits:**
- âœ… Type-safe (no JSON parsing errors)
- âœ… Automatic retries (LLM fixes its own mistakes)
- âœ… Clear error messages ("importance must be 0-1, got 1.5")
- âœ… No manual validation code

**How Retries Work:**
1. LLM returns: `{"importance": 1.5, ...}`
2. Validation fails: "importance must be â‰¤1.0"
3. PydanticAI tells LLM: "Your importance value was invalid, try again"
4. LLM returns: `{"importance": 0.95, ...}` âœ…

### 3. Docling HybridChunker

**What It Does:**
- Reads document structure (headings, paragraphs, tables)
- Creates chunks at semantic boundaries (never mid-sentence)
- Targets 512 tokens per chunk (flexible)
- Merges small adjacent chunks if they fit
- Attaches rich metadata to each chunk

**Configuration:**
```python
chunker = HybridChunker(
    tokenizer='sentence-transformers/all-mpnet-base-v2',  # Match your embeddings
    max_tokens=512,        # Target size (~400 words)
    merge_peers=True,      # Combine short paragraphs
    heading_as_metadata=True  # Track parent sections
)
```

**Benefits:**
- âœ… Respects document structure (never splits tables)
- âœ… Preserves heading hierarchy
- âœ… Deterministic (same input = same output)
- âœ… Free (no API calls)
- âœ… Fast (~5 seconds for 500 pages)

**Works For:**
- PDF (via Docling extraction)
- EPUB (via HTML semantic tags)
- Markdown (via heading structure)
- Any structured document

### 4. The 3-Engine Connection System

```
Engine 1: Semantic Similarity (Baseline)
â”œâ”€ Method: Vector cosine similarity
â”œâ”€ Finds: "These say similar things"
â”œâ”€ Speed: Fast (database query)
â””â”€ Cost: $0

Engine 2: Contradiction Detection (Tension)
â”œâ”€ Method: Metadata + LLM verification
â”œâ”€ Finds: "These disagree about the same topic"
â”œâ”€ Speed: Medium (filtered candidates)
â””â”€ Cost: $0

Engine 3: Thematic Bridge (Serendipity)
â”œâ”€ Method: Cross-domain concept matching
â”œâ”€ Finds: "These connect different domains"
â”œâ”€ Speed: Slow (full LLM analysis)
â””â”€ Cost: $0 (but ~200 LLM calls per document)

Personal Scoring (at display time):
finalScore = 0.25 Ã— semantic +
             0.40 Ã— contradiction +  â† Highest weight
             0.35 Ã— thematic_bridge

User can adjust weights in real-time
```

**Example Connection:**
```json
{
  "source": "Gravity's Rainbow, Chapter 1, p. 47",
  "target": "Surveillance Capitalism, Chapter 3",
  "type": "thematic_bridge",
  "strength": 0.87,
  "reason": "Both discuss institutional control mechanisms through data",
  "shared_concept": "surveillance",
  "domains": ["literature", "technology"]
}
```

---

## Database Schema

### Core Tables

```sql
-- Documents (metadata layer)
CREATE TABLE documents (
  id uuid PRIMARY KEY,
  user_id uuid REFERENCES users(id),
  title text NOT NULL,
  storage_path text,              -- Base path in storage
  markdown_path text,              -- Path to content.md
  
  -- Processing state
  processing_status text,          -- pending/processing/completed/failed
  processing_requested boolean DEFAULT true,
  
  -- Document metadata
  source_type text,                -- pdf/epub/markdown_asis/markdown_clean/txt
  source_url text,                 -- Original URL (YouTube, web)
  word_count integer,
  
  -- Rich structure from Docling
  structure jsonb,                 -- Sections, hierarchy, TOC
  outline jsonb[],                 -- Heading outline
  
  -- Availability flags
  markdown_available boolean DEFAULT false,
  embeddings_available boolean DEFAULT false,
  
  -- Timestamps
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now()
);

-- Chunks (semantic units with rich metadata)
CREATE TABLE chunks (
  id uuid PRIMARY KEY,
  document_id uuid REFERENCES documents(id) ON DELETE CASCADE,
  chunk_index integer NOT NULL,
  content text NOT NULL,
  
  -- Position in cleaned markdown
  start_offset integer,
  end_offset integer,
  word_count integer,
  
  -- PDF-specific (null for EPUB/Markdown)
  page_start integer,
  page_end integer,
  bboxes jsonb,                    -- PDF coordinates for highlighting
  
  -- Universal structure (all formats)
  heading text,
  heading_path text[],             -- Full hierarchy ["Part I", "Chapter 1", "Section 1.1"]
  heading_level integer,
  section_marker text,             -- "chapter_001", "part_003", etc.
  
  -- AI metadata
  themes text[],
  importance_score float,
  summary text,
  
  -- Emotional metadata
  emotional_metadata jsonb,        -- {polarity, primaryEmotion, intensity}
  
  -- Conceptual metadata
  conceptual_metadata jsonb,       -- {concepts: [{text, importance}]}
  
  -- Domain classification
  domain_metadata jsonb,           -- {primaryDomain, confidence}
  
  -- Vector for semantic search
  embedding vector(768),           -- or vector(384) for MiniLM
  
  -- Quality tracking
  position_confidence text,        -- exact/high/medium/synthetic
  position_method text,            -- exact_match/embedding_match/llm_assisted/interpolation
  position_validated boolean DEFAULT false,
  
  metadata_extracted_at timestamptz,
  created_at timestamptz DEFAULT now(),
  
  UNIQUE(document_id, chunk_index)
);

-- Connections (relationships between chunks)
CREATE TABLE connections (
  id uuid PRIMARY KEY,
  source_chunk_id uuid REFERENCES chunks(id) ON DELETE CASCADE,
  target_chunk_id uuid REFERENCES chunks(id) ON DELETE CASCADE,
  
  -- Connection type
  type text NOT NULL,              -- semantic/contradiction/thematic_bridge
  strength float NOT NULL,         -- 0-1 confidence
  
  -- Detection metadata
  auto_detected boolean DEFAULT true,
  user_validated boolean DEFAULT false,
  discovered_at timestamptz DEFAULT now(),
  
  -- Connection details
  metadata jsonb,                  -- {reason, shared_concepts, domains, etc.}
  
  UNIQUE(source_chunk_id, target_chunk_id, type)
);

-- Optional: Extracted structures
CREATE TABLE document_tables (
  id uuid PRIMARY KEY,
  document_id uuid REFERENCES documents(id),
  chunk_id uuid REFERENCES chunks(id),
  content text,                    -- Markdown table
  caption text,
  page_number integer,
  section_title text,
  created_at timestamptz DEFAULT now()
);

CREATE TABLE document_figures (
  id uuid PRIMARY KEY,
  document_id uuid REFERENCES documents(id),
  caption text,
  page_number integer,
  bbox jsonb,
  image_path text,
  section_title text,
  created_at timestamptz DEFAULT now()
);
```

### Indexes

```sql
-- Chunk queries
CREATE INDEX idx_chunks_document ON chunks(document_id, chunk_index);
CREATE INDEX idx_chunks_pages ON chunks(document_id, page_start, page_end);
CREATE INDEX idx_chunks_section ON chunks(document_id, section_marker);
CREATE INDEX idx_chunks_heading_path ON chunks USING gin(heading_path);
CREATE INDEX idx_chunks_confidence ON chunks(position_confidence);

-- Vector search (pgvector)
CREATE INDEX idx_chunks_embedding ON chunks 
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

-- Connection queries
CREATE INDEX idx_connections_source ON connections(source_chunk_id, type);
CREATE INDEX idx_connections_target ON connections(target_chunk_id, type);
CREATE INDEX idx_connections_strength ON connections(strength DESC);
```

### Example Queries

```sql
-- Find chunks on specific page
SELECT * FROM chunks 
WHERE document_id = $1 
  AND page_start <= $2 
  AND page_end >= $2
ORDER BY chunk_index;

-- Semantic search (nearest neighbors)
SELECT c.*, 
       1 - (c.embedding <=> $1::vector) as similarity
FROM chunks c
WHERE c.document_id = $2
ORDER BY c.embedding <=> $1::vector
LIMIT 20;

-- Get document structure
SELECT heading_path, 
       heading_level, 
       COUNT(*) as chunk_count
FROM chunks
WHERE document_id = $1
GROUP BY heading_path, heading_level
ORDER BY MIN(chunk_index);

-- Find contradictions
SELECT c1.content as source,
       c2.content as target,
       conn.strength,
       conn.metadata->>'reason' as reason
FROM connections conn
JOIN chunks c1 ON conn.source_chunk_id = c1.id
JOIN chunks c2 ON conn.target_chunk_id = c2.id
WHERE conn.type = 'contradiction'
  AND conn.strength > 0.8
ORDER BY conn.strength DESC;
```

---

## Performance & Costs

### Processing Time (500-page PDF book)

| Stage | Time | Notes |
|-------|------|-------|
| Docling Extraction | 9 min | PDF to markdown + structure |
| Regex Cleanup | <1 sec | Deterministic cleanup |
| LLM Cleanup (optional) | 10-30 min | Multi-pass with Qwen 32B |
| Bulletproof Matching | 50 sec | 5-layer failsafe |
| Metadata Enrichment | 10-20 min | Qwen 32B + PydanticAI |
| Embedding Generation | 1-2 min | sentence-transformers |
| Database Save | 30 sec | Batch inserts |
| **Total (without cleanup)** | **~25 min** | Fast path |
| **Total (with cleanup)** | **~40-80 min** | Full polish |
| Connection Detection | 30-60 min | Async, separate job |

### Resource Requirements

```yaml
CPU:
  Minimum: 4 cores (Apple M1 or equivalent)
  Recommended: 8+ cores (Apple M3 Max or equivalent)
  
RAM:
  Qwen 32B: 20-24GB (Q4_K_M quantization)
  sentence-transformers: 2-4GB
  Docling: 2-4GB
  System overhead: 4GB
  Total: 28-36GB recommended

Storage:
  Models (one-time):
    - Qwen 32B: 20GB
    - sentence-transformers: 500MB
    - Docling dependencies: 2GB
  Per document:
    - Markdown: ~500KB per 500 pages
    - Database: ~2MB per 500 pages (chunks + embeddings)
    - Total: ~3MB per 500-page book
```

### Cost Analysis

```
Operating Costs: $0/document
â”œâ”€ Docling extraction: $0 (local)
â”œâ”€ LLM cleanup: $0 (local Ollama)
â”œâ”€ Metadata extraction: $0 (local Ollama)
â”œâ”€ Embeddings: $0 (local sentence-transformers)
â”œâ”€ Connection detection: $0 (local Ollama)
â””â”€ Database storage: $0.01/GB/month (Supabase free tier: 500MB)

One-Time Setup Costs:
â”œâ”€ Model downloads: ~23GB bandwidth
â””â”€ Setup time: ~1 hour

Hardware Costs:
â”œâ”€ Can run on: M1 Mac, high-end PC, cloud GPU
â”œâ”€ Recommended: M3 Max or equivalent
â””â”€ Alternative: Rent GPU server ($1-2/hour when processing)

Comparison to Cloud Services:
â”œâ”€ Gemini (previous): $0.50/document
â”œâ”€ OpenAI GPT-4: $2-3/document
â”œâ”€ Anthropic Claude: $1-2/document
â””â”€ Our system: $0/document (âœ…)
```

---

## Troubleshooting

### Common Issues

#### 1. "Chunk matching failed for 10 chunks"

**Cause:** LLM cleanup changed text too drastically  
**Solution:** Chunks marked as "synthetic" - metadata preserved  
**Action:** Review synthetic chunks in UI, manually validate if needed

#### 2. "Qwen 32B out of memory"

**Cause:** Insufficient RAM  
**Solutions:**
- Use smaller model (Qwen 14B or 7B)
- Use Q4 quantization instead of Q5
- Process smaller batches (5 chunks instead of 10)
- Close other applications

#### 3. "Docling extraction timeout"

**Cause:** Very large or complex PDF  
**Solutions:**
- Increase timeout in options
- Split PDF into parts
- Check for corrupted/protected PDF

#### 4. "Embeddings dimension mismatch"

**Cause:** Wrong tokenizer in HybridChunker  
**Solution:** Ensure tokenizer matches embedding model:
```python
# If using all-mpnet-base-v2 (768 dims)
tokenizer='sentence-transformers/all-mpnet-base-v2'

# If using all-MiniLM-L6-v2 (384 dims)
tokenizer='sentence-transformers/all-MiniLM-L6-v2'
```

#### 5. "Connection detection taking too long"

**Cause:** Large library, many documents  
**Solutions:**
- Run connection detection during off-hours
- Increase filtering thresholds (importance > 0.7)
- Reduce candidates per chunk (top 10 instead of 15)
- Process in batches (10 documents at a time)

### Quality Issues

#### "Too many low-confidence chunks"

**Check:**
1. Is LLM cleanup too aggressive? Try disabling cleanup
2. Is source PDF very poor quality? Manual review needed
3. Review the specific chunks - what patterns do you see?

**Fix:**
- Adjust cleanup prompts for gentler changes
- Use review checkpoints to catch issues early
- Manually validate synthetic chunks

#### "Connections seem random"

**Check:**
1. Are themes extracted correctly? Review chunk metadata
2. Is importance scoring working? Check score distribution
3. Are personal weights appropriate? Adjust in UI

**Fix:**
- Review a sample of connections
- Adjust personal scoring weights
- Increase strength thresholds (0.8 instead of 0.7)

#### "Missing important chunks"

**Impossible:** We guarantee 100% chunk recovery  
**If you think this happened:**
1. Check if chunk is marked "synthetic" (interpolated)
2. Review confidence levels
3. Check database - chunk exists but may have approximate position

### Performance Issues

#### "Processing too slow"

**Check:**
1. Which stage is slow? Check logs
2. Is Ollama running efficiently? Check GPU usage
3. Are models loaded in RAM? Check memory

**Optimize:**
- Use GPU acceleration if available
- Use smaller/faster models for initial processing
- Process in batches during off-hours
- Consider cloud GPU for large batches

---

## Success Metrics

**What Good Looks Like:**

```yaml
Chunk Recovery:
  Exact matches: 85%+
  High confidence: 13%
  Synthetic: <2%
  Failed: 0% (impossible)

Processing Quality:
  Markdown readability: High (manual review)
  Structure preservation: 100%
  Metadata completeness: 100%
  
Performance:
  Time per 500 pages: <80 minutes
  Memory usage: <30GB peak
  CPU utilization: 70-90% during processing
  
Connection Quality:
  Connections per document: 50-100
  User validation rate: >80% useful
  False positives: <20%
```

**Monitor:**
- Processing time trends
- Chunk confidence distribution
- Connection strength distribution
- User feedback on connection quality

---

## Future Enhancements

**Potential Additions:**

1. **Multi-Document Summarization**
   - Synthesize themes across entire library
   - Generate reading lists based on connections
   - Timeline views for historical documents

2. **Advanced Visualizations**
   - 3D knowledge graph
   - Temporal evolution of themes
   - Citation network graphs

3. **Specialized Extractors**
   - Academic paper parser (citations, references)
   - Code documentation extractor
   - Poetry/verse structure preservation

4. **Enhanced Connections**
   - Temporal proximity (documents from same era)
   - Author similarity (writing style)
   - Citation chains (follow references)

5. **Performance Optimizations**
   - Parallel processing pipeline
   - Incremental updates (only changed chunks)
   - Smart caching for frequently accessed chunks

---

## Appendix: File Locations

```
Repository Structure:
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ docling_extract.py          # Python wrapper for Docling
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ docling-extractor.ts        # TypeScript interface
â”‚   â”œâ”€â”€ embeddings.ts                # Local embedding generation
â”‚   â”œâ”€â”€ ai-chunking-batch.ts        # Semantic chunking
â”‚   â”œâ”€â”€ markdown-cleanup-ai.ts      # LLM cleanup
â”‚   â””â”€â”€ chunking/
â”‚       â””â”€â”€ bulletproof-matcher.ts   # 5-layer failsafe
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ pdf-docling.ts              # PDF pipeline
â”‚   â”œâ”€â”€ epub.ts                     # EPUB pipeline
â”‚   â”œâ”€â”€ markdown.ts                 # Markdown pipeline
â”‚   â””â”€â”€ text.ts                     # Text pipeline
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ process-document.ts         # Background job handler
â””â”€â”€ docs/
    â””â”€â”€ PIPELINE_REFERENCE.md       # This file

Storage Paths:
â”œâ”€â”€ storage/documents/
â”‚   â””â”€â”€ {userId}/{documentId}/
â”‚       â”œâ”€â”€ content.md              # Clean markdown
â”‚       â”œâ”€â”€ annotations.json        # User highlights
â”‚       â””â”€â”€ metadata.json           # Document properties

Database:
â”œâ”€â”€ documents                       # Document metadata
â”œâ”€â”€ chunks                          # Semantic units
â”œâ”€â”€ connections                     # Relationships
â”œâ”€â”€ document_tables                 # Extracted tables
â””â”€â”€ document_figures                # Extracted figures
```

---

**End of Reference Document**

*For questions or issues, refer to the troubleshooting section or review the specific processor code.*