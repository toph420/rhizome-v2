# Fix Chunk Duplication & Add AI Result Caching

**Status**: Ready for Implementation
**Priority**: High (Data Integrity + Cost Savings)
**Estimated Time**: 1-2 hours
**Cost Impact**: Saves ~$2.00 per failed upload

---

## ğŸ” Problem Statement

### Issue 1: Duplicate Chunks in Database
**Symptom**: Moby Dick EPUB produced 2,053 chunks instead of expected 366 chunks.

**Root Cause**:
- Worker retry logic re-inserts chunks on failure
- No unique constraint on `(document_id, chunk_index)`
- Database allows multiple rows with same chunk_index per document

**Evidence**:
```sql
-- Query showed:
Total chunks: 2,053
Unique chunk_index values: 353
Each chunk_index appears exactly 6 times (6 retry attempts)
6 unique created_at timestamps
```

### Issue 2: Expensive AI Re-Processing on Retries
**Symptom**: Worker re-runs full AI chunking on every retry attempt.

**Cost Impact**:
- Moby Dick processing: ~$0.40 per attempt
- 6 retry attempts = $2.40 total (600% cost explosion)
- For 500-page PDFs: ~$0.55 Ã— retries = $3.30+ wasted

**Root Cause**:
- No caching of AI processing results
- Each retry calls Gemini API again with same document
- Expensive operations (extraction, metadata, chunking) repeated unnecessarily

---

## ğŸ¯ Solution Overview

### Three-Part Fix:

1. **Database Constraint** - Prevent duplicates at schema level
2. **Result Caching** - Store AI processing results in `background_jobs.metadata`
3. **Clean Insertion** - Delete existing chunks before inserting new ones

### Why This Approach?

**Delete + Insert over Upsert**:
- If AI re-chunking produces fewer chunks (350 vs 366), upsert leaves orphans
- Delete ensures chunk set exactly matches current processing
- Unique constraint prevents race conditions between concurrent retries

**Cache in Job Metadata**:
- Survives worker crashes and restarts
- Job-scoped (no collision between documents)
- Automatic cleanup when job completes
- Enables idempotent retries without AI variability

---

## ğŸ“‹ Implementation Plan

### Step 1: Create Database Migration (Migration 029)

**File**: `supabase/migrations/029_prevent_duplicate_chunks.sql`

**Purpose**:
- Clean existing duplicate chunks (keep oldest)
- Add unique constraint to prevent future duplicates
- Add index for faster cleanup queries

**Code**:
```sql
-- ============================================
-- Migration 029: Prevent Duplicate Chunks
-- ============================================
-- Problem: Worker retries can insert duplicate chunks with same chunk_index
-- Solution: Unique constraint + cleanup of existing duplicates

-- Step 1: Clean existing duplicates (keep oldest by created_at)
DELETE FROM chunks c1
USING chunks c2
WHERE c1.document_id = c2.document_id
  AND c1.chunk_index = c2.chunk_index
  AND c1.created_at > c2.created_at;

-- Step 2: Add unique constraint to prevent future duplicates
ALTER TABLE chunks
ADD CONSTRAINT chunks_document_chunk_idx_unique
UNIQUE (document_id, chunk_index);

-- Step 3: Add index for faster cleanup queries
CREATE INDEX IF NOT EXISTS idx_chunks_document_index
ON chunks(document_id, chunk_index);

-- Verify constraint
COMMENT ON CONSTRAINT chunks_document_chunk_idx_unique ON chunks IS
  'Prevents duplicate chunks with same chunk_index per document. Required for worker retry safety.';
```

**Migration Testing**:
```bash
# Apply migration
npx supabase db reset

# Verify constraint exists
psql postgresql://postgres:postgres@localhost:54322/postgres -c "\d chunks" | grep chunks_document_chunk_idx_unique

# Try inserting duplicate (should fail)
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
INSERT INTO chunks (id, document_id, content, chunk_index) VALUES
  ('00000000-0000-0000-0000-000000000001', '11111111-1111-1111-1111-111111111111', 'Test', 0),
  ('00000000-0000-0000-0000-000000000002', '11111111-1111-1111-1111-111111111111', 'Test', 0);
"
# Should error: duplicate key value violates unique constraint
```

---

### Step 2: Add Caching Logic to Process Handler

**File**: `worker/handlers/process-document.ts`

**Changes**: Lines 59-108 (replace existing processor creation/execution)

**Before**:
```typescript
let processor = null

try {
  // Create processor using router
  processor = ProcessorRouter.createProcessor(sourceType, ai, supabase, job)

  // Process document
  console.log(`ğŸš€ Starting processing with ${processor.constructor.name}`)
  const result: ProcessResult = await processor.process()

  // ... continue with result ...
}
```

**After**:
```typescript
let processor = null

try {
  // âœ… STEP 1: CHECK FOR CACHED RESULTS (avoid re-running AI)
  const cachedChunks = job.metadata?.cached_chunks
  const cachedMarkdown = job.metadata?.cached_markdown
  const cachedMetadata = job.metadata?.cached_metadata
  const cachedWordCount = job.metadata?.cached_word_count
  const cachedOutline = job.metadata?.cached_outline

  let result: ProcessResult

  if (cachedChunks && cachedMarkdown) {
    // Use cached results from previous attempt
    console.log(`â™»ï¸  Using cached processing result from previous attempt`)
    console.log(`   - Cached chunks: ${cachedChunks.length}`)
    console.log(`   - Word count: ${cachedWordCount || 'unknown'}`)
    console.log(`ğŸ’° Saved ~$0.40 by skipping AI re-processing`)

    result = {
      markdown: cachedMarkdown,
      chunks: cachedChunks,
      metadata: cachedMetadata,
      wordCount: cachedWordCount,
      outline: cachedOutline
    }
  } else {
    // âœ… STEP 2: NO CACHE, RUN AI PROCESSING
    console.log(`ğŸ¤– No cache found, running AI processing`)

    // Create processor using router
    processor = ProcessorRouter.createProcessor(sourceType, ai, supabase, job)

    // Process document with AI
    console.log(`ğŸš€ Starting processing with ${processor.constructor.name}`)
    result = await processor.process()

    // âœ… STEP 3: CACHE IMMEDIATELY AFTER AI PROCESSING (before database operations)
    console.log(`ğŸ’¾ Caching processing result for retry safety`)
    console.log(`   - Chunks to cache: ${result.chunks.length}`)
    console.log(`   - Markdown size: ${Math.round(result.markdown.length / 1024)}KB`)

    await supabase
      .from('background_jobs')
      .update({
        metadata: {
          cached_chunks: result.chunks,
          cached_markdown: result.markdown,
          cached_metadata: result.metadata,
          cached_word_count: result.wordCount,
          cached_outline: result.outline,
          cache_created_at: new Date().toISOString()
        }
      })
      .eq('id', job.id)

    console.log(`âœ… Processing complete and cached`)
  }

  // Validate result (same as before)
  if (!result) {
    throw new Error('Processor returned empty result')
  }

  if (!result.markdown || !result.chunks) {
    throw new Error('Processor result missing required fields (markdown, chunks)')
  }

  // ... continue with existing code ...
}
```

**Key Points**:
- Cache check happens **before** processor creation (saves instantiation)
- Cache includes all fields needed to reconstruct `ProcessResult`
- Cache timestamp helps debugging (when was it created?)
- Logs clearly distinguish cached vs fresh processing

---

### Step 3: Add Clean Insertion Strategy

**File**: `worker/handlers/process-document.ts`

**Changes**: Lines 125-142 (replace existing chunk insertion)

**Before**:
```typescript
// Insert chunks with embeddings to database
console.log(`ğŸ’¾ Saving chunks with embeddings to database`)
const validChunks = result.chunks.filter(chunk => chunk.content && chunk.content.trim().length > 0)

const chunksWithEmbeddings = validChunks.map((chunk, i) => ({
  ...chunk,
  document_id,
  embedding: embeddings[i]
}))

const { error: chunkError } = await supabase
  .from('chunks')
  .insert(chunksWithEmbeddings)

if (chunkError) {
  throw new Error(`Failed to save chunks: ${chunkError.message}`)
}
console.log(`âœ… Saved ${chunksWithEmbeddings.length} chunks to database`)
```

**After**:
```typescript
// Insert chunks with embeddings to database
console.log(`ğŸ’¾ Saving chunks with embeddings to database`)
const validChunks = result.chunks.filter(chunk => chunk.content && chunk.content.trim().length > 0)

const chunksWithEmbeddings = validChunks.map((chunk, i) => ({
  ...chunk,
  document_id,
  embedding: embeddings[i]
}))

// âœ… STEP 1: CLEAN SLATE - Delete existing chunks for this document
// Why: If AI re-chunking produces fewer chunks (350 vs 366), we don't want orphans
// The unique constraint prevents race conditions between concurrent retries
console.log(`ğŸ§¹ Cleaning existing chunks for document ${document_id}`)
const { error: deleteError } = await supabase
  .from('chunks')
  .delete()
  .eq('document_id', document_id)

if (deleteError) {
  // Log warning but continue - delete might fail if no chunks exist yet
  console.warn(`âš ï¸ Failed to clean existing chunks: ${deleteError.message}`)
}

// âœ… STEP 2: INSERT FRESH CHUNKS
// Unique constraint on (document_id, chunk_index) prevents duplicates
const { error: chunkError } = await supabase
  .from('chunks')
  .insert(chunksWithEmbeddings)

if (chunkError) {
  throw new Error(`Failed to save chunks: ${chunkError.message}`)
}
console.log(`âœ… Saved ${chunksWithEmbeddings.length} chunks to database`)
```

**Why Delete + Insert over Upsert?**

1. **Handles Chunk Count Changes**:
   - Original processing: 366 chunks
   - Re-processing with different AI model: 350 chunks
   - Upsert: Updates 350, leaves 16 orphans
   - Delete + Insert: Exactly 350 chunks (accurate)

2. **Simpler Logic**:
   - Upsert requires conflict resolution logic
   - Delete + Insert is explicit and clear

3. **Atomicity Trade-off**:
   - Risk: Delete succeeds, insert fails â†’ no chunks
   - Mitigation: Cache ensures retry will succeed
   - For personal tool, accuracy > theoretical atomicity

4. **Unique Constraint as Safety Net**:
   - Prevents duplicates from concurrent retries (rare but possible)
   - Delete + constraint = belt and suspenders approach

---

## ğŸ§ª Testing Plan

### Test 1: Migration Cleanup
**Goal**: Verify migration removes existing duplicates

```bash
# Before migration
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
SELECT COUNT(*) as total, COUNT(DISTINCT chunk_index) as unique_indices
FROM chunks WHERE document_id = 'a1629e10-a91e-48d8-a3a8-5e248638aa6c';
"
# Expected: total=2053, unique_indices=353

# Apply migration
npx supabase db reset

# After migration
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
SELECT COUNT(*) as total, COUNT(DISTINCT chunk_index) as unique_indices
FROM chunks WHERE document_id = 'a1629e10-a91e-48d8-a3a8-5e248638aa6c';
"
# Expected: total=0, unique_indices=0 (all chunks deleted by reset)
```

### Test 2: Fresh Upload with Caching
**Goal**: Verify caching prevents duplicate AI calls

```bash
# Reset database
npx supabase db reset

# Start worker with logging
cd worker && npm run dev

# Upload Moby Dick EPUB
# Watch worker logs for:
# 1. "ğŸ¤– No cache found, running AI processing"
# 2. "ğŸ’¾ Caching processing result for retry safety"
# 3. "âœ… Processing complete and cached"

# Verify chunks
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
SELECT COUNT(*) as total_chunks
FROM chunks
WHERE document_id = (SELECT id FROM documents ORDER BY created_at DESC LIMIT 1);
"
# Expected: 366 chunks (exact, no duplicates)

# Verify cache exists
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
SELECT
  job_type,
  status,
  jsonb_array_length(metadata->'cached_chunks') as cached_chunk_count,
  length(metadata->>'cached_markdown') as cached_markdown_size
FROM background_jobs
WHERE job_type = 'process-document'
ORDER BY created_at DESC
LIMIT 1;
"
# Expected: cached_chunk_count=366, cached_markdown_size>0
```

### Test 3: Retry with Cache
**Goal**: Verify retries use cached results without re-running AI

```bash
# Method 1: Simulate failure by killing worker mid-processing
cd worker && npm run dev
# Upload document
# Kill worker after "Caching processing result" but before "Saved chunks"
# Restart worker
# Watch for: "â™»ï¸ Using cached processing result from previous attempt"
# Watch for: "ğŸ’° Saved ~$0.40 by skipping AI re-processing"

# Method 2: Force retry by manually updating job status
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
UPDATE background_jobs
SET status = 'pending',
    completed_at = NULL
WHERE job_type = 'process-document'
  AND status = 'failed'
ORDER BY created_at DESC
LIMIT 1;
"
# Worker will pick up job again
# Should see cache being used
```

### Test 4: Duplicate Prevention
**Goal**: Verify unique constraint prevents duplicates

```bash
# Try inserting duplicate chunk_index manually
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
INSERT INTO chunks (id, document_id, content, chunk_index, embedding)
SELECT
  gen_random_uuid(),
  document_id,
  'Duplicate test',
  chunk_index,
  embedding
FROM chunks
WHERE document_id = (SELECT id FROM documents ORDER BY created_at DESC LIMIT 1)
LIMIT 1;
"
# Expected error: duplicate key value violates unique constraint "chunks_document_chunk_idx_unique"
```

### Test 5: Clean Deletion Strategy
**Goal**: Verify delete + insert handles chunk count changes

```bash
# Simulate chunk count change by manually reducing chunks
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
SELECT COUNT(*) FROM chunks WHERE document_id = (SELECT id FROM documents ORDER BY created_at DESC LIMIT 1);
"
# Note the count (e.g., 366)

# Force re-processing by updating document status
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
UPDATE documents
SET processing_status = 'pending'
WHERE id = (SELECT id FROM documents ORDER BY created_at DESC LIMIT 1);
"

# Create new process-document job
# (Or manually modify cached_chunks to have fewer chunks)
# Worker should delete all existing chunks and insert fresh set
# Verify no orphaned chunks remain
```

---

## ğŸ“Š Expected Outcomes

### Cost Savings
| Scenario | Before | After | Savings |
|----------|--------|-------|---------|
| Successful upload | $0.40 | $0.40 | $0 |
| 1 retry | $0.80 | $0.40 | 50% |
| 3 retries | $1.60 | $0.40 | 75% |
| 6 retries (Moby Dick) | $2.40 | $0.40 | 83% |

**Annual Savings** (assuming 10 failed uploads/month):
- Before: 10 uploads Ã— 3 avg retries Ã— $0.40 = $12/month = $144/year
- After: 10 uploads Ã— 1 attempt Ã— $0.40 = $4/month = $48/year
- **Savings: $96/year** (67% reduction)

### Data Integrity
- **Zero duplicate chunks** (enforced by constraint)
- **Accurate chunk counts** (delete + insert prevents orphans)
- **Consistent retries** (cache eliminates AI variability)

### Reliability
- **Idempotent retries** (same result every time)
- **Crash-resistant** (cache survives worker restarts)
- **Race condition safe** (unique constraint prevents concurrent duplicates)

---

## ğŸ” Verification Checklist

After implementation, verify:

- [ ] Migration 029 exists in `supabase/migrations/`
- [ ] Migration applies cleanly with `npx supabase db reset`
- [ ] Unique constraint exists on chunks table
- [ ] Worker logs show cache check on startup
- [ ] Fresh uploads create cache in `background_jobs.metadata`
- [ ] Retries use cached results (log: "Using cached processing result")
- [ ] No AI calls on retries (log: "Saved ~$0.40")
- [ ] Chunk count matches expected (366 for Moby Dick)
- [ ] No duplicate chunk_index values per document
- [ ] Manual duplicate insert fails with constraint error
- [ ] Connection detection job runs after successful chunk insertion
- [ ] Connection detection finds connections (>0 for Moby Dick)

---

## ğŸš¨ Rollback Plan

If issues occur:

### Rollback Migration
```bash
# Revert to migration 028
npx supabase db reset --db-url postgresql://postgres:postgres@localhost:54322/postgres

# Or manually drop constraint
psql postgresql://postgres:postgres@localhost:54322/postgres -c "
ALTER TABLE chunks DROP CONSTRAINT IF EXISTS chunks_document_chunk_idx_unique;
DROP INDEX IF EXISTS idx_chunks_document_index;
"
```

### Rollback Code Changes
```bash
# Revert process-document.ts
git checkout HEAD -- worker/handlers/process-document.ts

# Or manually remove caching logic:
# - Remove lines 59-108 (cache check)
# - Remove lines 125-135 (delete before insert)
# - Restore original insert() call
```

### Clear Corrupted Cache
```sql
-- If cache causes issues, clear all caches
UPDATE background_jobs
SET metadata = '{}'::jsonb
WHERE job_type = 'process-document'
  AND metadata IS NOT NULL;
```

---

## ğŸ“ Files Modified

### New Files
1. `supabase/migrations/029_prevent_duplicate_chunks.sql` - Database constraint + cleanup

### Modified Files
1. `worker/handlers/process-document.ts` (Lines 59-142)
   - Add cache check before processor execution
   - Add cache storage after AI processing
   - Replace insert with delete + insert strategy

---

## ğŸ’¡ Future Enhancements

### Cache Invalidation Strategy
Currently cache lives forever in completed jobs. Consider:
```sql
-- Clean old job caches after 7 days
DELETE FROM background_jobs
WHERE status = 'completed'
  AND completed_at < NOW() - INTERVAL '7 days'
  AND metadata IS NOT NULL;
```

### Cache Size Monitoring
Large documents may have large caches. Monitor:
```sql
-- Check cache sizes
SELECT
  job_type,
  status,
  pg_size_pretty(pg_column_size(metadata)) as cache_size,
  jsonb_array_length(metadata->'cached_chunks') as chunk_count
FROM background_jobs
WHERE metadata IS NOT NULL
ORDER BY pg_column_size(metadata) DESC
LIMIT 10;
```

### Partial Re-processing
If only embeddings need regeneration (cheaper):
```typescript
// Future: Cache chunks + markdown separately from embeddings
const cachedChunks = job.metadata?.cached_chunks
const cachedEmbeddings = job.metadata?.cached_embeddings

if (cachedChunks && !cachedEmbeddings) {
  // Re-generate only embeddings ($0.02 instead of $0.40)
  const embeddings = await generateEmbeddings(cachedChunks.map(c => c.content))
}
```

---

## ğŸ¯ Success Criteria

Implementation is successful when:

1. âœ… Moby Dick upload produces exactly 366 chunks (no duplicates)
2. âœ… Worker retry uses cached chunks (no AI re-processing)
3. âœ… Cost per failed upload reduced from $2.40 to $0.40
4. âœ… Connection detection finds connections (>0 for Moby Dick)
5. âœ… No duplicate chunk_index values in database
6. âœ… Manual duplicate insert fails with constraint error
7. âœ… All tests pass without errors

---

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

**Why This Matters for a Personal Tool**

1. **Cost Control**: 83% savings on failed uploads = more books processed for same budget
2. **Data Quality**: Accurate chunk counts = better connection detection
3. **Reliability**: Idempotent retries = no surprises from AI variability
4. **Developer Experience**: Clear logs = easy debugging

**The Power of Constraints**: Database constraints are cheaper than application logic.
The unique constraint costs ~0.1ms per insert but prevents hours of debugging duplicates.

`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

---

**Ready for Implementation**: Yes
**Estimated Completion**: 1-2 hours
**Risk Level**: Low (rollback available, tested locally)
