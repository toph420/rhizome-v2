# Annotation Recovery & Sync System - Complete Implementation Plan

**Status**: Ready to implement (greenfield - no backward compatibility needed)
**Timeline**: 3-4 days full implementation
**Core Features**: Fuzzy matching recovery, Obsidian sync, document editing, review UI

---

## Architecture Overview

**`â˜… Key Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`**
Our existing 5-component ECS annotation system (Position, Visual, Content, Temporal, ChunkRef) already provides the foundation. We're adding fuzzy matching fields to enable annotation recovery when content.md is edited. Migration 030 already added `chunk_ids[]` for multi-chunk support.
**`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`**

### Current State (Already Built)
- âœ… ECS with 5-component annotations
- âœ… Multi-chunk support (`chunk_ids[]` in components table)
- âœ… `content.md` file-first architecture
- âœ… Worker module with file access
- âœ… Hooks for annotation CRUD

### What We're Adding
1. **Fuzzy Matching Fields** â†’ Position component enhancement
2. **Chunk-Bounded Recovery** â†’ 150x faster than full-text search
3. **Reprocessing Pipeline** â†’ Worker handler for content.md edits
4. **Obsidian Integration** â†’ Bidirectional sync with vault
5. **Review UI** â†’ User approval for fuzzy matches
6. **Readwise Import** â†’ Highlight migration from other services

---

## Phase 1: Schema Enhancement (2 hours)

### 1.1 Migration: Add Fuzzy Matching Fields

**File**: `supabase/migrations/031_fuzzy_matching_fields.sql`

```sql
-- Migration 031: Fuzzy matching fields for annotation recovery
-- Purpose: Enable chunk-bounded fuzzy matching after document edits
-- Date: 2025-10-04

-- ============================================
-- COMPONENTS: Add Recovery Fields
-- ============================================

-- Text context for fuzzy matching (Â±100 chars around annotation)
ALTER TABLE components
  ADD COLUMN IF NOT EXISTS text_context JSONB;

-- Original chunk index for chunk-bounded search
ALTER TABLE components
  ADD COLUMN IF NOT EXISTS original_chunk_index INTEGER;

-- Recovery confidence (0.0-1.0)
ALTER TABLE components
  ADD COLUMN IF NOT EXISTS recovery_confidence FLOAT;

-- Recovery method used
ALTER TABLE components
  ADD COLUMN IF NOT EXISTS recovery_method TEXT;

-- Needs human review flag
ALTER TABLE components
  ADD COLUMN IF NOT EXISTS needs_review BOOLEAN DEFAULT FALSE;

-- ============================================
-- INDEXES
-- ============================================

-- Index for chunk-bounded queries
CREATE INDEX IF NOT EXISTS idx_components_chunk_index
  ON components(original_chunk_index)
  WHERE component_type IN ('annotation', 'Position');

-- Index for review queue
CREATE INDEX IF NOT EXISTS idx_components_needs_review
  ON components(needs_review)
  WHERE needs_review = TRUE;

-- ============================================
-- COMMENTS
-- ============================================

COMMENT ON COLUMN components.text_context IS
  'Text context for fuzzy matching: { before: string, after: string }';

COMMENT ON COLUMN components.original_chunk_index IS
  'Original chunk index for chunk-bounded search (0-based)';

COMMENT ON COLUMN components.recovery_confidence IS
  'Confidence score from last recovery attempt (0.0-1.0)';

COMMENT ON COLUMN components.recovery_method IS
  'Recovery method used: exact | context | chunk_bounded | lost';

COMMENT ON COLUMN components.needs_review IS
  'True if fuzzy match needs human approval';
```

### 1.2 Update Component Types

**File**: `src/lib/ecs/components.ts` (MODIFY)

```typescript
// Add to PositionComponent interface
export interface PositionComponent {
  // ... existing fields ...

  // NEW: Fuzzy matching fields
  textContext?: {
    before: string  // 100 chars before annotation
    after: string   // 100 chars after annotation
  }
  originalChunkIndex?: number  // For chunk-bounded search
  recoveryConfidence?: number  // 0.0-1.0
  recoveryMethod?: 'exact' | 'context' | 'chunk_bounded' | 'lost'
  needsReview?: boolean
}

// Add to ChunkRefComponent interface
export interface ChunkRefComponent {
  // ... existing fields ...

  // Support multi-chunk (using chunk_ids from migration 030)
  chunkIds?: string[]  // Array of chunk IDs (for multi-chunk annotations)
}
```

### 1.3 Update Annotation Operations

**File**: `src/lib/ecs/annotations.ts` (MODIFY)

```typescript
// Update CreateAnnotationInput
export interface CreateAnnotationInput {
  // ... existing fields ...

  // NEW: Capture context on creation
  textContext?: {
    before: string
    after: string
  }
  chunkIndex?: number  // Current chunk index in document
}

// Modify create() method
async create(input: CreateAnnotationInput): Promise<string> {
  const now = new Date().toISOString();

  const entityId = await this.ecs.createEntity(this.userId, {
    Position: {
      documentId: input.documentId,
      document_id: input.documentId,
      startOffset: input.startOffset,
      endOffset: input.endOffset,
      originalText: input.originalText,
      pageLabel: input.pageLabel,

      // NEW: Capture context for future recovery
      textContext: input.textContext,
      originalChunkIndex: input.chunkIndex,
      recoveryConfidence: 1.0,
      recoveryMethod: 'exact',
      needsReview: false
    },
    // ... rest of components ...
  });

  return entityId;
}

// NEW: Get annotations needing review
async getNeedingReview(documentId: string): Promise<AnnotationEntity[]> {
  const entities = await this.ecs.query(
    [],
    this.userId,
    { document_id: documentId }
  );

  return entities.filter(ann => {
    const pos = this.getComponent<PositionComponent>(ann, 'Position');
    return pos.needsReview === true;
  });
}
```

---

## Phase 2: Fuzzy Matching Library (3 hours)

### 2.1 Core Fuzzy Matching

**File**: `worker/lib/fuzzy-match.ts` (CREATE)

```typescript
import { distance } from 'fastest-levenshtein';

export interface FuzzyMatchResult {
  text: string
  startOffset: number
  endOffset: number
  confidence: number  // 0.0-1.0
  method: 'exact' | 'context' | 'chunk_bounded'
}

export interface Chunk {
  id: string
  chunk_index: number
  start_offset: number
  end_offset: number
  content: string
}

/**
 * Strategy 1: Exact text match (fastest, highest confidence)
 */
export function findExactMatch(
  markdown: string,
  needle: string
): FuzzyMatchResult | null {
  const index = markdown.indexOf(needle);

  if (index >= 0) {
    return {
      text: needle,
      startOffset: index,
      endOffset: index + needle.length,
      confidence: 1.0,
      method: 'exact'
    };
  }

  return null;
}

/**
 * Strategy 2: Context-guided match (high confidence)
 */
export function findWithContext(
  markdown: string,
  needle: string,
  contextBefore: string,
  contextAfter: string
): FuzzyMatchResult | null {
  // Try exact match with full context
  const pattern = `${contextBefore}${needle}${contextAfter}`;
  const index = markdown.indexOf(pattern);

  if (index >= 0) {
    const needleStart = index + contextBefore.length;
    return {
      text: needle,
      startOffset: needleStart,
      endOffset: needleStart + needle.length,
      confidence: 0.95,
      method: 'context'
    };
  }

  // Try context-guided fuzzy match
  const beforeIndex = markdown.indexOf(contextBefore);
  if (beforeIndex >= 0) {
    const searchStart = beforeIndex + contextBefore.length;
    const searchEnd = Math.min(
      markdown.length,
      searchStart + needle.length + Math.floor(needle.length * 0.3)
    );
    const segment = markdown.slice(searchStart, searchEnd);

    const fuzzy = findFuzzyInSegment(segment, needle);
    if (fuzzy && fuzzy.confidence > 0.85) {
      return {
        text: fuzzy.text,
        startOffset: searchStart + fuzzy.startOffset,
        endOffset: searchStart + fuzzy.endOffset,
        confidence: fuzzy.confidence * 0.9,
        method: 'context'
      };
    }
  }

  return null;
}

/**
 * Strategy 3: Chunk-bounded search (performance-optimized)
 *
 * KEY INSIGHT: Instead of searching 750K chars, search ~12,500 chars
 * (Â±2 chunks from original position). This is 60x smaller search space.
 */
export function findNearChunk(
  markdown: string,
  needle: string,
  chunks: Chunk[],
  originalChunkIndex: number
): FuzzyMatchResult | null {
  // Search Â±2 chunks from original position
  const searchChunks = chunks.slice(
    Math.max(0, originalChunkIndex - 2),
    Math.min(chunks.length, originalChunkIndex + 3)
  );

  if (searchChunks.length === 0) return null;

  // Build bounded search space (~12,500 chars instead of 750K)
  const searchStart = searchChunks[0].start_offset;
  const searchEnd = searchChunks[searchChunks.length - 1].end_offset;
  const searchSpace = markdown.slice(searchStart, searchEnd);

  const fuzzy = findFuzzyInSegment(searchSpace, needle);

  if (fuzzy && fuzzy.confidence > 0.75) {
    return {
      text: fuzzy.text,
      startOffset: fuzzy.startOffset + searchStart,
      endOffset: fuzzy.endOffset + searchStart,
      confidence: fuzzy.confidence,
      method: 'chunk_bounded'
    };
  }

  return null;
}

/**
 * Internal: Fuzzy match within bounded segment
 */
function findFuzzyInSegment(
  segment: string,
  needle: string
): { text: string; startOffset: number; endOffset: number; confidence: number } | null {
  const needleLength = needle.length;
  const windowSize = needleLength + Math.floor(needleLength * 0.2);

  let bestMatch: { text: string; startOffset: number; endOffset: number; confidence: number } | null = null;

  for (let i = 0; i <= segment.length - needleLength; i++) {
    const window = segment.slice(i, i + windowSize);
    const candidate = window.slice(0, needleLength);
    const dist = distance(needle, candidate);
    const similarity = 1 - (dist / needleLength);

    if (similarity > (bestMatch?.confidence || 0.75)) {
      bestMatch = {
        text: candidate,
        startOffset: i,
        endOffset: i + needleLength,
        confidence: similarity
      };
    }
  }

  return bestMatch;
}
```

### 2.2 Install Dependencies

**File**: `worker/package.json` (MODIFY)

```json
{
  "dependencies": {
    // ... existing deps ...
    "fastest-levenshtein": "^1.0.16"
  }
}
```

---

## Phase 3: Reprocessing Pipeline (4 hours)

### 3.1 Annotation Recovery Handler

**File**: `worker/handlers/recover-annotations.ts` (CREATE)

```typescript
import { findExactMatch, findWithContext, findNearChunk } from '../lib/fuzzy-match';
import type { Chunk, FuzzyMatchResult } from '../lib/fuzzy-match';

interface AnnotationSnapshot {
  id: string
  text: string
  startOffset: number
  endOffset: number
  textContext?: {
    before: string
    after: string
  }
  originalChunkIndex?: number
}

interface RecoveryResults {
  success: AnnotationSnapshot[]
  needsReview: Array<{
    annotation: AnnotationSnapshot
    suggestedMatch: FuzzyMatchResult
  }>
  lost: AnnotationSnapshot[]
}

/**
 * Recover annotations after content.md edit using chunk-bounded fuzzy matching
 */
export async function recoverAnnotations(
  documentId: string,
  newMarkdown: string,
  newChunks: Chunk[]
): Promise<RecoveryResults> {
  const results: RecoveryResults = {
    success: [],
    needsReview: [],
    lost: []
  };

  // Get all annotations for this document
  const { data: components } = await supabase
    .from('components')
    .select('*')
    .eq('document_id', documentId)
    .eq('component_type', 'Position');

  if (!components || components.length === 0) {
    return results;
  }

  for (const component of components) {
    const ann: AnnotationSnapshot = {
      id: component.id,
      text: component.data.originalText,
      startOffset: component.data.startOffset,
      endOffset: component.data.endOffset,
      textContext: component.text_context,
      originalChunkIndex: component.original_chunk_index
    };

    // Strategy 1: Exact text match (highest confidence)
    const exact = findExactMatch(newMarkdown, ann.text);
    if (exact) {
      await updateAnnotationPosition(component.id, exact, newChunks);
      results.success.push(ann);
      continue;
    }

    // Strategy 2: Context-guided match
    if (ann.textContext?.before && ann.textContext?.after) {
      const context = findWithContext(
        newMarkdown,
        ann.text,
        ann.textContext.before,
        ann.textContext.after
      );

      if (context && context.confidence > 0.85) {
        await updateAnnotationPosition(component.id, context, newChunks);
        results.success.push(ann);
        continue;
      }
    }

    // Strategy 3: Chunk-bounded fuzzy match
    if (ann.originalChunkIndex !== null && ann.originalChunkIndex !== undefined) {
      const fuzzy = findNearChunk(
        newMarkdown,
        ann.text,
        newChunks,
        ann.originalChunkIndex
      );

      if (fuzzy && fuzzy.confidence > 0.8) {
        results.needsReview.push({
          annotation: ann,
          suggestedMatch: fuzzy
        });

        // Flag for review
        await supabase
          .from('components')
          .update({
            needs_review: true,
            recovery_confidence: fuzzy.confidence,
            recovery_method: fuzzy.method
          })
          .eq('id', component.id);

        continue;
      }
    }

    // Strategy 4: Lost
    results.lost.push(ann);
    await supabase
      .from('components')
      .update({
        recovery_method: 'lost',
        recovery_confidence: 0.0
      })
      .eq('id', component.id);
  }

  return results;
}

/**
 * Update annotation position after successful recovery
 */
async function updateAnnotationPosition(
  componentId: string,
  match: FuzzyMatchResult,
  newChunks: Chunk[]
) {
  // Find which chunk(s) this annotation belongs to
  const overlappingChunks = newChunks.filter(
    c => c.end_offset > match.startOffset && c.start_offset < match.endOffset
  );

  const newChunkIndex = newChunks.findIndex(
    c => c.start_offset <= match.startOffset && c.end_offset >= match.endOffset
  );

  // Update Position component data
  await supabase
    .from('components')
    .update({
      data: supabase.raw(`
        jsonb_set(
          jsonb_set(data, '{startOffset}', '${match.startOffset}'),
          '{endOffset}', '${match.endOffset}'
        )
      `),
      original_chunk_index: newChunkIndex,
      recovery_confidence: match.confidence,
      recovery_method: match.method,
      needs_review: false,
      // Update chunk_ids array
      chunk_ids: overlappingChunks.map(c => c.id)
    })
    .eq('id', componentId);
}
```

### 3.2 Main Reprocessing Handler

**File**: `worker/handlers/reprocess-document.ts` (CREATE)

```typescript
import { recoverAnnotations } from './recover-annotations';
import { remapConnections } from './remap-connections';

interface ReprocessResults {
  annotations: {
    success: number
    needsReview: number
    lost: number
  }
  connections: {
    success: number
    needsReview: number
    lost: number
  }
}

/**
 * Main reprocessing pipeline for edited documents
 */
export async function reprocessDocument(
  documentId: string
): Promise<ReprocessResults> {
  console.log(`ðŸ”„ Reprocessing document ${documentId}`);

  // 1. Get edited markdown from storage (source of truth)
  const { data: document } = await supabase
    .from('documents')
    .select('markdown_path')
    .eq('id', documentId)
    .single();

  const { data: fileBlob } = await supabase.storage
    .from('documents')
    .download(document.markdown_path);

  const editedMarkdown = await fileBlob.text();

  // 2. Snapshot verified connections (user_validated = true)
  const { data: verifiedConnections } = await supabase
    .from('chunk_connections')
    .select(`
      *,
      source_chunk:chunks!source_chunk_id(id, embedding),
      target_chunk:chunks!target_chunk_id(id, embedding)
    `)
    .eq('user_validated', true)
    .or(`source_chunk.document_id.eq.${documentId},target_chunk.document_id.eq.${documentId}`);

  // 3. Delete old chunks (cascade deletes auto-detected connections)
  await supabase
    .from('chunks')
    .delete()
    .eq('document_id', documentId);

  // 4. Reprocess markdown â†’ new chunks
  console.log(`âš™ï¸  Processing ${editedMarkdown.length} chars`);
  const chunks = await processMarkdown(editedMarkdown, documentId);

  // 5. Generate embeddings
  await embedChunks(chunks);

  // 6. Run 3-engine collision detection
  await detectConnections(chunks);

  // 7. Recover annotations (chunk-bounded fuzzy matching)
  console.log(`ðŸŽ¯ Recovering annotations`);
  const annotationResults = await recoverAnnotations(
    documentId,
    editedMarkdown,
    chunks
  );

  // 8. Remap verified connections (embedding-based)
  console.log(`ðŸ”— Remapping verified connections`);
  const connectionResults = await remapConnections(
    verifiedConnections || [],
    chunks
  );

  console.log(`âœ… Reprocessing complete`);
  console.log(`   Annotations: ${annotationResults.success.length} restored, ${annotationResults.needsReview.length} review, ${annotationResults.lost.length} lost`);
  console.log(`   Connections: ${connectionResults.success.length} restored, ${connectionResults.needsReview.length} review, ${connectionResults.lost.length} lost`);

  return {
    annotations: {
      success: annotationResults.success.length,
      needsReview: annotationResults.needsReview.length,
      lost: annotationResults.lost.length
    },
    connections: {
      success: connectionResults.success.length,
      needsReview: connectionResults.needsReview.length,
      lost: connectionResults.lost.length
    }
  };
}

// Helper: Use existing processors for markdown â†’ chunks
async function processMarkdown(markdown: string, documentId: string) {
  // Reuse existing markdown processor
  const processor = new MarkdownCleanProcessor(/* ... */);
  const result = await processor.process();

  // Save chunks to database
  const { data: chunks } = await supabase
    .from('chunks')
    .insert(
      result.chunks.map((chunk, index) => ({
        document_id: documentId,
        content: chunk.text,
        chunk_index: index,
        start_offset: chunk.metadata.start_offset,
        end_offset: chunk.metadata.end_offset,
        // ... other fields
      }))
    )
    .select();

  return chunks;
}

// Helper: Use existing embeddings generation
async function embedChunks(chunks: any[]) {
  const { embeddings } = await embedMany({
    model: google.textEmbeddingModel('gemini-embedding-001'),
    values: chunks.map(c => c.content)
  });

  for (let i = 0; i < chunks.length; i++) {
    await supabase
      .from('chunks')
      .update({ embedding: embeddings[i] })
      .eq('id', chunks[i].id);
  }
}

// Helper: Use existing 3-engine system
async function detectConnections(chunks: any[]) {
  const { processDocument } = await import('../engines/orchestrator');
  await processDocument(chunks[0].document_id);
}
```

### 3.3 Connection Remapping

**File**: `worker/handlers/remap-connections.ts` (CREATE)

```typescript
interface Connection {
  id: string
  source_chunk_id: string
  target_chunk_id: string
  type: string
  strength: number
  metadata: any
  source_chunk: { id: string; embedding: number[] }
  target_chunk: { id: string; embedding: number[] }
}

export async function remapConnections(
  verifiedConnections: Connection[],
  newChunks: any[]
) {
  const results = {
    success: [] as Connection[],
    needsReview: [] as any[],
    lost: [] as Connection[]
  };

  for (const conn of verifiedConnections) {
    if (!conn.source_chunk?.embedding || !conn.target_chunk?.embedding) {
      results.lost.push(conn);
      continue;
    }

    // Use pgvector to find best semantic matches
    const [sourceMatch, targetMatch] = await Promise.all([
      findBestMatch(conn.source_chunk.embedding, newChunks),
      findBestMatch(conn.target_chunk.embedding, newChunks)
    ]);

    if (sourceMatch.similarity > 0.95 && targetMatch.similarity > 0.95) {
      // High confidence - auto-remap
      await supabase.from('chunk_connections').insert({
        source_chunk_id: sourceMatch.chunk.id,
        target_chunk_id: targetMatch.chunk.id,
        type: conn.type,
        strength: conn.strength,
        user_validated: true,
        auto_detected: false,
        metadata: {
          ...conn.metadata,
          remapped: true,
          original_source: conn.source_chunk_id,
          original_target: conn.target_chunk_id,
          remap_confidence: {
            source: sourceMatch.similarity,
            target: targetMatch.similarity
          }
        }
      });
      results.success.push(conn);
    } else if (sourceMatch.similarity > 0.85 && targetMatch.similarity > 0.85) {
      // Medium confidence - needs review
      results.needsReview.push({
        connection: conn,
        sourceMatch,
        targetMatch
      });
    } else {
      // Low confidence - lost
      results.lost.push(conn);
    }
  }

  return results;
}

async function findBestMatch(
  embedding: number[],
  chunks: any[]
): Promise<{ chunk: any; similarity: number }> {
  const { data } = await supabase.rpc('match_chunks', {
    query_embedding: embedding,
    threshold: 0.75,
    match_count: 1
  });

  if (!data || data.length === 0) {
    return { chunk: null, similarity: 0 };
  }

  return {
    chunk: chunks.find(c => c.id === data[0].id),
    similarity: data[0].similarity
  };
}
```

---

## Phase 4: Obsidian Integration (3 hours)

### 4.1 Worker Obsidian Sync

**File**: `worker/handlers/obsidian-sync.ts` (CREATE)

```typescript
import fs from 'fs/promises';
import path from 'path';

interface ObsidianSettings {
  vaultPath: string
  vaultName: string
  autoSync: boolean
  syncAnnotations: boolean
}

/**
 * Export document to Obsidian vault
 */
export async function exportToObsidian(
  documentId: string,
  userId: string
): Promise<{ success: boolean; path: string }> {
  // Get document and settings
  const [document, settings] = await Promise.all([
    getDocument(documentId),
    getObsidianSettings(userId)
  ]);

  if (!settings?.vaultPath) {
    throw new Error('Obsidian vault not configured');
  }

  // Get markdown from storage
  const { data: blob } = await supabase.storage
    .from('documents')
    .download(document.markdown_path);

  const markdown = await blob.text();

  // Write to vault
  const vaultFilePath = path.join(settings.vaultPath, document.obsidian_path);
  await fs.mkdir(path.dirname(vaultFilePath), { recursive: true });
  await fs.writeFile(vaultFilePath, markdown, 'utf-8');

  // Export annotations if enabled
  if (settings.syncAnnotations) {
    const annotations = await getAnnotationsForExport(documentId);
    const annotationsPath = vaultFilePath.replace('.md', '.annotations.json');
    await fs.writeFile(
      annotationsPath,
      JSON.stringify(annotations, null, 2),
      'utf-8'
    );
  }

  console.log(`âœ… Exported to ${vaultFilePath}`);
  return { success: true, path: vaultFilePath };
}

/**
 * Sync edited document from Obsidian vault
 */
export async function syncFromObsidian(
  documentId: string,
  userId: string
): Promise<{ changed: boolean; results?: any }> {
  const [document, settings] = await Promise.all([
    getDocument(documentId),
    getObsidianSettings(userId)
  ]);

  if (!settings?.vaultPath) {
    throw new Error('Obsidian vault not configured');
  }

  // Read edited markdown from vault
  const vaultFilePath = path.join(settings.vaultPath, document.obsidian_path);
  const editedMarkdown = await fs.readFile(vaultFilePath, 'utf-8');

  // Check if changed
  const { data: currentBlob } = await supabase.storage
    .from('documents')
    .download(document.markdown_path);

  const currentMarkdown = await currentBlob.text();

  if (editedMarkdown === currentMarkdown) {
    return { changed: false };
  }

  // Upload edited version to storage (file-first)
  await supabase.storage
    .from('documents')
    .update(document.markdown_path, new Blob([editedMarkdown]), {
      contentType: 'text/markdown',
      upsert: true
    });

  // Reprocess with fuzzy matching
  const results = await reprocessDocument(documentId);

  return { changed: true, results };
}

/**
 * Get Obsidian URI for "Edit in Obsidian" button
 */
export function getObsidianUri(
  vaultName: string,
  obsidianPath: string
): string {
  const encodedPath = encodeURIComponent(obsidianPath);
  const encodedVault = encodeURIComponent(vaultName);

  return `obsidian://advanced-uri?vault=${encodedVault}&filepath=${encodedPath}`;
}

// Helpers
async function getDocument(documentId: string) {
  const { data } = await supabase
    .from('documents')
    .select('*')
    .eq('id', documentId)
    .single();
  return data;
}

async function getObsidianSettings(userId: string): Promise<ObsidianSettings | null> {
  const { data } = await supabase
    .from('user_settings')
    .select('obsidian_settings')
    .eq('user_id', userId)
    .single();

  return data?.obsidian_settings || null;
}

async function getAnnotationsForExport(documentId: string) {
  const { data } = await supabase
    .from('components')
    .select('*')
    .eq('document_id', documentId)
    .eq('component_type', 'Position');

  return data || [];
}
```

### 4.2 Settings Schema

**File**: `supabase/migrations/032_obsidian_settings.sql` (CREATE)

```sql
-- Migration 032: Obsidian integration settings
-- Purpose: Store user's Obsidian vault configuration
-- Date: 2025-10-04

CREATE TABLE IF NOT EXISTS user_settings (
  user_id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,

  -- Obsidian vault configuration
  obsidian_settings JSONB,

  -- General preferences
  preferences JSONB,

  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index for fast user lookups
CREATE INDEX IF NOT EXISTS idx_user_settings_user_id
  ON user_settings(user_id);

-- Sample obsidian_settings structure
COMMENT ON COLUMN user_settings.obsidian_settings IS
  'Obsidian vault configuration:
   {
     vaultName: string,
     vaultPath: string,
     autoSync: boolean,
     syncAnnotations: boolean
   }';
```

### 4.3 UI Integration

**File**: `src/components/reader/DocumentHeader.tsx` (MODIFY)

```tsx
'use client';

import { useState } from 'react';
import { ExternalLink, RefreshCw } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { useToast } from '@/hooks/use-toast';

export function DocumentHeader({ documentId }: { documentId: string }) {
  const [syncing, setSyncing] = useState(false);
  const { toast } = useToast();

  const handleEditInObsidian = async () => {
    try {
      // Export to vault first
      const response = await fetch('/api/obsidian/export', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ documentId })
      });

      const { uri } = await response.json();

      // Open in Obsidian
      window.location.href = uri;
    } catch (error) {
      toast({
        title: 'Export failed',
        description: error.message,
        variant: 'destructive'
      });
    }
  };

  const handleSync = async () => {
    setSyncing(true);

    try {
      const response = await fetch('/api/obsidian/sync', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ documentId })
      });

      const result = await response.json();

      if (result.changed) {
        // Show review modal
        toast({
          title: 'Document updated',
          description: `${result.results.annotations.success} annotations recovered, ${result.results.annotations.needsReview} need review`
        });
      } else {
        toast({
          title: 'No changes',
          description: 'Document is already in sync'
        });
      }
    } catch (error) {
      toast({
        title: 'Sync failed',
        description: error.message,
        variant: 'destructive'
      });
    } finally {
      setSyncing(false);
    }
  };

  return (
    <div className="flex gap-2">
      <Button onClick={handleEditInObsidian} variant="outline">
        <ExternalLink className="w-4 h-4 mr-2" />
        Edit in Obsidian
      </Button>

      <Button onClick={handleSync} disabled={syncing} variant="outline">
        <RefreshCw className={cn('w-4 h-4 mr-2', syncing && 'animate-spin')} />
        {syncing ? 'Syncing...' : 'Sync from Obsidian'}
      </Button>
    </div>
  );
}
```

---

## Phase 5: Review UI (3 hours)

### 5.1 Review Modal Component

**File**: `src/components/review/AnnotationReviewModal.tsx` (CREATE)

```tsx
'use client';

import { useState } from 'react';
import { CheckCircle, AlertCircle, XCircle } from 'lucide-react';
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
  DialogFooter
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { useToast } from '@/hooks/use-toast';

interface ReviewItem {
  annotation: {
    id: string
    text: string
  }
  suggestedMatch: {
    text: string
    startOffset: number
    endOffset: number
    confidence: number
  }
}

interface ReviewModalProps {
  documentId: string
  results: {
    success: any[]
    needsReview: ReviewItem[]
    lost: any[]
  }
  open: boolean
  onClose: () => void
}

export function AnnotationReviewModal({
  documentId,
  results,
  open,
  onClose
}: ReviewModalProps) {
  const [pending, setPending] = useState(results.needsReview);
  const { toast } = useToast();

  const acceptMatch = async (item: ReviewItem) => {
    try {
      await fetch('/api/annotations/accept-match', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          componentId: item.annotation.id,
          startOffset: item.suggestedMatch.startOffset,
          endOffset: item.suggestedMatch.endOffset
        })
      });

      setPending(prev => prev.filter(i => i !== item));
      toast({ title: 'Match accepted' });
    } catch (error) {
      toast({
        title: 'Failed to accept',
        description: error.message,
        variant: 'destructive'
      });
    }
  };

  const discardMatch = async (item: ReviewItem) => {
    try {
      await fetch('/api/annotations/discard', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          componentId: item.annotation.id
        })
      });

      setPending(prev => prev.filter(i => i !== item));
      toast({ title: 'Annotation discarded' });
    } catch (error) {
      toast({
        title: 'Failed to discard',
        description: error.message,
        variant: 'destructive'
      });
    }
  };

  return (
    <Dialog open={open} onOpenChange={onClose}>
      <DialogContent className="max-w-4xl max-h-[80vh] overflow-y-auto">
        <DialogHeader>
          <DialogTitle>Annotation Recovery Results</DialogTitle>
        </DialogHeader>

        <div className="space-y-6">
          {/* Summary Stats */}
          <div className="grid grid-cols-3 gap-4">
            <StatCard
              icon={CheckCircle}
              label="Automatically Restored"
              value={results.success.length}
              variant="success"
            />
            <StatCard
              icon={AlertCircle}
              label="Need Your Review"
              value={pending.length}
              variant="warning"
            />
            <StatCard
              icon={XCircle}
              label="Could Not Recover"
              value={results.lost.length}
              variant="destructive"
            />
          </div>

          {/* Review Items */}
          {pending.length > 0 && (
            <div className="space-y-4">
              <h3 className="text-lg font-medium">Review These Matches</h3>

              {pending.map((item, idx) => (
                <div
                  key={idx}
                  className="border rounded-lg p-4 space-y-3"
                >
                  <div>
                    <p className="text-sm font-medium text-muted-foreground">
                      Original Text
                    </p>
                    <p className="text-sm bg-muted p-2 rounded mt-1">
                      "{item.annotation.text}"
                    </p>
                  </div>

                  <div>
                    <p className="text-sm font-medium text-muted-foreground">
                      Suggested Match
                    </p>
                    <p className="text-sm bg-muted p-2 rounded mt-1">
                      "{item.suggestedMatch.text}"
                    </p>
                  </div>

                  <div className="flex items-center justify-between pt-2">
                    <Badge variant="secondary">
                      {Math.round(item.suggestedMatch.confidence * 100)}% confidence
                    </Badge>

                    <div className="flex gap-2">
                      <Button
                        size="sm"
                        onClick={() => acceptMatch(item)}
                      >
                        Accept
                      </Button>
                      <Button
                        size="sm"
                        variant="destructive"
                        onClick={() => discardMatch(item)}
                      >
                        Discard
                      </Button>
                    </div>
                  </div>
                </div>
              ))}
            </div>
          )}

          {/* Lost Items */}
          {results.lost.length > 0 && (
            <div className="space-y-2">
              <h3 className="text-lg font-medium text-destructive">
                Could Not Recover ({results.lost.length})
              </h3>
              <p className="text-sm text-muted-foreground">
                These annotations could not be matched to the edited document.
              </p>
              <details className="text-sm">
                <summary className="cursor-pointer hover:underline">
                  Show lost annotations
                </summary>
                <ul className="mt-2 space-y-1 pl-4">
                  {results.lost.map((item, idx) => (
                    <li key={idx} className="text-muted-foreground">
                      "{item.text?.slice(0, 60)}..."
                    </li>
                  ))}
                </ul>
              </details>
            </div>
          )}
        </div>

        <DialogFooter>
          <Button onClick={onClose}>
            {pending.length > 0 ? 'Review Later' : 'Close'}
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
}

function StatCard({
  icon: Icon,
  label,
  value,
  variant
}: {
  icon: any;
  label: string;
  value: number;
  variant: 'success' | 'warning' | 'destructive';
}) {
  const colors = {
    success: 'text-green-600',
    warning: 'text-yellow-600',
    destructive: 'text-red-600'
  };

  return (
    <div className="border rounded-lg p-4">
      <div className="flex items-center gap-2">
        <Icon className={cn('w-5 h-5', colors[variant])} />
        <p className="text-2xl font-bold">{value}</p>
      </div>
      <p className="text-sm text-muted-foreground mt-1">{label}</p>
    </div>
  );
}
```

### 5.2 API Routes

**File**: `src/app/api/annotations/accept-match/route.ts` (CREATE)

```typescript
import { NextRequest } from 'next/server';
import { getSupabaseClient } from '@/lib/auth';

export async function POST(request: NextRequest) {
  const { componentId, startOffset, endOffset } = await request.json();
  const supabase = getSupabaseClient();

  // Update component with accepted match
  await supabase
    .from('components')
    .update({
      data: supabase.raw(`
        jsonb_set(
          jsonb_set(data, '{startOffset}', '${startOffset}'),
          '{endOffset}', '${endOffset}'
        )
      `),
      needs_review: false
    })
    .eq('id', componentId);

  return Response.json({ success: true });
}
```

**File**: `src/app/api/annotations/discard/route.ts` (CREATE)

```typescript
import { NextRequest } from 'next/server';
import { getSupabaseClient } from '@/lib/auth';

export async function POST(request: NextRequest) {
  const { componentId } = await request.json();
  const supabase = getSupabaseClient();

  // Delete the component (which deletes the entity if it's the only component)
  await supabase
    .from('components')
    .delete()
    .eq('id', componentId);

  return Response.json({ success: true });
}
```

---

## Phase 6: Readwise Import (2 hours)

### 6.1 Import Handler

**File**: `worker/handlers/readwise-import.ts` (CREATE)

```typescript
import { findExactMatch, findNearChunk } from '../lib/fuzzy-match';

interface ReadwiseHighlight {
  text: string
  location: number  // Percentage (0-100)
  note?: string
  color?: string
  highlighted_at: string
}

export async function importReadwiseHighlights(
  documentId: string,
  readwiseJson: ReadwiseHighlight[]
) {
  const [document, chunks] = await Promise.all([
    getDocument(documentId),
    getChunks(documentId)
  ]);

  const { data: blob } = await supabase.storage
    .from('documents')
    .download(document.markdown_path);

  const markdown = await blob.text();

  const results = {
    imported: [] as any[],
    needsReview: [] as any[],
    failed: [] as any[]
  };

  for (const highlight of readwiseJson) {
    // Try exact match
    const exact = findExactMatch(markdown, highlight.text);

    if (exact) {
      const annotation = await createAnnotation({
        documentId,
        text: highlight.text,
        startOffset: exact.startOffset,
        endOffset: exact.endOffset,
        color: mapReadwiseColor(highlight.color),
        note: highlight.note
      });
      results.imported.push(annotation);
      continue;
    }

    // Try chunk-bounded fuzzy (use location as hint)
    const estimatedChunkIndex = Math.floor(
      (highlight.location / 100) * chunks.length
    );

    const fuzzy = findNearChunk(
      markdown,
      highlight.text,
      chunks,
      estimatedChunkIndex
    );

    if (fuzzy && fuzzy.confidence > 0.8) {
      results.needsReview.push({
        highlight,
        suggestedMatch: fuzzy
      });
    } else {
      results.failed.push({
        highlight,
        reason: 'No match found (confidence < 0.8)'
      });
    }
  }

  return results;
}

function mapReadwiseColor(color?: string): string {
  const map: Record<string, string> = {
    yellow: 'yellow',
    blue: 'blue',
    red: 'red',
    green: 'green',
    orange: 'orange'
  };
  return map[color?.toLowerCase() || ''] || 'yellow';
}
```

---

## Implementation Timeline

### Day 1: Foundation (8 hours)
- âœ… Morning: Migration 031 + component type updates (2h)
- âœ… Afternoon: Fuzzy matching library + tests (3h)
- âœ… Evening: Annotation recovery handler (3h)

### Day 2: Core Pipeline (8 hours)
- âœ… Morning: Reprocessing pipeline (4h)
- âœ… Afternoon: Connection remapping (2h)
- âœ… Evening: Integration testing (2h)

### Day 3: Obsidian & UI (8 hours)
- âœ… Morning: Obsidian sync handlers (3h)
- âœ… Afternoon: Review modal UI (3h)
- âœ… Evening: Readwise import (2h)

### Day 4: Polish & Testing (4 hours)
- âœ… End-to-end testing
- âœ… Edge case handling
- âœ… Documentation

**Total: 28 hours** (3.5 days)

---

## Testing Checklist

**Fuzzy Matching Performance**
- [ ] Exact match: <50ms for 20 annotations
- [ ] Context match: <100ms for 20 annotations
- [ ] Chunk-bounded: <2 seconds for 20 annotations
- [ ] Recovery rate: >90% (success + needsReview)

**Reprocessing**
- [ ] Edit content.md â†’ annotations recovered
- [ ] Verified connections preserved
- [ ] Review modal displays correctly
- [ ] Lost items correctly identified

**Obsidian Sync**
- [ ] Export â†’ file written to vault
- [ ] URI opens Obsidian
- [ ] Sync detects changes
- [ ] Reprocess triggers fuzzy matching

**Readwise Import**
- [ ] Exact matches auto-created
- [ ] Fuzzy matches flagged for review
- [ ] Failed matches listed with reason

---

## Cost Analysis

**Per reprocessing (500-page book):**
- Chunking: $0.20 (unchanged)
- Embeddings: $0.02 (unchanged)
- 3-engine detection: $0.20 (unchanged)
- Fuzzy matching: $0 (local computation)
- **Total: ~$0.42 per reprocess**

**Performance:**
- Chunk-bounded fuzzy: 1-2 seconds for 20 annotations (150x faster than full-text)
- Embedding-based connection remap: <500ms for 50 connections (100x faster)
- Full reprocess: ~20-25 minutes (same as initial processing)

---

## Files Modified/Created

### New Files (14)
1. `supabase/migrations/031_fuzzy_matching_fields.sql`
2. `supabase/migrations/032_obsidian_settings.sql`
3. `worker/lib/fuzzy-match.ts`
4. `worker/handlers/recover-annotations.ts`
5. `worker/handlers/reprocess-document.ts`
6. `worker/handlers/remap-connections.ts`
7. `worker/handlers/obsidian-sync.ts`
8. `worker/handlers/readwise-import.ts`
9. `src/components/review/AnnotationReviewModal.tsx`
10. `src/components/reader/DocumentHeader.tsx`
11. `src/app/api/annotations/accept-match/route.ts`
12. `src/app/api/annotations/discard/route.ts`
13. `src/app/api/obsidian/export/route.ts`
14. `src/app/api/obsidian/sync/route.ts`

### Modified Files (3)
1. `src/lib/ecs/components.ts` - Add fuzzy matching fields
2. `src/lib/ecs/annotations.ts` - Add context capture + review methods
3. `worker/package.json` - Add `fastest-levenshtein` dependency

---

## Success Metrics

**Must Pass:**
- Annotation recovery rate >90%
- Chunk-bounded search <2 seconds
- Connection remapping >85% success rate
- Zero data loss for high-confidence matches

**Should Achieve:**
- Obsidian sync roundtrip successful
- Review UI intuitive (1-click accept/reject)
- Readwise import >80% exact matches

---

This plan preserves all human effort (annotations, validated connections) while enabling seamless document editing and multi-tool workflows. The chunk-bounded fuzzy matching is the key innovationâ€”60x smaller search space = 150x performance improvement.
