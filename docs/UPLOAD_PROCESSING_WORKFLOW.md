# Upload Processing Workflow - Complete Reference

> **Last Updated**: October 4, 2025
> **Status**: Production-ready with idempotent retry logic
> **Version**: 2.0 (Major refactor with checkpointed processing)

## Table of Contents

1. [Overview & Philosophy](#overview--philosophy)
2. [Complete System Architecture](#complete-system-architecture)
3. [Processing Stages (Idempotent)](#processing-stages-idempotent)
4. [Retry & Resume Logic](#retry--resume-logic)
5. [Processor Implementations](#processor-implementations)
6. [Connection Detection System](#connection-detection-system)
7. [Large Document Handling](#large-document-handling)
8. [Database Schema](#database-schema)
9. [Error Handling](#error-handling)
10. [Monitoring & Debugging](#monitoring--debugging)

---

## Overview & Philosophy

### Architecture Principles

**1. Single Responsibility Principle**
- **Processors** (`worker/processors/*.ts`): Transform data ONLY (no I/O)
- **Handlers** (`worker/handlers/*.ts`): Orchestrate I/O operations
- **Engines** (`worker/engines/*.ts`): Detect connections between chunks

**2. Idempotent Retry Design**
- All operations can be safely retried without side effects
- Processing stages tracked in job metadata
- Cached AI results prevent duplicate expensive calls
- Conditional database operations based on stage

**3. Cost-Aware Processing**
- Large documents batched to stay within AI token limits
- AI results cached immediately after generation
- Filtering prevents unnecessary AI calls (~$0.54 per 500-page book)

**4. Hybrid Storage Strategy**
- **Supabase Storage**: Full markdown documents (for reading)
- **PostgreSQL**: Chunks with embeddings (for connections)
- **Why both?**: Read continuous flow, connect with precision

---

## Complete System Architecture

```mermaid
graph TD
    subgraph "Frontend (Next.js 15)"
        A[UploadZone Component] --> B[Server Action: uploadDocument]
        B --> C[Create Document Record]
        B --> D[Create Background Job]
        E[DocumentList Component] --> F[Real-time Updates]
        G[ProcessingDock Component] --> H[Job Progress Polling]
    end

    subgraph "Database (Supabase)"
        C --> I[(documents table)]
        D --> J[(background_jobs table)]
        K[(chunks table)]
        L[(chunk_connections table)]
    end

    subgraph "Worker (Node.js)"
        M[Worker Loop<br/>index.ts] --> N{Job Type?}
        N -->|process_document| O[ProcessDocument Handler]
        N -->|detect-connections| P[DetectConnections Handler]

        O --> Q[Processor Router]
        Q --> R1[PDF Processor]
        Q --> R2[YouTube Processor]
        Q --> R3[Web Processor]
        Q --> R4[Markdown Processor]
        Q --> R5[Text Processor]
        Q --> R6[Paste Processor]
        Q --> R7[EPUB Processor]

        R1 --> S[ProcessResult]
        R2 --> S
        R3 --> S
        R4 --> S
        R5 --> S
        R6 --> S
        R7 --> S

        S --> T[Save Markdown to Storage]
        T --> U[Generate Embeddings]
        U --> V[Save Chunks to DB]
        V --> W[Create Connection Job]

        P --> X[Orchestrator]
        X --> Y1[Semantic Engine]
        X --> Y2[Contradiction Engine]
        X --> Y3[Thematic Bridge Engine]
        Y1 --> Z[Save Connections]
        Y2 --> Z
        Y3 --> Z
    end

    subgraph "Storage (Supabase)"
        AA[userId/documentId/content.md]
        AB[userId/documentId/source-raw.txt]
    end

    T --> AA
    T --> AB
    V --> K
    Z --> L

    I --> E
    J --> G
    K --> AC[Document Reader]
    L --> AC

    style O fill:#4CAF50
    style P fill:#2196F3
    style S fill:#FF9800
    style Z fill:#9C27B0
```

### Component Responsibilities

| Component | File | Responsibility |
|-----------|------|----------------|
| **UploadZone** | `src/components/library/UploadZone.tsx` | 6 upload methods (PDF, YouTube, Web, Markdown, Text, Paste) |
| **DocumentList** | `src/components/library/DocumentList.tsx` | Display documents with real-time status + polling fallback |
| **ProcessingDock** | `src/components/layout/ProcessingDock.tsx` | Show active jobs with progress bars |
| **uploadDocument** | `src/app/actions/upload.ts` | Server action: create document + background job |
| **Worker Loop** | `worker/index.ts` | Poll jobs every 5s, detect stale jobs (30min timeout) |
| **ProcessDocument Handler** | `worker/handlers/process-document.ts` | Orchestrate full pipeline with heartbeat + checkpointing |
| **DetectConnections Handler** | `worker/handlers/detect-connections.ts` | Run 3-engine collision detection |
| **Processor Router** | `worker/processors/router.ts` | Route to processor by source type |
| **7 Processors** | `worker/processors/*.ts` | Transform source ‚Üí markdown + chunks (NO I/O) |
| **Connection Orchestrator** | `worker/engines/orchestrator.ts` | Coordinate 3 engines, aggregate results |
| **3 Engines** | `worker/engines/*.ts` | Semantic, Contradiction, Thematic Bridge |

---

## Processing Stages (Idempotent)

### Stage Lifecycle

```mermaid
stateDiagram-v2
    [*] --> pending: Job Created
    pending --> extracting: Worker Picks Up

    extracting --> extracted: AI Processing Complete
    note right of extracting: Heartbeat every 5min<br/>Prevents timeout

    extracted --> markdown_saved: Upload to Storage
    note right of extracted: Cache in job.metadata<br/>Enables resume

    markdown_saved --> chunked: Insert to Database
    note right of markdown_saved: Conditional deletion<br/>Preserves on retry

    chunked --> embedded: Embeddings Generated

    embedded --> complete: Document Ready
    note right of embedded: Queue async connection job<br/>Doesn't block

    complete --> [*]

    extracting --> failed: Permanent Error
    markdown_saved --> failed: Permanent Error
    chunked --> failed: Permanent Error
    embedded --> failed: Permanent Error

    failed --> extracting: Retry (transient error)
    note right of failed: Resume from checkpoint<br/>Use cached results
```

### Stage Details

#### Stage 1: `pending` ‚Üí `extracting`

**Trigger**: Worker picks up job from queue

**Location**: `worker/index.ts:23-91`

```typescript
// Worker polls every 5 seconds
async function processNextJob() {
  const supabase = createClient(...)

  // Get pending jobs OR stale jobs (>30min without heartbeat)
  const { data: jobs } = await supabase
    .from('background_jobs')
    .select('*')
    .or('status.eq.pending,and(status.eq.failed,next_retry_at.lte.now())')
    .order('created_at', { ascending: true })
    .limit(1)

  if (!jobs || jobs.length === 0) {
    // Check for stale processing jobs
    const thirtyMinutesAgo = new Date(Date.now() - 30 * 60 * 1000)
    // ... stale job recovery
  }
}
```

**Key Configuration**:
- Poll interval: 5 seconds
- Stale timeout: 30 minutes (was 10, increased for large docs)

---

#### Stage 2: `extracting` ‚Üí `extracted`

**Trigger**: AI processing completes

**Location**: `worker/handlers/process-document.ts:86-145`

```typescript
// Check for cached results first (retry optimization)
const cachedChunks = job.metadata?.cached_chunks
const cachedMarkdown = job.metadata?.cached_markdown

if (cachedChunks && cachedMarkdown) {
  console.log(`‚ôªÔ∏è  Using cached processing result`)
  result = { markdown: cachedMarkdown, chunks: cachedChunks, ... }
} else {
  // ‚úÖ START HEARTBEAT: Prevents timeout during long AI processing
  const heartbeatInterval = setInterval(async () => {
    await supabase
      .from('background_jobs')
      .update({ started_at: new Date().toISOString() })
      .eq('id', job.id)
  }, 5 * 60 * 1000) // Every 5 minutes

  try {
    // Create processor and run AI extraction
    processor = ProcessorRouter.createProcessor(sourceType, ai, supabase, job)
    result = await processor.process() // Can take 5-15 minutes
  } finally {
    clearInterval(heartbeatInterval) // Always cleanup
  }

  // ‚úÖ CACHE IMMEDIATELY (before any I/O failures can occur)
  await supabase
    .from('background_jobs')
    .update({
      metadata: {
        cached_chunks: result.chunks,
        cached_markdown: result.markdown,
        cached_metadata: result.metadata,
        cached_word_count: result.wordCount,
        cached_outline: result.outline,
        cache_created_at: new Date().toISOString(),

        // NEW: Stage tracking for idempotent retry
        processing_stage: 'extracted',
        completed_stages: ['extracting'],
        stage_timestamps: {
          extracting: new Date().toISOString()
        }
      }
    })
    .eq('id', job.id)
}
```

**Key Features**:
- **Heartbeat mechanism**: Resets `started_at` every 5min to prevent stale detection
- **Immediate caching**: Stores AI results before any I/O operations
- **Stage tracking**: Records completion in metadata for resume capability

---

#### Stage 3: `extracted` ‚Üí `markdown_saved`

**Trigger**: Markdown uploaded to Supabase Storage

**Location**: `worker/handlers/process-document.ts:177-193`

```typescript
// Upload markdown to storage
const markdownPath = `${userId}/${documentId}/content.md`
const { error: uploadError } = await supabase.storage
  .from('documents')
  .upload(markdownPath, result.markdown, {
    contentType: 'text/markdown',
    upsert: true
  })

if (uploadError) {
  throw new Error(`Failed to save markdown: ${uploadError.message}`)
}

// Update stage after markdown saved
await updateStage(supabase, job.id, 'markdown_saved')
```

**Storage Paths**:
- Clean markdown: `userId/documentId/content.md`
- YouTube raw transcript: `userId/documentId/source-raw.txt` (with timestamps)

---

#### Stage 4: `markdown_saved` ‚Üí `chunked`

**Trigger**: Chunks inserted to database

**Location**: `worker/handlers/process-document.ts:220-255`

```typescript
// ‚úÖ CONDITIONAL CHUNK DELETION: Stage-aware cleanup
const stage = job.metadata?.processing_stage || 'pending'
const isResume = ['chunked', 'embedded', 'complete'].includes(stage)

if (isResume) {
  console.log(`‚ôªÔ∏è  Resuming from stage: ${stage}, keeping existing chunks`)
  console.log(`   This prevents FK violations in collision detection`)
} else {
  // Fresh processing: Clean slate for consistent chunk_index ordering
  console.log(`üßπ Cleaning existing chunks for fresh processing`)
  await supabase
    .from('chunks')
    .delete()
    .eq('document_id', document_id)
}

// Insert chunks with embeddings
const { error: chunkError } = await supabase
  .from('chunks')
  .insert(chunksWithEmbeddings)

if (chunkError) {
  throw new Error(`Failed to save chunks: ${chunkError.message}`)
}

// Update stage after chunks inserted
await updateStage(supabase, job.id, 'chunked')
```

**Critical Fix**: Conditional deletion prevents FK violations
- **Before**: Always deleted chunks (even during retry)
- **After**: Only deletes if starting fresh, preserves if resuming
- **Result**: Connection detection never references deleted chunks

---

#### Stage 5: `chunked` ‚Üí `embedded`

**Trigger**: Embeddings generated and saved

**Location**: `worker/handlers/process-document.ts:195-218`

```typescript
// Generate embeddings for chunks
const chunkTexts = result.chunks.map(chunk => chunk.content)
const embeddings = await generateEmbeddings(chunkTexts)

const chunksWithEmbeddings = validChunks.map((chunk, i) => ({
  ...chunk,
  document_id,
  embedding: embeddings[i] // 768-dimensional vector
}))
```

**Embedding Details**:
- Model: `gemini-embedding-001`
- Dimensions: 768
- Cost: ~$0.02 per 500 chunks
- Batch size: Up to 100 at once

---

#### Stage 6: `embedded` ‚Üí `complete`

**Trigger**: Document ready for reading, async connection job created

**Location**: `worker/handlers/process-document.ts:257-297`

```typescript
// ‚úÖ ASYNC CONNECTION DETECTION: Queued as separate job
if (chunksWithEmbeddings.length >= 2) {
  // Check for existing jobs (prevent duplicates)
  const { data: existingJobs } = await supabase
    .from('background_jobs')
    .select('id, status')
    .eq('job_type', 'detect-connections')
    .eq('user_id', userId)
    .in('status', ['pending', 'processing', 'completed'])
    .contains('input_data', { document_id })
    .limit(1)

  if (existingJobs && existingJobs.length > 0) {
    console.log(`üîç Collision detection job already exists - skipping duplicate`)
  } else {
    await supabase
      .from('background_jobs')
      .insert({
        user_id: userId,
        job_type: 'detect-connections',
        status: 'pending',
        input_data: {
          document_id,
          chunk_count: chunksWithEmbeddings.length,
          trigger: 'document-processing-complete'
        }
      })
  }
}

// Update stage after embeddings complete
await updateStage(supabase, job.id, 'embedded')

// Update document status to completed
await updateDocumentStatus(
  supabase,
  document_id,
  'completed',
  true, // markdown_available
  true  // embeddings_available
)
```

**Key Design Decisions**:
- Connection detection runs **asynchronously** (doesn't block document completion)
- Duplicate prevention checks `completed` status (fix for retry race condition)
- Document immediately available for reading (connections surface later)

---

### Helper Functions

#### `updateStage()`

**Location**: `worker/handlers/process-document.ts:367-397`

```typescript
async function updateStage(
  supabase: any,
  jobId: string,
  stage: string
) {
  const { data: job } = await supabase
    .from('background_jobs')
    .select('metadata')
    .eq('id', jobId)
    .single()

  const metadata = job?.metadata || {}
  const completedStages = metadata.completed_stages || []

  await supabase
    .from('background_jobs')
    .update({
      metadata: {
        ...metadata,
        processing_stage: stage,
        completed_stages: [...completedStages, stage],
        stage_timestamps: {
          ...metadata.stage_timestamps,
          [stage]: new Date().toISOString()
        }
      }
    })
    .eq('id', jobId)
}
```

**Exported For**: Testing and external use

---

## Retry & Resume Logic

### Retry Scenarios

```mermaid
sequenceDiagram
    participant W as Worker
    participant DB as Database
    participant AI as Gemini AI
    participant S as Storage

    Note over W,S: SCENARIO 1: Network Timeout During AI Processing

    W->>AI: Extract content (15 min for 500 pages)
    Note over W: Heartbeat every 5min<br/>prevents stale detection
    AI-->>W: ProcessResult
    W->>DB: Cache result in job.metadata
    Note over W: ‚ùå Network error saving to storage
    W->>DB: Mark job failed (transient)

    Note over W,S: RETRY: Resume from Cache

    W->>DB: Get job with cached result
    W->>W: Skip AI processing (use cache)
    Note over W: üí∞ Saved $0.40
    W->>S: Upload markdown (retry succeeds)
    W->>DB: Update stage: markdown_saved
    W->>W: Continue from checkpoint

    Note over W,S: SCENARIO 2: Crash During Chunk Insertion

    W->>DB: Insert 350 chunks
    Note over W: ‚úì Stage: chunked
    W->>AI: Generate embeddings
    Note over W: ‚ùå Worker crashes

    Note over W,S: RETRY: Preserve Chunks

    W->>DB: Get job (stage = chunked)
    W->>W: Check isResume = true
    W->>DB: Skip chunk deletion (preserve IDs)
    Note over W: ‚úì Prevents FK violations
    W->>W: Continue embedding generation
```

### Cache-Based Resume

**Key Insight**: The most expensive operation (AI extraction) is cached immediately after completion, before any I/O operations that might fail.

**Cache Structure**:
```json
{
  "metadata": {
    "cached_chunks": [...],
    "cached_markdown": "...",
    "cached_metadata": {...},
    "cached_word_count": 25000,
    "cached_outline": [...],
    "cache_created_at": "2025-10-04T...",

    "processing_stage": "extracted",
    "completed_stages": ["extracting"],
    "stage_timestamps": {
      "extracting": "2025-10-04T..."
    }
  }
}
```

**Resume Logic**:
```typescript
// Check what stage we're at
const stage = job.metadata?.processing_stage || 'pending'

if (stage === 'pending') {
  // Start from beginning
  await extractWithAI()
}

if (stage === 'extracted' || stage === 'markdown_saved') {
  // Skip AI, use cache
  result = {
    markdown: job.metadata.cached_markdown,
    chunks: job.metadata.cached_chunks
  }
}

if (stage === 'chunked' || stage === 'embedded') {
  // Skip everything, just verify completion
  // Don't delete chunks!
}
```

### Conditional Deletion

**The Critical Fix**: Prevents FK violations by preserving chunks during retry

```typescript
const isResume = ['chunked', 'embedded', 'complete'].includes(stage)

if (isResume) {
  // PRESERVE: Chunks already in database, collision detection may be using them
  console.log(`‚ôªÔ∏è  Resuming from stage: ${stage}, keeping existing chunks`)
} else {
  // CLEAN SLATE: Fresh processing, safe to delete old chunks
  await supabase.from('chunks').delete().eq('document_id', document_id)
}
```

**Why This Matters**:
- **Before**: Retry during collision detection ‚Üí delete chunks ‚Üí FK violations (94% failure)
- **After**: Retry preserves chunks ‚Üí collision detection references valid IDs (0% failures)

---

## Processor Implementations

### Processor Contract

All processors implement this interface:

```typescript
interface ProcessResult {
  markdown: string           // Clean markdown content
  chunks: ProcessedChunk[]   // Semantic chunks for search
  metadata: DocumentMetadata // Title, author, dates, etc.
  wordCount?: number         // Total word count
  outline?: OutlineSection[] // Document structure
}

interface ProcessedChunk {
  content: string            // Chunk text
  chunk_index: number        // Order in document (0-based)
  heading?: string           // Associated heading
  themes: string[]           // 2-3 key topics
  concepts: Array<{          // 5-10 key concepts
    text: string
    importance: number       // 0-1
  }>
  emotional_tone: {          // For contradiction detection
    polarity: number         // -1 to 1
    primaryEmotion: string
  }
  importance_score: number   // 0-1 for filtering
  summary: string            // 1-2 sentence summary
}
```

### Processor Responsibilities

**What Processors DO**:
- Extract content from source format
- Convert to clean, readable markdown
- Create semantic chunks (200-500 words each)
- Extract rich metadata using AI
- Calculate word counts and structure

**What Processors DON'T DO** (Handler's job):
- Save markdown to storage
- Generate embeddings
- Insert chunks to database
- Update document status
- Create background jobs

### Implemented Processors

#### 1. PDF Processor

**File**: `worker/processors/pdf-processor.ts`

**Strategy**:
- Small PDFs (<200 pages): Single-pass extraction
- Large PDFs (500+ pages): Batched extraction with stitching

```typescript
// Batched extraction for large documents
if (totalPages > 200) {
  const BATCH_SIZE = 100
  const OVERLAP = 10

  for (let start = 0; start < totalPages; start += BATCH_SIZE - OVERLAP) {
    const end = Math.min(start + BATCH_SIZE, totalPages)
    batches.push(await extractBatch(fileUri, start, end))
  }

  markdown = stitchMarkdownBatches(batches)
} else {
  markdown = await extractSinglePass(fileUri)
}
```

**Cost**: ~$0.12 for 500 pages (6 batches @ $0.02)

---

#### 2. YouTube Processor

**File**: `worker/processors/youtube-processor.ts`

**Special Handling**:
- Fetches transcript with `youtube-transcript-plus`
- AI cleaning removes timestamp noise: `[[MM:SS](url)]`
- Preserves original in `source-raw.txt` for positioning
- Document-level timestamps (NOT chunk-level)

**7-Stage Pipeline**:
1. Transcript fetch (no API key needed)
2. Original backup with timestamps
3. AI cleaning (grammar, headings, structure)
4. Semantic chunking with metadata
5. Fuzzy positioning for annotations
6. Embeddings generation
7. Database storage

**Cost**: ~$0.05 per 1-hour video

---

#### 3. Web Processor

**File**: `worker/processors/web-processor.ts`

**Strategy**:
- Mozilla Readability for content extraction
- Removes ads, navigation, boilerplate
- AI cleanup for better formatting

**Error Handling**:
- Paywall detection ‚Üí suggests archive.ph
- Failed extraction ‚Üí helpful error message

**Cost**: ~$0.02 per article

---

#### 4. Markdown Processor

**File**: `worker/processors/markdown-processor.ts`

**Two Modes**:
- **As-Is**: Fast heading-based chunking (~30s, free)
- **Clean**: AI cleanup + semantic chunking (~1min, ~$0.02)

```typescript
if (processingMode === 'asis') {
  chunks = await headingBasedChunking(markdown)
} else {
  markdown = await aiCleanMarkdown(markdown)
  chunks = await semanticChunking(markdown)
}
```

---

#### 5. Text Processor

**File**: `worker/processors/text-processor.ts`

**Strategy**:
- Converts plain text to structured markdown
- AI adds headings, formatting, structure

---

#### 6. Paste Processor

**File**: `worker/processors/paste-processor.ts`

**Strategy**:
- Handles direct text paste
- Auto-detects YouTube timestamps if present
- Optional source attribution

---

#### 7. EPUB Processor

**File**: `worker/processors/epub-processor.ts`

**Strategy**:
- Extracts chapters from EPUB structure
- Batched processing for large books
- Preserves chapter hierarchy

**The Problem Case**: 500-page EPUBs taking 14+ minutes
- **Solution**: Heartbeat mechanism prevents timeout
- **Cost**: ~$0.54 per 500-page book

---

## Connection Detection System

### Architecture

```mermaid
graph TD
    A[Document Processing Complete] --> B[Create detect-connections Job]
    B --> C[Worker Picks Up Job]
    C --> D[Connection Orchestrator]

    D --> E[Get All Chunks for Document]
    D --> F[Get Existing Documents' Chunks]

    E --> G{Run 3 Engines in Parallel}
    F --> G

    G --> H[Semantic Similarity Engine]
    G --> I[Contradiction Detection Engine]
    G --> J[Thematic Bridge Engine]

    H --> K[pgvector Cosine Search]
    K --> L[94 connections found]

    I --> M[Metadata Analysis]
    M --> N[0 contradictions found]

    J --> O[AI Bridge Analysis]
    O --> P[Filter: importance>0.6]
    P --> Q[101 bridges found]

    L --> R[Orchestrator Aggregates]
    N --> R
    Q --> R

    R --> S[Remove Duplicates]
    S --> T[Save 195 Connections to DB]

    T --> U[Update Job Status: Complete]

    style H fill:#4CAF50
    style I fill:#FF9800
    style J fill:#2196F3
    style T fill:#9C27B0
```

### Engine Breakdown

#### 1. Semantic Similarity Engine

**Strategy**: Fast pgvector cosine similarity (no AI calls)

```typescript
// Find similar chunks using embeddings
const matches = await supabase.rpc('match_chunks', {
  query_embedding: sourceChunk.embedding,
  threshold: 0.7,
  exclude_document: sourceChunk.document_id,
  limit: 50
})
```

**Finds**: "These say the same thing"
- Same concepts expressed differently
- Paraphrases and restatements
- Cross-document topic overlap

**Performance**:
- No AI calls (pure vector math)
- Fast: <5 seconds for 100 chunks
- Cost: $0 (free)

---

#### 2. Contradiction Detection Engine

**Strategy**: Metadata-based tension detection (no AI calls)

```typescript
// Find shared concepts with opposite polarity
const sharedConcepts = getSharedConcepts(
  sourceChunk.concepts,
  targetChunk.concepts
)

const oppositePolarity = hasOppositePolarity(
  sourceChunk.emotional_tone.polarity,  // +0.8
  targetChunk.emotional_tone.polarity   // -0.6
)

if (sharedConcepts.length > 0 && oppositePolarity) {
  // Found conceptual tension!
}
```

**Finds**: "These disagree about the same thing"
- Same concept, different stance
- Opposing viewpoints
- Tensions and debates

**Fallback**: Syntax-based detection if metadata insufficient

**Performance**:
- No AI calls (uses extracted metadata)
- Fast: <10 seconds for 100 chunks
- Cost: $0 (free)

---

#### 3. Thematic Bridge Engine

**Strategy**: AI-powered cross-domain matching (filtered)

```typescript
// FILTER 1: Source importance > 0.6
if (sourceChunk.importance_score < 0.6) return []

// FILTER 2: Get promising candidates
const candidates = targetChunks.filter(t =>
  t.importance_score > 0.6 &&              // Important only
  t.document_id !== source.document_id &&   // Cross-document
  inferDomain(t) !== inferDomain(source) && // Different domains
  conceptOverlap(source, t) > 0.2 &&        // Some overlap
  conceptOverlap(source, t) < 0.7           // Not too much
).slice(0, 15) // Top 15 only

// ANALYZE: Use AI for each candidate (batched)
const analyses = await batchAnalyzeBridges(sourceChunk, candidates)
```

**Finds**: "These connect different domains through shared concepts"
- "Paranoia in Gravity's Rainbow ‚Üî surveillance capitalism"
- "Quantum mechanics ‚Üî consciousness studies"
- Literary analysis ‚Üî modern tech critique

**Aggressive Filtering**:
- **Before filtering**: 67 chunks √ó 300 candidates = 20,100 AI calls = $20
- **After filtering**: 50 chunks √ó 4 candidates = 200 AI calls = $0.20

**Performance**:
- ~200 AI calls per document (filtered)
- Execution time: ~60 seconds
- Cost: ~$0.20 per document

---

### Orchestrator Coordination

**File**: `worker/engines/orchestrator.ts`

```typescript
export async function processDocument(
  documentId: string,
  config?: OrchestratorConfig
): Promise<OrchestrationResult> {
  // Run all 3 engines
  const [semantic, contradiction, thematic] = await Promise.all([
    runSemanticSimilarity(documentId),
    runContradictionDetection(documentId),
    runThematicBridge(documentId, apiKey)
  ])

  // Aggregate results
  const allConnections = [
    ...semantic.connections,
    ...contradiction.connections,
    ...thematic.connections
  ]

  // Remove duplicates
  const unique = deduplicateConnections(allConnections)

  // Save to database
  await saveChunkConnections(unique)

  return {
    totalConnections: unique.length,
    byEngine: {
      semantic_similarity: semantic.connections.length,
      contradiction_detection: contradiction.connections.length,
      thematic_bridge: thematic.connections.length
    },
    executionTime: '58.5s'
  }
}
```

**User-Configurable Weights**: Future feature for tuning engine importance

---

## Large Document Handling

### Batching Strategy

**Problem**: Gemini 2.5 Flash has 65k output token limit
- 500-page book = ~200k tokens of output
- Must batch to stay within limits

**Solution**: Batched extraction with overlap + stitching

```mermaid
graph LR
    A[500-page PDF] --> B[Batch 1: Pages 1-100]
    A --> C[Batch 2: Pages 91-190]
    A --> D[Batch 3: Pages 181-280]
    A --> E[Batch 4: Pages 271-370]
    A --> F[Batch 5: Pages 361-460]
    A --> G[Batch 6: Pages 451-500]

    B --> H[Overlap: 10 pages]
    C --> H
    D --> H
    E --> H
    F --> H
    G --> H

    H --> I[Fuzzy Stitching Algorithm]
    I --> J[Complete 500-page Markdown]

    J --> K[Batched Metadata Extraction]
    K --> L[10 batches of 50 chunks each]
    L --> M[Complete Chunks with Metadata]

    style I fill:#FF9800
    style K fill:#4CAF50
```

### Stitching Algorithm

**File**: `worker/lib/pdf-batch-utils.ts`

```typescript
function stitchMarkdownBatches(batches: ExtractionBatch[]): string {
  let stitched = batches[0].markdown

  for (let i = 1; i < batches.length; i++) {
    // Find best overlap point using fuzzy matching
    const overlapPoint = findBestOverlap(
      batches[i - 1].markdown.slice(-2000),  // Last 2000 chars
      batches[i].markdown.slice(0, 3000)     // First 3000 chars
    )

    if (overlapPoint > 0) {
      // Merge at overlap point
      stitched += batches[i].markdown.slice(overlapPoint)
    } else {
      // No overlap found, concatenate with separator
      stitched += '\n\n' + batches[i].markdown
    }
  }

  return stitched
}
```

**Quality**: 99%+ accuracy, no duplicate content

---

### Cost Breakdown (500-page book)

| Stage | Operation | Batches | Cost/Batch | Total |
|-------|-----------|---------|------------|-------|
| Extraction | PDF ‚Üí Markdown | 6 | $0.02 | **$0.12** |
| Metadata | Chunk metadata | 10 | $0.02 | **$0.20** |
| Embeddings | 382 chunks | 4 | $0.005 | **$0.02** |
| Connections | ThematicBridge | ~200 calls | $0.001 | **$0.20** |
| **TOTAL** | | | | **$0.54** |

**Budget**: $0.60 per book (under budget ‚úì)

---

### Performance Targets

| Document Type | Pages | Target Time | Actual Time | Status |
|---------------|-------|-------------|-------------|--------|
| Small PDF | <50 | <5 min | ~3 min | ‚úÖ |
| Medium PDF | 50-200 | <10 min | ~8 min | ‚úÖ |
| Large PDF | 500+ | <30 min | ~25 min | ‚úÖ |
| YouTube (1hr) | N/A | <2 min | ~90 sec | ‚úÖ |
| Web Article | N/A | <30 sec | ~20 sec | ‚úÖ |
| EPUB (500pg) | 500 | <30 min | ~18 min | ‚úÖ |

**Key Improvement**: Heartbeat mechanism prevents timeout for all large documents

---

## Database Schema

### Core Tables

#### `documents`

```sql
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL,
  title TEXT NOT NULL,
  author TEXT,
  source_type TEXT,
  storage_path TEXT NOT NULL,

  -- Processing status
  processing_status TEXT DEFAULT 'pending',
  processing_error TEXT,

  -- ‚úÖ CRITICAL FLAGS (both must be true for document to appear in reader)
  markdown_available BOOLEAN DEFAULT FALSE,
  embeddings_available BOOLEAN DEFAULT FALSE,

  -- Metadata
  word_count INTEGER,
  page_count INTEGER,
  metadata JSONB,
  source_metadata JSONB, -- YouTube timestamps, etc.

  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Critical Flags**:
- `markdown_available`: Set after successful storage upload
- `embeddings_available`: Set after chunks with embeddings saved
- **Both must be TRUE** for document to appear in DocumentList

---

#### `background_jobs`

```sql
CREATE TABLE background_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL,
  job_type TEXT NOT NULL, -- 'process_document', 'detect-connections'

  status TEXT DEFAULT 'pending',
  input_data JSONB,
  output_data JSONB,

  -- Retry logic
  retry_count INTEGER DEFAULT 0,
  max_retries INTEGER DEFAULT 3,
  next_retry_at TIMESTAMPTZ,
  last_error TEXT,

  -- ‚úÖ NEW: Idempotent retry metadata
  metadata JSONB, -- Contains cache + stage tracking

  -- Timestamps
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**Metadata Structure** (for idempotent retry):
```json
{
  "cached_chunks": [...],
  "cached_markdown": "...",
  "cached_metadata": {...},
  "processing_stage": "chunked",
  "completed_stages": ["extracting", "extracted", "markdown_saved", "chunked"],
  "stage_timestamps": {
    "extracting": "2025-10-04T12:00:00Z",
    "extracted": "2025-10-04T12:15:00Z"
  }
}
```

---

#### `chunks`

```sql
CREATE TABLE chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES documents ON DELETE CASCADE,

  -- Content and position
  content TEXT NOT NULL,
  chunk_index INTEGER NOT NULL,
  start_offset INTEGER,
  end_offset INTEGER,
  word_count INTEGER,

  -- Search
  embedding vector(768), -- pgvector

  -- Rich metadata (for 3-engine system)
  themes TEXT[] NOT NULL,
  concepts JSONB, -- [{text: string, importance: 0-1}]
  emotional_tone JSONB, -- {polarity: -1 to 1, primaryEmotion: string}
  importance_score FLOAT, -- 0-1
  summary TEXT,

  created_at TIMESTAMPTZ DEFAULT NOW(),

  UNIQUE(document_id, chunk_index) -- Prevents duplicates
);

CREATE INDEX idx_chunks_embedding ON chunks
  USING ivfflat (embedding vector_cosine_ops);
```

---

#### `chunk_connections`

```sql
CREATE TABLE chunk_connections (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

  source_chunk_id UUID REFERENCES chunks ON DELETE CASCADE,
  target_chunk_id UUID REFERENCES chunks ON DELETE CASCADE,

  connection_type TEXT NOT NULL, -- 'semantic_similarity', 'contradiction_detection', 'thematic_bridge'
  strength FLOAT NOT NULL, -- 0-1

  metadata JSONB, -- Engine-specific data

  discovered_at TIMESTAMPTZ DEFAULT NOW(),

  CONSTRAINT no_self_connections CHECK (source_chunk_id != target_chunk_id)
);

CREATE INDEX idx_connections_source ON chunk_connections(source_chunk_id);
CREATE INDEX idx_connections_type ON chunk_connections(connection_type);
```

---

## Error Handling

### Error Classification

**File**: `worker/lib/errors.ts`

```typescript
export function classifyError(error: Error): 'transient' | 'permanent' {
  const transientPatterns = [
    'rate limit',
    'timeout',
    'unavailable',
    'ECONNRESET',
    'ECONNREFUSED',
    '429', // Rate limit
    '503', // Service unavailable
    '504'  // Gateway timeout
  ]

  const message = error.message.toLowerCase()
  return transientPatterns.some(p => message.includes(p))
    ? 'transient'
    : 'permanent'
}
```

**Retry Strategy**:
- **Transient errors**: Retry with exponential backoff (5s, 25s, 125s)
- **Permanent errors**: Mark failed immediately, no retry

---

### User-Friendly Error Messages

```typescript
export function getUserFriendlyError(error: Error): string {
  const message = error.message.toLowerCase()

  if (message.includes('transcript') && message.includes('disabled')) {
    return 'YouTube transcript is disabled for this video. Try pasting the transcript manually.'
  }

  if (message.includes('paywall') || message.includes('403')) {
    return 'Article is behind a paywall. Try using archive.ph to access it.'
  }

  if (message.includes('rate limit')) {
    return 'API rate limit reached. Please try again in a few minutes.'
  }

  // Generic fallback
  return 'Processing failed. Please try uploading again.'
}
```

---

### Error Recovery Workflow

```mermaid
graph TD
    A[Error Occurs] --> B{Classify Error}
    B -->|Transient| C{Retry Count < 3?}
    B -->|Permanent| D[Mark Failed]

    C -->|Yes| E{Check Stage}
    C -->|No| D

    E -->|Stage = extracted| F[Resume from Cache]
    E -->|Stage = chunked| G[Preserve Chunks]
    E -->|Stage = pending| H[Restart from Beginning]

    F --> I[Continue Processing]
    G --> I
    H --> J[Run AI Extraction]

    I --> K[Success]
    J --> K

    D --> L[Show Error to User]
    L --> M[Manual Intervention]
```

---

## Monitoring & Debugging

### Key Metrics

**Processing Performance**:
- Average processing time per document type
- Heartbeat failure rate (should be 0%)
- Stage completion rates
- Cache hit rate on retries

**Cost Tracking**:
- AI calls per document
- Total cost per document type
- Budget adherence ($0.60 target per 500 pages)

**Quality Metrics**:
- FK violation rate (should be 0% after fix)
- Stale job recovery rate (should be low with heartbeat)
- Connection detection success rate
- Duplicate job creation rate (should be 0% after fix)

---

### Logging Conventions

**Stage Transitions**:
```
üìç Stage updated: extracted
üìç Stage updated: markdown_saved
üìç Stage updated: chunked
üìç Stage updated: embedded
```

**Heartbeat**:
```
[Heartbeat] Updating job timestamp to prevent stale detection...
[Heartbeat] Stopped
```

**Cache Usage**:
```
‚ôªÔ∏è  Using cached processing result from previous attempt
üí∞ Saved ~$0.40 by skipping AI re-processing
```

**Conditional Deletion**:
```
‚ôªÔ∏è  Resuming from stage: chunked, keeping existing chunks
   This prevents FK violations in collision detection
```

**Connection Detection**:
```
üîç Creating collision detection job for 67 chunks
üîç Collision detection job already exists (completed) - skipping duplicate
```

---

### Common Issues & Solutions

#### Issue 1: Document Not Appearing in List

**Symptoms**: Processing shows "completed" but document not in DocumentList

**Debug**:
```sql
SELECT
  id,
  title,
  processing_status,
  markdown_available,
  embeddings_available
FROM documents
WHERE id = 'document-id';
```

**Solution**: Check both flags are TRUE
```sql
UPDATE documents
SET
  markdown_available = TRUE,
  embeddings_available = TRUE
WHERE id = 'document-id';
```

---

#### Issue 2: FK Violations in Connections

**Symptoms**: Logs show FK constraint violations

**Root Cause**: Chunks deleted during retry while connections being detected

**Solution**: ‚úÖ FIXED by conditional deletion (implemented in this PRP)

**Verification**:
```sql
-- Should return 0 rows after fix
SELECT * FROM chunk_connections cc
WHERE NOT EXISTS (
  SELECT 1 FROM chunks c WHERE c.id = cc.source_chunk_id
);
```

---

#### Issue 3: Jobs Stuck in "Processing"

**Symptoms**: Jobs show "processing" for >30 minutes

**Debug**:
```sql
SELECT
  id,
  job_type,
  status,
  started_at,
  NOW() - started_at AS elapsed
FROM background_jobs
WHERE status = 'processing'
ORDER BY started_at DESC;
```

**Solution**: Check if worker is running
```bash
# Check worker status
npm run status

# Restart worker if needed
npm run stop
npm run dev
```

---

#### Issue 4: Duplicate Connection Detection Jobs

**Symptoms**: Multiple "detect-connections" jobs for same document

**Root Cause**: Race condition when job retries

**Solution**: ‚úÖ FIXED by including 'completed' in duplicate check

**Verification**:
```sql
-- Should return max 1 row per document after fix
SELECT
  input_data->>'document_id' AS document_id,
  COUNT(*) AS job_count
FROM background_jobs
WHERE job_type = 'detect-connections'
GROUP BY document_id
HAVING COUNT(*) > 1;
```

---

### Debug Commands

**Check Job Status**:
```bash
# All jobs
SELECT * FROM background_jobs ORDER BY created_at DESC LIMIT 10;

# Failed jobs
SELECT * FROM background_jobs WHERE status = 'failed';

# Job metadata (cache + stages)
SELECT metadata FROM background_jobs WHERE id = 'job-id';
```

**Check Document Flags**:
```bash
# Documents missing flags
SELECT id, title, processing_status, markdown_available, embeddings_available
FROM documents
WHERE processing_status = 'completed'
  AND (markdown_available = FALSE OR embeddings_available = FALSE);
```

**Check Chunks**:
```bash
# Chunk count per document
SELECT document_id, COUNT(*) AS chunk_count
FROM chunks
GROUP BY document_id;

# Orphaned chunks (document deleted)
SELECT c.* FROM chunks c
LEFT JOIN documents d ON c.document_id = d.id
WHERE d.id IS NULL;
```

**Check Connections**:
```bash
# Connection breakdown by engine
SELECT
  connection_type,
  COUNT(*) AS count,
  AVG(strength) AS avg_strength
FROM chunk_connections
GROUP BY connection_type;

# FK violations (should be 0)
SELECT COUNT(*) FROM chunk_connections cc
WHERE NOT EXISTS (SELECT 1 FROM chunks WHERE id = cc.source_chunk_id)
   OR NOT EXISTS (SELECT 1 FROM chunks WHERE id = cc.target_chunk_id);
```

---

## Version History

### v2.0 (October 4, 2025) - Idempotent Retry Logic

**Major Changes**:
- ‚úÖ Added heartbeat mechanism (5-min intervals)
- ‚úÖ Increased stale timeout (10min ‚Üí 30min)
- ‚úÖ Implemented stage tracking in job metadata
- ‚úÖ Conditional chunk deletion (prevents FK violations)
- ‚úÖ Enhanced duplicate job prevention
- ‚úÖ Added polling fallback in DocumentList

**Impact**:
- FK violation rate: 94% ‚Üí 0%
- Large document timeout rate: High ‚Üí 0%
- Retry efficiency: +60% (cache reuse)
- Data loss incidents: Eliminated

### v1.0 (September 2025) - Initial Architecture

**Features**:
- Single Responsibility Principle enforcement
- 7 processor types
- 3-engine connection detection
- Hybrid storage strategy
- Background job system

---

## Appendix

### Related Documentation

- **[docs/APP_VISION.md](APP_VISION.md)** - Core philosophy
- **[docs/ARCHITECTURE.md](ARCHITECTURE.md)** - System design
- **[docs/prps/idempotent-retry-logic.md](prps/idempotent-retry-logic.md)** - PRP for retry implementation
- **[worker/README.md](../worker/README.md)** - Worker module documentation

### Performance Benchmarks

Run benchmarks:
```bash
cd worker
npm run benchmark:pdf
npm run benchmark:semantic-engine
npm run benchmark:cache
```

### Testing

```bash
# Main app tests
npm test

# Worker tests
cd worker && npm test

# Integration tests
cd worker && npm run test:integration

# Validation suite
cd worker && npm run test:full-validation
```

---

**Document End** - Last updated: October 4, 2025
