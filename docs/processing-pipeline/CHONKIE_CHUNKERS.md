# Chonkie Chunkers Guide

Complete reference for Rhizome V2's 9 chunking strategies powered by [Chonkie](https://docs.chonkie.ai).

**Quick Start**: Default chunker is `recursive` (3-5 min, high quality). Works great for most documents.

---

## Overview

| Chunker | Speed | Quality | Use Case | API Cost |
|---------|-------|---------|----------|----------|
| **recursive** | ‚ö°‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê High | **Default** - General purpose | $0 |
| token | ‚ö°‚ö°‚ö° Fast | ‚≠ê‚≠ê Good | Fixed-size chunks | $0 |
| sentence | ‚ö°‚ö°‚ö° Fast | ‚≠ê‚≠ê Good | Sentence boundaries | $0 |
| table | ‚ö°‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê High | Markdown tables | $0 |
| code | ‚ö°‚ö° Medium | ‚≠ê‚≠ê‚≠ê High | Source code (AST-aware) | $0 |
| semantic | ‚ö° Slow | ‚≠ê‚≠ê‚≠ê‚≠ê Very High | Thematic documents | $0 |
| late | üêå Very Slow | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Highest | RAG/embeddings quality | $0 |
| neural | üêå Very Slow | ‚≠ê‚≠ê‚≠ê‚≠ê Very High | BERT semantic shifts | $0 |
| slumber | üêåüêå Slowest | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Highest | LLM-powered agentic | **$$** |

**Processing Times** (M1 Max 64GB, 200-page book):
- Fast (token/sentence/recursive/table): 3-5 min
- Medium (code): 5-10 min
- Slow (semantic): 8-15 min
- Very Slow (late): 10-20 min
- Very Slow (neural): 15-25 min
- Slowest (slumber): 30-60 min

---

## Chunker Strategies

### 1. RecursiveChunker (Default) ‚≠ê

**Philosophy**: Hierarchical splitting with structural awareness.

**How it works**:
1. Split by paragraphs (`\n\n`)
2. If too large, split by sentences (`. `, `! `, `? `)
3. If still too large, split by tokens

**Best for**:
- General documents (PDFs, EPUBs, web articles)
- Balanced speed and quality
- Preserving document structure

**Configuration**:
```typescript
{
  chunker_type: "recursive",
  chunk_size: 512,        // Max tokens per chunk
  tokenizer: "gpt2"       // Token counter
}
```

**Example**:
```markdown
# Chapter 1: Introduction

This is the first paragraph. It stays together.

This is the second paragraph. Also stays together.
```

**Chunks produced**:
```
Chunk 1: "# Chapter 1: Introduction\n\nThis is the first paragraph..."
Chunk 2: "This is the second paragraph..."
```

---

### 2. TokenChunker

**Philosophy**: Fixed-size token windows with optional overlap.

**How it works**:
- Splits text into exactly N tokens
- Optional overlap between chunks for context preservation

**Best for**:
- Uniform chunk sizes for embeddings
- Simple, predictable chunking
- When structure doesn't matter

**Configuration**:
```typescript
{
  chunker_type: "token",
  chunk_size: 512,
  chunk_overlap: 128,     // Tokens of overlap
  tokenizer: "gpt2"
}
```

**Trade-offs**:
- ‚úÖ Fast and predictable
- ‚úÖ Uniform chunk sizes
- ‚ùå May split mid-sentence
- ‚ùå No structural awareness

---

### 3. SentenceChunker

**Philosophy**: Respect sentence boundaries.

**How it works**:
- Splits on sentence delimiters (`. `, `! `, `? `)
- Groups sentences up to chunk_size
- Minimum sentences per chunk

**Best for**:
- Preserving complete thoughts
- Reading comprehension tasks
- When sentences are natural units

**Configuration**:
```typescript
{
  chunker_type: "sentence",
  chunk_size: 512,
  min_sentences: 1,       // Min sentences per chunk
  tokenizer: "gpt2"
}
```

**Trade-offs**:
- ‚úÖ Natural reading units
- ‚úÖ No mid-sentence splits
- ‚ùå Variable chunk sizes
- ‚ùå Slow for very long sentences

---

### 4. SemanticChunker

**Philosophy**: Split on semantic topic shifts using embeddings.

**How it works**:
1. Split text into sentences
2. Generate embeddings for each sentence
3. Calculate semantic similarity between adjacent sentences
4. Create chunk boundaries where similarity drops below threshold

**Best for**:
- Documents with clear topic transitions
- Narrative text (books, articles)
- Preserving thematic coherence

**Configuration**:
```typescript
{
  chunker_type: "semantic",
  chunk_size: 512,
  embedding_model: "minishlab/potion-base-32M",
  threshold: 0.8          // Higher = smaller chunks
}
```

**Parameters**:
- `threshold`: 0.0-1.0 (default: 0.8)
  - Lower (0.5): Larger chunks, loose topic grouping
  - Higher (0.9): Smaller chunks, tight topic grouping

**Trade-offs**:
- ‚úÖ Semantically coherent chunks
- ‚úÖ Natural topic boundaries
- ‚ùå 3-5x slower than recursive
- ‚ùå Requires embedding model (CPU or GPU)

**Note**: On Apple Silicon, may require `PYTORCH_ENABLE_MPS_FALLBACK=1` if you encounter MPS errors.

---

### 5. LateChunker

**Philosophy**: "Late chunking" - chunk after embedding for optimal retrieval quality.

**How it works**:
1. Split text using recursive rules
2. Generate contextual embeddings
3. Optimize chunk boundaries based on embedding coherence

**Best for**:
- High-quality RAG applications
- When retrieval accuracy is critical
- Research/academic documents

**Configuration**:
```typescript
{
  chunker_type: "late",
  chunk_size: 2048,
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
}
```

**Trade-offs**:
- ‚úÖ Highest retrieval quality
- ‚úÖ Better context preservation
- ‚ùå 5-10x slower than recursive
- ‚ùå Higher memory usage

**Use case**: When search quality matters more than speed (research databases, legal documents).

---

### 6. CodeChunker

**Philosophy**: AST-aware code splitting.

**How it works**:
1. Parse code into Abstract Syntax Tree (AST)
2. Identify semantic units (functions, classes, blocks)
3. Chunk along structural boundaries

**Best for**:
- Source code files
- Preserving function/class boundaries
- Code documentation

**Configuration**:
```typescript
{
  chunker_type: "code",
  chunk_size: 2048,
  language: "python",     // python, javascript, typescript, etc.
  include_nodes: false,   // Include AST node info
  tokenizer: "gpt2"
}
```

**Supported languages**: Python, JavaScript, TypeScript, Java, C++, Rust, Go, etc. (via tree-sitter)

**Requirements**:
```bash
pip install chonkie[code]
```

**Trade-offs**:
- ‚úÖ Preserves code structure
- ‚úÖ No mid-function splits
- ‚ùå Requires language-specific parsing
- ‚ùå 2-3x slower than recursive

---

### 7. NeuralChunker

**Philosophy**: BERT-based semantic shift detection.

**How it works**:
1. Use fine-tuned BERT model to detect semantic boundaries
2. Identify topic shifts at sentence level
3. Create chunks at natural semantic breaks

**Best for**:
- Complex narrative documents
- Multi-topic articles
- When semantic precision is critical

**Configuration**:
```typescript
{
  chunker_type: "neural",
  model: "mirth/chonky_modernbert_base_1",
  device_map: "cpu",      // or "cuda", "mps"
  min_characters_per_chunk: 10
}
```

**Trade-offs**:
- ‚úÖ High semantic accuracy
- ‚úÖ Better than rule-based for complex text
- ‚ùå 5-8x slower than recursive
- ‚ùå Requires BERT model download (~400MB)

**Note**: Does NOT use `tokenizer` or `chunk_size` parameters (uses BERT's own tokenization).

---

### 8. TableChunker

**Philosophy**: Markdown table-aware chunking.

**How it works**:
- Identifies markdown tables
- Keeps tables intact when possible
- Splits large tables intelligently

**Best for**:
- Documents with many tables
- Data-heavy content
- Preserving tabular structure

**Configuration**:
```typescript
{
  chunker_type: "table",
  chunk_size: 2048,
  tokenizer: "gpt2"
}
```

**Trade-offs**:
- ‚úÖ Preserves table structure
- ‚úÖ Fast processing
- ‚ùå Only useful for table-heavy docs
- ‚ùå Limited documentation (newer chunker)

---

### 9. SlumberChunker (Agentic LLM)

**Philosophy**: Use LLM intelligence to make chunking decisions.

**How it works**:
1. Present candidate split points to LLM (Gemini or OpenAI)
2. LLM evaluates semantic coherence
3. Makes intelligent decisions about chunk boundaries
4. Iterates until optimal chunking achieved

**Best for**:
- Highest quality requirements
- Complex documents with mixed content
- When cost/time isn't a constraint

**Configuration**:
```typescript
{
  chunker_type: "slumber",
  chunk_size: 1024,
  genie_model: "gemini-2.5-flash-lite",  // or custom model
  candidate_size: 128,    // Context window for decisions
  verbose: true,          // Show decision process
  tokenizer: "gpt2"
}
```

**Requirements**:
```bash
# In worker/.env
GEMINI_API_KEY=your_api_key_here

# Python dependencies
pip install chonkie[genie]
```

**Trade-offs**:
- ‚úÖ Highest quality chunking
- ‚úÖ Handles complex edge cases
- ‚ùå 10-20x slower than recursive
- ‚ùå **Costs API credits** (every decision = API call)
- ‚ùå Requires internet connection

**Cost estimate**: ~$0.05-0.15 per 200-page document (with gemini-2.5-flash-lite)

---

## Decision Tree

```
Do you need the absolute best quality regardless of cost/time?
‚îú‚îÄ YES ‚Üí Use "slumber" (LLM-powered, costs API credits)
‚îî‚îÄ NO ‚Üì

Is your document primarily source code?
‚îú‚îÄ YES ‚Üí Use "code" (AST-aware)
‚îî‚îÄ NO ‚Üì

Do you have 15+ minutes and need semantic precision?
‚îú‚îÄ YES ‚Üí Use "semantic" or "neural"
‚îî‚îÄ NO ‚Üì

Is this for high-quality RAG/retrieval?
‚îú‚îÄ YES ‚Üí Use "late" (10-20 min)
‚îî‚îÄ NO ‚Üì

Do you need it fast and good quality?
‚îî‚îÄ YES ‚Üí Use "recursive" (DEFAULT, 3-5 min) ‚≠ê
```

---

## Implementation Details

### Parameter Reference

#### Common Parameters (Most Chunkers)

```typescript
interface CommonConfig {
  chunker_type: ChonkieStrategy
  chunk_size: number          // Max tokens per chunk (default: 512)
  tokenizer: string           // "gpt2", "character", etc. (default: "gpt2")
}
```

**Note**: `SemanticChunker`, `LateChunker`, and `NeuralChunker` do NOT use `tokenizer` parameter.

#### Chunker-Specific Parameters

**RecursiveChunker**:
```typescript
{
  rules?: RecursiveRules,     // Custom splitting rules
  recipe?: "markdown"         // Pre-configured ruleset
}
```

**SemanticChunker**:
```typescript
{
  embedding_model: string,    // Default: "minishlab/potion-base-32M"
  threshold: number           // 0.0-1.0, default: 0.8
}
```

**LateChunker**:
```typescript
{
  embedding_model: string     // Default: "sentence-transformers/all-MiniLM-L6-v2"
}
```

**NeuralChunker**:
```typescript
{
  model: string,              // Default: "mirth/chonky_modernbert_base_1"
  device_map: "cpu" | "cuda" | "mps",
  min_characters_per_chunk: number
}
```

**CodeChunker**:
```typescript
{
  language: string,           // "python", "javascript", etc.
  include_nodes: boolean      // Include AST node info
}
```

**SlumberChunker**:
```typescript
{
  genie_model: string,        // Default: "gemini-2.5-flash-lite"
  candidate_size: number,     // Default: 128
  verbose: boolean            // Default: true
}
```

---

## Usage Examples

### TypeScript (Worker Module)

```typescript
import { chunkWithChonkie } from './lib/chonkie/chonkie-chunker.js'

// Default recursive chunking
const chunks = await chunkWithChonkie(markdown, {
  chunker_type: 'recursive',
  chunk_size: 512
})

// Semantic chunking with custom threshold
const semanticChunks = await chunkWithChonkie(markdown, {
  chunker_type: 'semantic',
  chunk_size: 512,
  threshold: 0.7  // Lower = larger chunks
})

// Code chunking
const codeChunks = await chunkWithChonkie(sourceCode, {
  chunker_type: 'code',
  language: 'python',
  chunk_size: 2048
})

// LLM-powered chunking (costs API credits)
const slumberChunks = await chunkWithChonkie(markdown, {
  chunker_type: 'slumber',
  genie_model: 'gemini-2.5-flash-lite',
  chunk_size: 1024
})
```

### Python Script (Direct)

```bash
# Recursive chunking
echo '{"markdown": "# Chapter 1\n\nContent...", "config": {"chunker_type": "recursive"}}' | \
  python3 worker/scripts/chonkie_chunk.py

# Semantic chunking
echo '{"markdown": "...", "config": {"chunker_type": "semantic", "threshold": 0.8}}' | \
  python3 worker/scripts/chonkie_chunk.py
```

---

## Performance Characteristics

### Speed Comparison (200-page PDF)

```
token       ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  2-3 min
sentence    ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  2-3 min
recursive   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  3-5 min (DEFAULT)
table       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  3-5 min
code        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  5-10 min
semantic    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë  8-15 min
late        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  10-20 min
neural      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  15-25 min
slumber     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  30-60 min
```

### Memory Usage

- **Low** (token, sentence, recursive, table): <500MB
- **Medium** (code): ~1GB
- **High** (semantic, late, neural): 2-4GB (embedding models)
- **Very High** (slumber): 3-5GB (LLM + embeddings)

### Quality Metrics

Based on chunk coherence testing:

```
slumber     ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ  95-98% semantic coherence
late        ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ  92-95% semantic coherence
semantic    ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ  88-92% semantic coherence
neural      ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ  85-90% semantic coherence
recursive   ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  75-85% semantic coherence
code        ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  80-85% (code-specific)
sentence    ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ  70-75% semantic coherence
table       ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  75-80% (table-specific)
token       ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ  65-70% semantic coherence
```

---

## Troubleshooting

### SemanticChunker: MPS Device Error

**Error**: `NotImplementedError: The operator 'aten::_embedding_bag' is not currently implemented for the MPS device`

**Solution**: Add to `worker/.env`:
```bash
PYTORCH_ENABLE_MPS_FALLBACK=1
```

**Why**: Apple Silicon (M1/M2/M3) uses MPS backend, but some PyTorch ops aren't implemented. This falls back to CPU.

---

### CodeChunker: Import Error

**Error**: `ImportError: One or more dependencies are not installed: [ tree-sitter, magika ]`

**Solution**:
```bash
pip install chonkie[code]
```

---

### SlumberChunker: Missing API Key

**Error**: `GeminiGenie requires an API key`

**Solution**: Add to `worker/.env`:
```bash
GEMINI_API_KEY=your_api_key_here
```

Get API key: https://aistudio.google.com/apikey

---

### NeuralChunker: Model Download Slow

**Issue**: First run downloads ~400MB BERT model

**Solution**: Be patient on first use. Model is cached for future runs.

**Location**: `~/.cache/huggingface/hub/`

---

## Migration from Old System

### Old Chunker ‚Üí New Chunker

```typescript
// Old: Fixed token chunking
const chunks = await tokenChunk(text, 512)

// New: Recursive chunking (better)
const chunks = await chunkWithChonkie(text, {
  chunker_type: 'recursive',
  chunk_size: 512
})
```

### Metadata Transfer

All chunkers return character offsets:

```typescript
interface ChonkieChunk {
  text: string
  start_index: number      // Character offset in original
  end_index: number        // Character offset in original
  token_count: number
  chunker_type: string
}
```

This ensures **bulletproof metadata transfer** from Docling chunks to Chonkie chunks.

---

## Best Practices

### 1. Start with Recursive

Unless you have specific needs, use `recursive`:
- Fast enough (3-5 min)
- Good quality (75-85% coherence)
- Free (no API costs)
- Works for 90% of documents

### 2. Use Semantic for Thematic Documents

If your document has clear topic transitions (books, long articles), upgrade to `semantic`:
- Better coherence (88-92%)
- Only 3x slower than recursive
- Still free (no API costs)

### 3. Reserve Slumber for Critical Documents

Only use `slumber` when quality is paramount:
- Research papers requiring perfect chunking
- Legal documents needing precision
- When you can afford time (30-60 min) and cost (API credits)

### 4. Code Always Uses CodeChunker

For source code, always use `code`:
- Preserves function/class boundaries
- AST-aware splitting
- No mid-function chunks

### 5. Tune chunk_size for Your Use Case

```typescript
// Tight retrieval (more precise matches)
chunk_size: 256

// Balanced (default)
chunk_size: 512

// Long context (preserve narrative flow)
chunk_size: 1024

// Maximum context (embeddings)
chunk_size: 2048
```

---

## Architecture Integration

### Pipeline Flow

```
Document Upload
    ‚Üì
Docling Extraction (768 tokens, HybridChunker)
    ‚Üì
Clean Markdown
    ‚Üì
Chonkie Chunking ‚Üê YOU ARE HERE
    ‚Üì
Metadata Transfer (Bulletproof Matching)
    ‚Üì
Embeddings (Transformers.js)
    ‚Üì
Database Storage
```

### File Locations

```
worker/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ chonkie_chunk.py          # Python chunking script (9 strategies)
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îî‚îÄ‚îÄ chonkie/
‚îÇ       ‚îú‚îÄ‚îÄ chonkie-chunker.ts    # TypeScript IPC wrapper
‚îÇ       ‚îî‚îÄ‚îÄ types.ts              # Type definitions
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ job-schemas.ts            # Zod validation for chunker configs
```

---

## Related Documentation

- **Processing Pipeline**: `docs/PROCESSING_PIPELINE.md`
- **Bulletproof Metadata**: `docs/processing-pipeline/bulletproof-metadata-extraction.md`
- **Local Pipeline Setup**: `docs/local-pipeline-setup.md`
- **Chonkie Official Docs**: https://docs.chonkie.ai

---

## Future Improvements

### Planned

- [ ] Hybrid chunking (combine multiple strategies)
- [ ] Custom recipe support for RecursiveChunker
- [ ] Chunk quality scoring
- [ ] A/B testing framework

### Research

- [ ] Multi-modal chunking (images + text)
- [ ] Cross-document chunking (preserve references)
- [ ] Adaptive chunking (dynamic strategy selection)

---

**Remember**: For most documents, `recursive` chunking (default) provides the best balance of speed, quality, and cost. Only upgrade to advanced chunkers when you have specific needs.
